---
apiVersion: v1
items:
- apiVersion: v1
  data:
    ca.crt: |
      -----BEGIN CERTIFICATE-----
      MIIDMjCCAhqgAwIBAgIIOZgkPKjiSXMwDQYJKoZIhvcNAQELBQAwNzESMBAGA1UE
      CxMJb3BlbnNoaWZ0MSEwHwYDVQQDExhrdWJlLWFwaXNlcnZlci1sYi1zaWduZXIw
      HhcNMjQwNjExMTAxOTAxWhcNMzQwNjA5MTAxOTAxWjA3MRIwEAYDVQQLEwlvcGVu
      c2hpZnQxITAfBgNVBAMTGGt1YmUtYXBpc2VydmVyLWxiLXNpZ25lcjCCASIwDQYJ
      KoZIhvcNAQEBBQADggEPADCCAQoCggEBAOMNJQ31plTY73mhN9xkeDKdBtdIMt+4
      7WAaFY8heeQdmlZfUZkZsjl1pvSugZ/hZcRXumQ3K1tdB+cKZOTKJQgPzIzOETHM
      bt0kQ6oQoul19LaT3suFT2TJ9vqNRK6fCd4IQOpLgmg51h3lBfcDz42Dd6vCWWjt
      mlGbLWsDXV0//db2PNdLlSPljFaqPdUJ60cE9eQwdzvDiTZkjN0wm4Cal/QRPmN6
      wm66FafdJAmGNhKqz23eR/KpNvf5UfZjvWXiCAgPH1okpIQswSIdPBCp3lrkftga
      XLqeX48BEJAU2+X5IZjzihzDyPngyHxiH4cXz/x6WKPpoEvdsabiCB8CAwEAAaNC
      MEAwDgYDVR0PAQH/BAQDAgKkMA8GA1UdEwEB/wQFMAMBAf8wHQYDVR0OBBYEFAFe
      +hvToOpBQ1OjZmFKflYn87mGMA0GCSqGSIb3DQEBCwUAA4IBAQDdgDIWYEqHNCNx
      fVGa+5Sgh4qE/rKsILCAPKjFtvSWg5jkfTyTyZgig6ikby1xA8gVjL1dHy6jGv2g
      2RV8zavt9HBLSSHM/Gz+85dADsot5aE+fypMx3TQuFvxXNytcqE8TI1GUroQnwp4
      CEXcRaPJ3Tt9bqS3yJuPcwdREhUFjEdMO9ZFbVUk94tPsZ07tfPvaCTCrikf8Occ
      EmIBN0ahylZk8dw9OmKw3wtYgoLAeLfTFxNYTdVb1VYei4AK/yC29f8/skrwAPBX
      /9TNA9p5xnRtyaa9mYbO20LdfED7m4PsHzCw0U0XjqDvOosN/Xi1xD5ZxKsDRuxj
      EpPdHinx
      -----END CERTIFICATE-----
      -----BEGIN CERTIFICATE-----
      MIIDQDCCAiigAwIBAgIIK/muZYsht/cwDQYJKoZIhvcNAQELBQAwPjESMBAGA1UE
      CxMJb3BlbnNoaWZ0MSgwJgYDVQQDEx9rdWJlLWFwaXNlcnZlci1sb2NhbGhvc3Qt
      c2lnbmVyMB4XDTI0MDYxMTEwMTkwMVoXDTM0MDYwOTEwMTkwMVowPjESMBAGA1UE
      CxMJb3BlbnNoaWZ0MSgwJgYDVQQDEx9rdWJlLWFwaXNlcnZlci1sb2NhbGhvc3Qt
      c2lnbmVyMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA4iXHckm/7n1U
      QUlmnOPjm6Co/1q/cl/cWJRtsdAMvd9V5hsKehnPJ4Q4mDCQ3rkpz7fagNbHSEvO
      P9rAjEI6Bgzwbo+JAwMqDelN7CbEO9p2AW2ciuXQX2OhMow+13NBbbZxa8UHZcqG
      2rh+yRv/SIYT+A/WotWCQZwQJc9JiGsK1q1oLe5kNNr/2sCjVe3o63Tf74TTMC52
      AY1rDbnhrsx2VKB3JMV2XcQS5k9IxZbFPLJWzvGM2t4zZBnu0U9hN4JtzArsNSgl
      b3LiKAC6PCLV3kCVGbXlOiTykOTyNq3lhARHJzd5Lm6cN+uXywmVtj+QE45pgfbJ
      yroFyOiOZwIDAQABo0IwQDAOBgNVHQ8BAf8EBAMCAqQwDwYDVR0TAQH/BAUwAwEB
      /zAdBgNVHQ4EFgQUUZKSa8GUZ9qjiL/E7pSges35z4AwDQYJKoZIhvcNAQELBQAD
      ggEBAC6QwBuPhM8F+wB5zqt9ldzYXOzzlVy7u1+4agEvTlfz0vB7l2Bi22rFEimw
      hEXsmuZhVIJxgGNWVqLbg2dr8ax1QZ/gPeXqZXJY8wz0kzlNQB3tERBlxCQhIIoa
      cuQipYTA51C7WEoIQQc68+HrPmmHnhBbFw5LZbN6k6jRmLFotg7RCtjZLDS+2bxw
      qZT0YencAZMBVpvDoDZyW4HEdUd4bzrmK2p1DUNkMlZuSR+Rjjp8CLz+0ikveesK
      cGanBxN+oW1OzlVyaCI2Epu+aagPwHbUUFdlw7hsyNc5lofab+nfm2ViHZhZwBcj
      A0LSVVmltDG7jfi0Xj49ug51mUA=
      -----END CERTIFICATE-----
      -----BEGIN CERTIFICATE-----
      MIIDTDCCAjSgAwIBAgIINO0yAMSHeFcwDQYJKoZIhvcNAQELBQAwRDESMBAGA1UE
      CxMJb3BlbnNoaWZ0MS4wLAYDVQQDEyVrdWJlLWFwaXNlcnZlci1zZXJ2aWNlLW5l
      dHdvcmstc2lnbmVyMB4XDTI0MDYxMTEwMTkwMVoXDTM0MDYwOTEwMTkwMVowRDES
      MBAGA1UECxMJb3BlbnNoaWZ0MS4wLAYDVQQDEyVrdWJlLWFwaXNlcnZlci1zZXJ2
      aWNlLW5ldHdvcmstc2lnbmVyMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKC
      AQEA3PLsu6vfUb4qCKyXxaExvUXWGRiUrV3b1BiEvyER08XIovGzpHDQ41aD7mxT
      w1+mjW19MsSWD8XXLHaj8ad6GiZ3aNJ2iIAp6PGATQrKWY+LlOFnSrk21XArYOx3
      8HNx6kjgFdQOs3A4unbbCBNQh3MTbnJXfUaSDNBEYY+QMWaHSAoQPVThS4VcD6rI
      F8d7GG726ogUuAM4eDum3b7Dw6h9xZWFHS9A3L/05S1jD/p6r/E7BghRNRvvJsSm
      M78c68fQmygMIwAHYV0iQTXYzFl4qeMovCUTlm933P56flpghXTLbV858pJEiVKq
      eInZ6QWrH4oEp1Wvduxsm6I3AQIDAQABo0IwQDAOBgNVHQ8BAf8EBAMCAqQwDwYD
      VR0TAQH/BAUwAwEB/zAdBgNVHQ4EFgQUxgNzyToLk4YHrryxwHRdCi1YVZ4wDQYJ
      KoZIhvcNAQELBQADggEBACMEI0/eYMPlJf/TKVlQh+8zuFaqsmV4SEo4bcz2QR/5
      ytw7cC3W87lCAJ+yhWLF0c8ddjn2EgckASE97MVEvdY7z/JqD48tzS4MTBzKu5V6
      7g4qX0N8sPG8i7fKeB5VIdtpvk1THhwpJBgAYeRs2Vt3oR3hts38Hunf1Fm1Iq9a
      uC2a63NTfI5SFAg2jiICuokbdPcRSJWQhufS5vwBR5rTAoyP2iVJLtHrtPp+2+Fq
      O7G4qMDY8M8sVOsmTLnuXdoS7z+j9bwMGhYZG3xW3nvgEFLUEtB62OjM/8YQGzKj
      8PBR8CmLd/3Zz8/Bf4nIwJ9bB9acCCYHXh2Vr23X0sE=
      -----END CERTIFICATE-----
      -----BEGIN CERTIFICATE-----
      MIIDlzCCAn+gAwIBAgIIEtJWch74/5wwDQYJKoZIhvcNAQELBQAwWTFXMFUGA1UE
      AwxOb3BlbnNoaWZ0LWt1YmUtYXBpc2VydmVyLW9wZXJhdG9yX2xvY2FsaG9zdC1y
      ZWNvdmVyeS1zZXJ2aW5nLXNpZ25lckAxNzE4MTAyOTAzMB4XDTI0MDYxMTEwNDgy
      MloXDTM0MDYwOTEwNDgyM1owWTFXMFUGA1UEAwxOb3BlbnNoaWZ0LWt1YmUtYXBp
      c2VydmVyLW9wZXJhdG9yX2xvY2FsaG9zdC1yZWNvdmVyeS1zZXJ2aW5nLXNpZ25l
      ckAxNzE4MTAyOTAzMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA2gfY
      ieeA1s5yIPIyr4nreolJtWJU316iq04zgL77CwCJSCLdqovBxvSFgaprS5Cd+V3+
      7w3t5JrZ+/o1DeVfvMGcQdeYG0ruYp6tPAqzbV9huKjXNC4B1tipWTWL0xKfjfol
      VJJ+idbKQQm5cFg0q4MgwG8gT0B8sfJr7Tnz9pfjmUffcTqHk9xkq8A6ba+HnWc+
      AA5iZl7fmGUAtIRwaFHDZgMTGER7Hy7iY6DlaZhga2tHwk2srdj80gxFDutzIDXm
      DQla8I2ZEoEVbWZndub5JPpl0OrVc1l6JZ1Zo4dKz0Dg9ef9vd/AKTinVx+9sFUy
      XKBsbJnqWssuUNMYbwIDAQABo2MwYTAOBgNVHQ8BAf8EBAMCAqQwDwYDVR0TAQH/
      BAUwAwEB/zAdBgNVHQ4EFgQUVC5qGhSPlvXcUxjPlGQx5kRN+JAwHwYDVR0jBBgw
      FoAUVC5qGhSPlvXcUxjPlGQx5kRN+JAwDQYJKoZIhvcNAQELBQADggEBADlCo4YF
      aa3VGBLnkAvygstPOaeCRAeUXTVPeFeo6IgkrNBrkR2l1lP3cmH2XhGfxEbTILxT
      enFFwDjhM1zm4LsygLqA9FC08TzWuQyS1uDOajvLaXxtam6s6d3N6nDAElZzuwh2
      19XYLGEFWxgs1Pt71KoN16kVdPW/rVFgCPFyn0CVTjbPeoPSmqgJbsqyK/Rs2VOR
      4segDUoBmTmGYAsBx0Kq9BuOAAtfaA7UlluIQ8q5qrGd9xQ/3s8qmCV+syah0ljJ
      X0/Z2S0DrWBqX5bzkR6i6vzUg/ACMt+5B/j++NCyqs03ls862GD7AeiP8jaS51a2
      VBPno2MjcbkakiI=
      -----END CERTIFICATE-----
      -----BEGIN CERTIFICATE-----
      MIIDsTCCApmgAwIBAgIIdCvH9RE3lzgwDQYJKoZIhvcNAQELBQAwJjEkMCIGA1UE
      AwwbaW5ncmVzcy1vcGVyYXRvckAxNzE4MTAyOTYzMB4XDTI0MDYxMTEwNDkyNFoX
      DTI2MDYxMTEwNDkyNVowSDFGMEQGA1UEAww9Ki5hcHBzLmNpLW9wLTl4eDcxcnZx
      LTFlMjhlLnFlLmF6dXJlLmRldmNsdXN0ZXIub3BlbnNoaWZ0LmNvbTCCASIwDQYJ
      KoZIhvcNAQEBBQADggEPADCCAQoCggEBAMEXt7td5wBAHJLibHXL3j6/zeypnUlZ
      zFwfnYhxHFpmHdDXkj4+8yXVj/asJOtL9gTHh9VqNwnSr1slC+yq16+KEQPMGiwc
      Z94MpCUV4zNHTHqxCXOHWFCmQ+gXZXExRpWnZXN6jzUv29dCax/XJ+xA+Rre2Aly
      B45hkmfnDx3tggIcypQoFG3MMJVZxktKJZA/W9R/rRcQyYHRvIaWD96CNzaApjnR
      Z9l+FJ3U8zXEb2jUeNH9oa2/t2zPc5JOwaHFG0xaTPyTIif77qES6XbaqJgco6zr
      cCfim6/w6/9oaYcz9MD6cR1hGAoxokGphkazISMxL3UroLA+SFZ/cj8CAwEAAaOB
      wDCBvTAOBgNVHQ8BAf8EBAMCBaAwEwYDVR0lBAwwCgYIKwYBBQUHAwEwDAYDVR0T
      AQH/BAIwADAdBgNVHQ4EFgQUxxTcJW1j1SIm1srEoxJRfxoYXv0wHwYDVR0jBBgw
      FoAU0EJZY45sDzwQHJ9Fbs4J0+5YQXAwSAYDVR0RBEEwP4I9Ki5hcHBzLmNpLW9w
      LTl4eDcxcnZxLTFlMjhlLnFlLmF6dXJlLmRldmNsdXN0ZXIub3BlbnNoaWZ0LmNv
      bTANBgkqhkiG9w0BAQsFAAOCAQEAJyrM8ojm2I0nTliQ9ax/wRX5mFQ9rB00O/Oo
      GIeyObuNTQPkyxEjAr7s/Gdh0FriZMZbfYj1LeEg9kwjRsufzZqxvpLmJxMPFZxp
      ol4n8LNctWlYIWWG4KH2edc6c/pvBaxOJC9R0MgnjuNqPfvrKAk1OcNada1QQF4H
      PHJGeDSR2nBUBJj8WTLD9EbAaXkuiK2hf6xBBKsn3+Eu8wOVtphxs7CpPj3bwZsM
      n5N7IqLMaE9exNFXVMd+0tALBpte6UmpRG7YpAzNkDSCVHVTR9H+KdpdPmTyZ8Al
      +LGjhyal9KyTiDFwlwUa3hnK2P3w4Q0EjH7jRBj85bALW2454w==
      -----END CERTIFICATE-----
      -----BEGIN CERTIFICATE-----
      MIIDDDCCAfSgAwIBAgIBATANBgkqhkiG9w0BAQsFADAmMSQwIgYDVQQDDBtpbmdy
      ZXNzLW9wZXJhdG9yQDE3MTgxMDI5NjMwHhcNMjQwNjExMTA0OTIzWhcNMjYwNjEx
      MTA0OTI0WjAmMSQwIgYDVQQDDBtpbmdyZXNzLW9wZXJhdG9yQDE3MTgxMDI5NjMw
      ggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDWuPsQsmr2xRHEfP71JdYl
      0GBQDYw3/OmIHjvzE6uOfM1PAGtVQ482iExzYG0AIJq5AFMr7L3+XJuPdaDiudWu
      2+x32AOlFayWaWzBnbIqkdT1Gmotprgj5qXnCVI2maXzsQbJwefEFMfjkq0+4hs0
      K1y5aKQKqgNP8cl4OdENL3J7B1AJT4i3zqHB7Uu1li72TYRs0K68+IVN37GdBFi4
      kpdpOLTjE+akA2mg7A9MbC5gZsIN0hfbBNxhkPRf8137YfajsfCCxMLbjb94mW5M
      pcNZAfMnrFTMgAX3ADOckuw2EndjLmPyRjQHDplJrcjF4TxQ83IReEZRNcvvXN1v
      AgMBAAGjRTBDMA4GA1UdDwEB/wQEAwICpDASBgNVHRMBAf8ECDAGAQH/AgEAMB0G
      A1UdDgQWBBTQQlljjmwPPBAcn0VuzgnT7lhBcDANBgkqhkiG9w0BAQsFAAOCAQEA
      Ujk7rdAYEprDClAOKLCbnuv1FiAFrZgSp+JHHxujTTupCisSmcLeShXtg9g0DJ+l
      ae2bW9ukn4UVTrXjIXgurE98HhOXf+7+VGDlFehPcfBNvwHr9vbOI1TXfyB0bPov
      m4YAfVVY7XEH9yDCqzPIWuEfPyBlY1/k+XPezEoMukrAT5d49nuBdtqsIN3cQEN9
      +0HTffr3BEZcxBdDAhp2/tpmxwPWcC4G4W9V/ptCKBoYDarSzCUViTvEV2VpC8Yl
      v24kgR5RgQWWaLN1jUI6lxZtcUVelHV98UBTpjhNPXPEVjzWhUzxLcPjwJXfLPyA
      CCnuScmLsxK+fnsBo6WM0Q==
      -----END CERTIFICATE-----
  kind: ConfigMap
  metadata:
    annotations:
      kubernetes.io/description: Contains a CA bundle that can be used to verify the
        kube-apiserver when using internal endpoints such as the internal service
        IP or kubernetes.default.svc. No other usage is guaranteed across distributions
        of Kubernetes clusters.
    creationTimestamp: "2024-06-11T10:46:58Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:data:
          .: {}
          f:ca.crt: {}
        f:metadata:
          f:annotations:
            .: {}
            f:kubernetes.io/description: {}
      manager: kube-controller-manager
      operation: Update
      time: "2024-06-11T10:53:21Z"
    name: kube-root-ca.crt
    namespace: openshift-ovn-kubernetes
    resourceVersion: "16907"
    uid: 9097f289-bcbe-4197-971e-c3fe110ab610
- apiVersion: v1
  data:
    service-ca.crt: |
      -----BEGIN CERTIFICATE-----
      MIIDUTCCAjmgAwIBAgIIQdgmVTJQT4kwDQYJKoZIhvcNAQELBQAwNjE0MDIGA1UE
      Awwrb3BlbnNoaWZ0LXNlcnZpY2Utc2VydmluZy1zaWduZXJAMTcxODEwMjkwMjAe
      Fw0yNDA2MTExMDQ4MjFaFw0yNjA4MTAxMDQ4MjJaMDYxNDAyBgNVBAMMK29wZW5z
      aGlmdC1zZXJ2aWNlLXNlcnZpbmctc2lnbmVyQDE3MTgxMDI5MDIwggEiMA0GCSqG
      SIb3DQEBAQUAA4IBDwAwggEKAoIBAQDVP6m7d0GDdpqj+2+64a6iVjk9x7vJ+Eu5
      LfUty1iTVrlAUAhluIi7467k12iZ+EOM4zZwQ1+W/NH8Igl40Khj4rxgnDyyGW++
      NuOy6fg8AXq1t2F2e4rL5kx5Cl0U+CmcGJEI/Wg/0v9kgFbVPFU/XqBk6i1dm2bF
      t0LwnjU6ir/lc1W+ehYAlpQhAnnqK6+cgwyNs0LaHXKC3vHHsSi/dbG8ez0Lmgk6
      hxrTKC3bCLENabvfsOB1xZWvEbsk0D34B/QDz7AWBKNAyJDuO8sI8kl+6gFFBwl8
      nWSNFrawluFvkdcAIUIuSdnCj5xMjZhImsHZhq5aDe7e1iJCVb9ZAgMBAAGjYzBh
      MA4GA1UdDwEB/wQEAwICpDAPBgNVHRMBAf8EBTADAQH/MB0GA1UdDgQWBBTaQZF8
      zSjkHehTBQZMbjIJt/sA6jAfBgNVHSMEGDAWgBTaQZF8zSjkHehTBQZMbjIJt/sA
      6jANBgkqhkiG9w0BAQsFAAOCAQEAf7Ax/iimmJcbY0A7m1pkAYNYOM6fRP72ISGu
      6c+St77EADA9r3fYn3jtIJ/Br/rnsxNOAwAcGxUkofb8Pa8IFO61L8aykcEZljM2
      t98q890v/HWGt+msUqkaVDagEN/wUVnkbL8qcvI5I4CMGAnqOb3Cocyu/6NgpFlw
      5EkP2ncxaDN6WIYe7DZvZzsLHgkHbqFKq2Z9NXJoyZT3oh8wlW8YmxUxQ/87BS/o
      l2uj/A9HuAYB+0F0QRh167jCS1BQ0nK1hInxJ9ZkzPMcpLd5Z5JC6E71shQpx04u
      4egQz28icTrxjUEcZXYhX6aJ+4eHRE/Hoxst6QAOx1WK6kO6tA==
      -----END CERTIFICATE-----
  kind: ConfigMap
  metadata:
    annotations:
      service.beta.openshift.io/inject-cabundle: "true"
    creationTimestamp: "2024-06-11T10:46:58Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:data: {}
        f:metadata:
          f:annotations:
            .: {}
            f:service.beta.openshift.io/inject-cabundle: {}
      manager: kube-controller-manager
      operation: Update
      time: "2024-06-11T10:46:58Z"
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:data:
          f:service-ca.crt: {}
      manager: service-ca-operator
      operation: Update
      time: "2024-06-11T10:48:38Z"
    name: openshift-service-ca.crt
    namespace: openshift-ovn-kubernetes
    resourceVersion: "8675"
    uid: 1dd9c4f3-00f1-46a2-89e9-24ad1ef8c055
- apiVersion: v1
  data:
    ca-bundle.crt: |
      -----BEGIN CERTIFICATE-----
      MIIDTzCCAjegAwIBAgIIcryQGf0u2tgwDQYJKoZIhvcNAQELBQAwNTEzMDEGA1UE
      Awwqb3BlbnNoaWZ0LW92bi1rdWJlcm5ldGVzX292bi1jYUAxNzE4MTAyODIyMB4X
      DTI0MDYxMTEwNDcwMVoXDTM0MDYwOTEwNDcwMlowNTEzMDEGA1UEAwwqb3BlbnNo
      aWZ0LW92bi1rdWJlcm5ldGVzX292bi1jYUAxNzE4MTAyODIyMIIBIjANBgkqhkiG
      9w0BAQEFAAOCAQ8AMIIBCgKCAQEAvFwesR/wXW+ZhHl9YmPMO7b6X2atTwjpFggh
      befrMQ3/7Xlz/g7QWlPcP7wHvdOk3GgxTCzdGIyTj9vAUc2JM6p/6wCVwCjNOUPy
      LaBhffSkh2GWUCZ4B+Wb20nMEIejLpHEQOL3hSYB83wLyKs3jFDUnG5oTYel4Il3
      3OPLTDksMVTnsn3ohUOKmBFgYITk6zL7iXq0UfRKcjbHDSOxod7+VogVEXTM1Jq8
      l5Ueo7GQz9/3BthEll+k0Qodx4BEZY3V4XrTKs5tBazY/wurjhQyLRLl4NUbVygY
      M9qadJd5oCrUEEqbcTVewB12y69FKa1E6Cl895m5mblOCMrU/wIDAQABo2MwYTAO
      BgNVHQ8BAf8EBAMCAqQwDwYDVR0TAQH/BAUwAwEB/zAdBgNVHQ4EFgQUWoDXQvf0
      w/hNW0ogdDh3LAY+FigwHwYDVR0jBBgwFoAUWoDXQvf0w/hNW0ogdDh3LAY+Figw
      DQYJKoZIhvcNAQELBQADggEBAJ/IREZa6Mjkx5E+sj+HHYbfqn7Sve2vKHHAXhJj
      0W//Wk+U/zb8x8y22yGmd1PweYsC5D15eW30+EiAAG7h4GnUJv6o4xjoxxfjZWyI
      xbLnKJFCLW18t1PvfKgpuGlq+YQaqAS6Tszre6y0P3fFJ+f67Hgv/Wtr7fZmo72t
      hgPU2WjQk/5h5+TyX4gBQZELBMBTdlZhMCryT4Oup3WYpVPeIWu9w4smfIQphTgL
      l94+W6bpp1oJHd1/fRZsKOFyFWw/EyNbhxbpCBLsxRiZxhrXHqH9tXLC9OsQmWzq
      Y1w3MqaE4jqdjhJflRLp15Fvjsi6Em1KezqIxSK7YK6m/Bk=
      -----END CERTIFICATE-----
  kind: ConfigMap
  metadata:
    annotations:
      openshift.io/description: ""
      openshift.io/owning-component: Networking / cluster-network-operator
    creationTimestamp: "2024-06-11T10:47:02Z"
    labels:
      auth.openshift.io/managed-certificate-type: ca-bundle
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:data:
          .: {}
          f:ca-bundle.crt: {}
        f:metadata:
          f:annotations:
            .: {}
            f:openshift.io/description: {}
            f:openshift.io/owning-component: {}
          f:labels:
            .: {}
            f:auth.openshift.io/managed-certificate-type: {}
      manager: network-operator
      operation: Update
      time: "2024-06-11T10:47:02Z"
    name: ovn-ca
    namespace: openshift-ovn-kubernetes
    resourceVersion: "4734"
    uid: 7b1db5b7-d916-4352-afed-c7c3855a7085
- apiVersion: v1
  data:
    ovnkube.conf: |-
      [default]
      mtu="1400"
      cluster-subnets="10.128.0.0/14/23"
      encap-port="6081"
      enable-lflow-cache=true
      lflow-cache-limit-kb=1048576
      enable-udp-aggregation=true

      [kubernetes]
      service-cidrs="172.30.0.0/16"
      ovn-config-namespace="openshift-ovn-kubernetes"
      apiserver="https://api-int.ci-op-9xx71rvq-1e28e.qe.azure.devcluster.openshift.com:6443"
      host-network-namespace="openshift-host-network"
      no-hostsubnet-nodes="kubernetes.io/os=windows"
      platform-type="Azure"
      healthz-bind-address="0.0.0.0:10256"
      dns-service-namespace="openshift-dns"
      dns-service-name="dns-default"

      [ovnkubernetesfeature]
      enable-egress-ip=true
      enable-egress-firewall=true
      enable-egress-qos=true
      enable-egress-service=true
      egressip-node-healthcheck-port=9107
      enable-multi-network=true
      enable-admin-network-policy=true
      enable-multi-external-gateway=true

      [gateway]
      mode=shared
      nodeport=true

      [logging]
      libovsdblogfile=/var/log/ovnkube/libovsdb.log
      logfile-maxsize=100
      logfile-maxbackups=5
      logfile-maxage=0

      [hybridoverlay]
      enabled=true
      cluster-subnets="10.132.0.0/14"
  kind: ConfigMap
  metadata:
    creationTimestamp: "2024-06-11T10:47:01Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:data:
          f:ovnkube.conf: {}
        f:metadata:
          f:ownerReferences:
            k:{"uid":"62ee1ce2-90a7-43aa-95fc-751c249264a6"}: {}
      manager: cluster-network-operator/operconfig
      operation: Apply
      time: "2024-06-11T10:47:01Z"
    name: ovnkube-config
    namespace: openshift-ovn-kubernetes
    ownerReferences:
    - apiVersion: operator.openshift.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: Network
      name: cluster
      uid: 62ee1ce2-90a7-43aa-95fc-751c249264a6
    resourceVersion: "4717"
    uid: 014c8e04-6a32-41b6-a400-12011a462691
- apiVersion: v1
  data:
    ovnkube-lib.sh: |-
      #!/bin/bash
      set -x
      # Add node-specific overrides if the container has mounted any
      K8S_NODE=${K8S_NODE:-}
      if [[ -n "${K8S_NODE}" && -f "/env/${K8S_NODE}" ]]; then
        set -o allexport
        source "/env/${K8S_NODE}"
        set +o allexport
      fi

      northd_pidfile="/var/run/ovn/ovn-northd.pid"
      controller_pidfile="/var/run/ovn/ovn-controller.pid"
      controller_logfile="/var/log/ovn/acl-audit-log.log"
      vswitch_dbsock="/var/run/openvswitch/db.sock"
      nbdb_pidfile="/var/run/ovn/ovnnb_db.pid"
      nbdb_sock="/var/run/ovn/ovnnb_db.sock"
      nbdb_ctl="/var/run/ovn/ovnnb_db.ctl"
      sbdb_pidfile="/var/run/ovn/ovnsb_db.pid"
      sbdb_sock="/var/run/ovn/ovnsb_db.sock"
      sbdb_ctl="/var/run/ovn/ovnsb_db.ctl"

      # start-ovn-controller() starts ovn-controller and does not return until
      # ovn-controller exits
      #
      # Requires the following volume mounts:
      #   /run/openvswitch
      #   /run/ovn/
      #   /etc/openvswitch
      #   /etc/ovn/
      #   /var/lib/openvswitch
      #   /var/log/ovn/
      #   /dev/log
      start-ovn-controller()
      {
        local log_level=$1

        if [[ $# -ne 1 ]]; then
          echo "Expected one argument but got $#"
          exit 1
        fi

        echo "$(date -Iseconds) - starting ovn-controller"
        exec ovn-controller \
          unix:${vswitch_dbsock} \
          -vfile:off \
          --no-chdir \
          --pidfile=${controller_pidfile} \
          --syslog-method="null" \
          --log-file=${controller_logfile} \
          -vFACILITY:"local0" \
          -vconsole:"${log_level}" \
          -vconsole:"acl_log:off" \
          -vPATTERN:console:"%D{%Y-%m-%dT%H:%M:%S.###Z}|%05N|%c%T|%p|%m" \
          -vsyslog:"acl_log:info" \
          -vfile:"acl_log:info"
      }

      # quit-ovn-northd() will cleanly shut down ovn-northd. It is intended
      # to be run from a bash 'trap' like so:
      #
      #    trap quit-ovn-northd TERM INT
      quit-ovn-northd()
      {
        echo "$(date -Iseconds) - stopping ovn-northd"
        OVN_MANAGE_OVSDB=no /usr/share/ovn/scripts/ovn-ctl stop_northd
        echo "$(date -Iseconds) - ovn-northd stopped"
        rm -f ${northd_pidfile}
        exit 0
      }

      # run-ovn-northd() starts ovn-northd and does not return until
      # northd exits.
      #
      # Requires the following volume mounts:
      #   /etc/openvswitch/
      #   /var/lib/openvswitch/
      #   /run/openvswitch/
      #   /run/ovn/
      #   /var/log/ovn/
      start-ovn-northd()
      {
        local log_level=$1

        if [[ $# -ne 1 ]]; then
          echo "Expected one argument but got $#"
          exit 1
        fi

        echo "$(date -Iseconds) - starting ovn-northd"
        exec ovn-northd \
          --no-chdir \
          -vconsole:"${log_level}" \
          -vfile:off \
          -vPATTERN:console:"%D{%Y-%m-%dT%H:%M:%S.###Z}|%05N|%c%T|%p|%m" \
          --pidfile ${northd_pidfile} \
          --n-threads=1 &
        wait $!
      }

      # start-audit-log-rotation() continuously watches ovn-controller's audit
      # log directory and deletes old logs to ensure the total size of the logs
      # does not exceed a given threshold. This function does not return.
      #
      # Requires the following volume mounts:
      #   /var/log/ovn/
      #   /run/ovn/
      start-audit-log-rotation()
      {
        # Rotate audit log files when then get to max size (in bytes)
        MAXFILESIZE=$(( "50"*1000000 ))
        MAXLOGFILES="5"
        LOGDIR=$(dirname ${controller_logfile})

        # wait a bit for ovn-controller to start
        local retries=0
        while [[ 30 -gt "${retries}" ]]; do
          (( retries += 1 ))
          CONTROLLERPID=$(cat ${controller_pidfile})
          if [[ -n "${CONTROLLERPID}" ]]; then
            break
          fi
          sleep 2
        done
        if [[ -z "${CONTROLLERPID}" ]]; then
          echo "Timed out waiting for ${controller_pidfile}"
          return 1
        fi

        # Redirect err to null so no messages are shown upon rotation
        tail -F ${controller_logfile} 2> /dev/null &

        while true
        do
          # Make sure ovn-controller's logfile exists, and get current size in bytes
          if [ -f "${controller_logfile}" ]; then
            file_size=`du -b ${controller_logfile} | tr -s '\t' ' ' | cut -d' ' -f1`
          else
            ovs-appctl -t /var/run/ovn/ovn-controller.${CONTROLLERPID}.ctl vlog/reopen
            file_size=`du -b ${controller_logfile} | tr -s '\t' ' ' | cut -d' ' -f1`
          fi

          if [ $file_size -gt $MAXFILESIZE ];then
            echo "Rotating OVN ACL Log File"
            timestamp=`date '+%Y-%m-%dT%H-%M-%S'`
            mv ${controller_logfile} ${LOGDIR}/acl-audit-log.$timestamp.log
            ovs-appctl -t /run/ovn/ovn-controller.${CONTROLLERPID}.ctl vlog/reopen
            CONTROLLERPID=$(cat ${controller_pidfile})
          fi

          # Ensure total number of log files does not exceed the maximum configured from OVNPolicyAuditMaxLogFiles
          num_files=$(ls -1 ${LOGDIR}/acl-audit-log* 2>/dev/null | wc -l)
          if [ "$num_files" -gt "$MAXLOGFILES" ]; then
            num_to_delete=$(( num_files - ${MAXLOGFILES} ))
            ls -1t ${LOGDIR}/acl-audit-log* 2>/dev/null | tail -$num_to_delete | xargs -I {} rm {}
          fi

          # sleep for 30 seconds to avoid wasting CPU
          sleep 30
        done
      }

      wait-for-certs()
      {
        local detail=$1
        local privkey=$2
        local clientcert=$3

        if [[ $# -ne 3 ]]; then
          echo "Expected three arguments but got $#"
          exit 1
        fi

        retries=0
        TS=$(date +%s)
        WARN_TS=$(( ${TS} + $(( 20 * 60)) ))
        HAS_LOGGED_INFO=0
        while [[ ! -f "${privkey}" ||  ! -f "${clientcert}" ]] ; do
          CUR_TS=$(date +%s)
          if [[ "${CUR_TS}" -gt "WARN_TS"  ]]; then
            echo "$(date -Iseconds) WARN: ${detail} certs not mounted after 20 minutes."
          elif [[ "${HAS_LOGGED_INFO}" -eq 0 ]] ; then
            echo "$(date -Iseconds) INFO: ${detail} certs not mounted. Waiting one hour."
            HAS_LOGGED_INFO=1
          fi
          sleep 5
        done
      }

      # start-rbac-proxy() starts the kube-rbac-proxy to expose ovnkube metrics to
      # Prometheus on the given listen_port, proxying from upstream_port. This
      # function does not return.
      #
      # Requires the following volume mounts:
      #   /etc/pki/tls/metrics-cert
      start-rbac-proxy-node()
      {
        local detail=$1
        local listen_port=$2
        local upstream_port=$3
        local privkey=$4
        local clientcert=$5

        if [[ $# -ne 5 ]]; then
          echo "Expected five arguments but got $#"
          exit 1
        fi

        # As the secret mount is optional we must wait for the files to be present.
        # The service is created in monitor.yaml and this is created in sdn.yaml.
        # If it isn't created there is probably an issue so we want to crashloop.
        echo "$(date -Iseconds) INFO: waiting for ${detail} certs to be mounted"
        wait-for-certs "${detail}" "${privkey}" "${clientcert}"

        echo "$(date -Iseconds) INFO: ${detail} certs mounted, starting kube-rbac-proxy"
        exec /usr/bin/kube-rbac-proxy \
          --logtostderr \
          --secure-listen-address=:${listen_port} \
          --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256 \
          --upstream=http://127.0.0.1:${upstream_port}/ \
          --tls-private-key-file=${privkey} \
          --tls-cert-file=${clientcert}
      }

      # quit-nbdb() will cleanly shut down the northbound dbserver. It is intended
      # to be run from a bash 'trap' like so:
      #
      #    trap quit-nbdb TERM INT
      quit-nbdb()
      {
        echo "$(date -Iseconds) - stopping nbdb"
        /usr/share/ovn/scripts/ovn-ctl stop_nb_ovsdb
        echo "$(date -Iseconds) - nbdb stopped"
        rm -f ${nbdb_pidfile}
        exit 0
      }

      # start-nbdb() starts the OVN northbound database. This function does not
      # return.
      #
      # Requires the following volume mounts:
      #   /etc/ovn
      #   /var/log/ovn
      #   /run/ovn/
      start-nbdb()
      {
        local log_level=$1

        if [[ $# -ne 1 ]]; then
          echo "Expected one argument but got $#"
          exit 1
        fi

        exec /usr/share/ovn/scripts/ovn-ctl \
          --no-monitor \
          --db-nb-sock=${nbdb_sock} \
          --ovn-nb-log="-vconsole:${log_level} -vfile:off -vPATTERN:console:%D{%Y-%m-%dT%H:%M:%S.###Z}|%05N|%c%T|%p|%m" \
          run_nb_ovsdb &
        wait $!
      }

      # retry() an operation a number of times, sleeping 2 seconds between each try
      retry() {
        local tries=${1}
        local desc=${2}
        local cmd=${3}

        local retries=0
        while ! ${cmd}; do
          (( retries += 1 ))
          if [[ "${retries}" -gt ${tries} ]]; then
            echo "$(date -Iseconds) - ERROR - ${desc} - too many failed attempts, giving up"
            return 1
          fi
          echo "$(date -Iseconds) - WARN - ${desc} - failed try ${retries}, retrying..."
          sleep 2
        done
        echo "$(date -Iseconds) - INFO - ${desc} - success"
        return 0
      }

      # nbdb-post-start() tweaks nbdb database server settings and sets a number
      # of options in NB_Globals to configure OVN global settings
      nbdb-post-start()
      {
        local northd_probe_interval=${1:-10000}

        rm -f ${nbdb_pidfile}

        # set inactivity probe
        if ! retry 60 "inactivity-probe" "ovn-nbctl -t 5 --inactivity-probe=60000 set-connection punix:${nbdb_sock}"; then
          exit 1
        fi

        # set IC zone
        echo "Setting the IC zone to ${K8S_NODE}"
        IC_OPTION="name=\"${K8S_NODE}\" options:name=\"${K8S_NODE}\""

        # northd probe interval
        echo "Setting northd probe interval to ${northd_probe_interval} ms"
        NORTHD_PROBE_OPTION="options:northd_probe_interval=${northd_probe_interval}"

        # let northd sleep so it takes less CPU
        NORTHD_SLEEP_OPTION="options:northd-backoff-interval-ms=300"

        local ipsec=false
        local ipsec_encapsulation=false

        IPSEC_OPTION="ipsec=${ipsec} options:ipsec_encapsulation=${ipsec_encapsulation}"

        # set all the NB_GLOBAL options
        if ! retry 20 "nb-global options" "ovn-nbctl -t 5 set nb_global . ${IC_OPTION} ${NORTHD_PROBE_OPTION} ${NORTHD_SLEEP_OPTION} ${IPSEC_OPTION}"; then
          exit 1
        fi
      }

      # ovndb-readiness-probe() checks if the the database is in the active state
      # and if not, exits with an error code.
      ovndb-readiness-probe()
      {
        # dbname should be 'sb' or 'nb'
        local dbname=$1

        if [[ $# -ne 1 ]]; then
          echo "Expected one argument but got $#"
          exit 1
        fi

        local ctlfile
        if [[ "${dbname}" = "nb" ]]; then
          ctlfile=${nbdb_ctl}
        elif [[ "${dbname}" = "sb" ]]; then
          ctlfile=${sbdb_ctl}
        else
          echo "unknown DB name ${dbname}"
          exit 1
        fi

        status=$(/usr/bin/ovn-appctl -t ${ctlfile} --timeout=3 ovsdb-server/sync-status  2>/dev/null | { grep "state: active" || false; })
        if [[ -z "${status}" ]]; then
          echo "${dbname} DB is not running or active."
          exit 1
        fi
      }

      # quit-sbdb() will cleanly shut down the southbound dbserver. It is intended
      # to be run from a bash 'trap' like so:
      #
      #    trap quit-sbdb TERM INT
      quit-sbdb()
      {
        echo "$(date -Iseconds) - stopping sbdb"
        /usr/share/ovn/scripts/ovn-ctl stop_sb_ovsdb
        echo "$(date -Iseconds) - sbdb stopped"
        rm -f ${sbdb_pidfile}
        exit 0
      }

      # start-sbdb() starts the OVN southbound database. This function does not
      # return.
      #
      # Requires the following volume mounts:
      #   /etc/ovn
      #   /var/log/ovn
      #   /run/ovn/
      start-sbdb()
      {
        local log_level=$1

        if [[ $# -ne 1 ]]; then
          echo "Expected one argument but got $#"
          exit 1
        fi

        exec /usr/share/ovn/scripts/ovn-ctl \
          --no-monitor \
          --db-sb-sock=${sbdb_sock} \
          --ovn-sb-log="-vconsole:${log_level} -vfile:off -vPATTERN:console:%D{%Y-%m-%dT%H:%M:%S.###Z}|%05N|%c%T|%p|%m" \
          run_sb_ovsdb &
        wait $!
      }

      # sbdb-post-start() tweaks sbdb database server settings
      sbdb-post-start()
      {
        rm -f ${sbdb_pidfile}

        # set inactivity probe
        if ! retry 60 "inactivity-probe" "ovn-sbctl -t 5 --inactivity-probe=180000 set-connection punix:${sbdb_sock}"; then
          exit 1
        fi
      }

      function log()
      {
          echo "$(date --iso-8601=seconds) [{$1}] ${2}"
      }

      # cni-bin-copy() detects the host OS and copies the correct shim binary to
      # the CNI binary directory.
      #
      # Requires the following volume mounts:
      #   /host
      #   /cni-bin-dir
      cni-bin-copy()
      {
        # collect host os information
        . /host/etc/os-release
        rhelmajor=
        # detect which version we're using in order to copy the proper binaries
        case "${ID}" in
          rhcos|scos)
            RHEL_VERSION=$(echo "${CPE_NAME}" | cut -f 5 -d :)
            rhelmajor=$(echo $RHEL_VERSION | sed -E 's/([0-9]+)\.{1}[0-9]+(\.[0-9]+)?/\1/')
          ;;
          rhel) rhelmajor=$(echo "${VERSION_ID}" | cut -f 1 -d .)
          ;;
          fedora)
            if [ "${VARIANT_ID}" == "coreos" ]; then
              rhelmajor=8
            else
              log "cnibincopy" "FATAL ERROR: Unsupported Fedora variant=${VARIANT_ID}"
              exit 1
            fi
          ;;
          *) log "cnibincopy" "FATAL ERROR: Unsupported OS ID=${ID}"; exit 1
          ;;
        esac

        # Set which directory we'll copy from, detect if it exists
        sourcedir=/usr/libexec/cni/
        case "${rhelmajor}" in
          8)
            sourcedir=/usr/libexec/cni/rhel8
          ;;
          9)
            sourcedir=/usr/libexec/cni/rhel9
          ;;
          *)
            log "cnibincopy" "ERROR: RHEL Major Version Unsupported, rhelmajor=${rhelmajor}"
          ;;
        esac

        cp -f "$sourcedir/ovn-k8s-cni-overlay" /cni-bin-dir/
      }

      # start-ovnkube-node starts the ovnkube-node process. This function does not
      # return.
      start-ovnkube-node()
      {
        local log_level=$1
        local metrics_port=$2
        local ovn_metrics_port=$3

        if [[ $# -ne 3 ]]; then
          echo "Expected three arguments but got $#"
          exit 1
        fi

        # copy the right CNI shim for the host OS
        cni-bin-copy

        echo "I$(date "+%m%d %H:%M:%S.%N") - disable conntrack on geneve port"
        iptables -t raw -A PREROUTING -p udp --dport 6081 -j NOTRACK
        iptables -t raw -A OUTPUT -p udp --dport 6081 -j NOTRACK
        ip6tables -t raw -A PREROUTING -p udp --dport 6081 -j NOTRACK
        ip6tables -t raw -A OUTPUT -p udp --dport 6081 -j NOTRACK

        echo "I$(date "+%m%d %H:%M:%S.%N") - starting ovnkube-node"

        if [ "shared" == "shared" ]; then
          gateway_mode_flags="--gateway-mode shared --gateway-interface br-ex"
        elif [ "shared" == "local" ]; then
          gateway_mode_flags="--gateway-mode local --gateway-interface br-ex"
        else
          echo "Invalid OVN_GATEWAY_MODE: \"shared\". Must be \"local\" or \"shared\"."
          exit 1
        fi

        export_network_flows_flags=
        if [[ -n "${NETFLOW_COLLECTORS}" ]] ; then
          export_network_flows_flags="--netflow-targets ${NETFLOW_COLLECTORS}"
        fi
        if [[ -n "${SFLOW_COLLECTORS}" ]] ; then
          export_network_flows_flags="$export_network_flows_flags --sflow-targets ${SFLOW_COLLECTORS}"
        fi
        if [[ -n "${IPFIX_COLLECTORS}" ]] ; then
          export_network_flows_flags="$export_network_flows_flags --ipfix-targets ${IPFIX_COLLECTORS}"
        fi
        if [[ -n "${IPFIX_CACHE_MAX_FLOWS}" ]] ; then
          export_network_flows_flags="$export_network_flows_flags --ipfix-cache-max-flows ${IPFIX_CACHE_MAX_FLOWS}"
        fi
        if [[ -n "${IPFIX_CACHE_ACTIVE_TIMEOUT}" ]] ; then
          export_network_flows_flags="$export_network_flows_flags --ipfix-cache-active-timeout ${IPFIX_CACHE_ACTIVE_TIMEOUT}"
        fi
        if [[ -n "${IPFIX_SAMPLING}" ]] ; then
          export_network_flows_flags="$export_network_flows_flags --ipfix-sampling ${IPFIX_SAMPLING}"
        fi
        gw_interface_flag=
        # if br-ex1 is configured on the node, we want to use it for external gateway traffic
        if [ -d /sys/class/net/br-ex1 ]; then
          gw_interface_flag="--exgw-interface=br-ex1"
        fi

        node_mgmt_port_netdev_flags=
        if [[ -n "${OVNKUBE_NODE_MGMT_PORT_NETDEV}" ]] ; then
          node_mgmt_port_netdev_flags="--ovnkube-node-mgmt-port-netdev ${OVNKUBE_NODE_MGMT_PORT_NETDEV}"
        fi
        if [[ -n "${OVNKUBE_NODE_MGMT_PORT_DP_RESOURCE_NAME}" ]] ; then
          node_mgmt_port_netdev_flags="$node_mgmt_port_netdev_flags --ovnkube-node-mgmt-port-dp-resource-name ${OVNKUBE_NODE_MGMT_PORT_DP_RESOURCE_NAME}"
        fi

        multi_network_enabled_flag=
        if [[ "true" == "true" ]]; then
          multi_network_enabled_flag="--enable-multi-network"
        fi

        multi_network_policy_enabled_flag=
        if [[ "false" == "true" ]]; then
          multi_network_policy_enabled_flag="--enable-multi-networkpolicy"
        fi

        admin_network_policy_enabled_flag=
        if [[ "true" == "true" ]]; then
          admin_network_policy_enabled_flag="--enable-admin-network-policy"
        fi

        dns_name_resolver_enabled_flag=
        if [[ "false" == "true" ]]; then
          dns_name_resolver_enabled_flag="--enable-dns-name-resolver"
        fi

        # If IP Forwarding mode is global set it in the host here.
        ip_forwarding_flag=
        if [ "" == "Global" ]; then
          sysctl -w net.ipv4.ip_forward=1
          sysctl -w net.ipv6.conf.all.forwarding=1
        else
          ip_forwarding_flag="--disable-forwarding"
        fi

        NETWORK_NODE_IDENTITY_ENABLE=
        if [[ "true" == "true" ]]; then
          NETWORK_NODE_IDENTITY_ENABLE="
            --bootstrap-kubeconfig=/var/lib/kubelet/kubeconfig
            --cert-dir=/etc/ovn/ovnkube-node-certs
            --cert-duration=24h
          "
        fi

        ovn_v4_join_subnet_opt=
        if [[ "" != "" ]]; then
          ovn_v4_join_subnet_opt="--gateway-v4-join-subnet "
        fi
        ovn_v6_join_subnet_opt=
        if [[ "" != "" ]]; then
          ovn_v6_join_subnet_opt="--gateway-v6-join-subnet "
        fi

        exec /usr/bin/ovnkube \
          --init-ovnkube-controller "${K8S_NODE}" \
          --init-node "${K8S_NODE}" \
          --config-file=/run/ovnkube-config/ovnkube.conf \
          --ovn-empty-lb-events \
          --loglevel "${log_level}" \
          --inactivity-probe="${OVN_CONTROLLER_INACTIVITY_PROBE}" \
          ${gateway_mode_flags} \
          ${node_mgmt_port_netdev_flags} \
          --metrics-bind-address "127.0.0.1:${metrics_port}" \
          --ovn-metrics-bind-address "127.0.0.1:${ovn_metrics_port}" \
          --metrics-enable-pprof \
          --metrics-enable-config-duration \
          --export-ovs-metrics \
          --disable-snat-multiple-gws \
          ${export_network_flows_flags} \
          ${multi_network_enabled_flag} \
          ${multi_network_policy_enabled_flag} \
          ${admin_network_policy_enabled_flag} \
          ${dns_name_resolver_enabled_flag} \
          --enable-multicast \
          --zone ${K8S_NODE} \
          --enable-interconnect \
          --acl-logging-rate-limit "20" \
          ${gw_interface_flag} \
          ${ip_forwarding_flag} \
          ${NETWORK_NODE_IDENTITY_ENABLE} \
          ${ovn_v4_join_subnet_opt} \
          ${ovn_v6_join_subnet_opt}
      }
  kind: ConfigMap
  metadata:
    annotations:
      kubernetes.io/description: |
        This is a script used by the ovn-kubernetes daemonset
      release.openshift.io/version: 4.16.0-0.nightly-2024-06-10-211334
    creationTimestamp: "2024-06-11T10:47:03Z"
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:data:
          f:ovnkube-lib.sh: {}
        f:metadata:
          f:annotations:
            f:kubernetes.io/description: {}
            f:release.openshift.io/version: {}
          f:ownerReferences:
            k:{"uid":"62ee1ce2-90a7-43aa-95fc-751c249264a6"}: {}
      manager: cluster-network-operator/operconfig
      operation: Apply
      time: "2024-06-11T10:47:29Z"
    name: ovnkube-script-lib
    namespace: openshift-ovn-kubernetes
    ownerReferences:
    - apiVersion: operator.openshift.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: Network
      name: cluster
      uid: 62ee1ce2-90a7-43aa-95fc-751c249264a6
    resourceVersion: "5286"
    uid: 40b94bef-2a9a-4454-85cc-27191ce67a23
- apiVersion: v1
  data:
    ca-bundle.crt: |
      -----BEGIN CERTIFICATE-----
      MIIDVTCCAj2gAwIBAgIIIM4tDed/0DMwDQYJKoZIhvcNAQELBQAwODE2MDQGA1UE
      Awwtb3BlbnNoaWZ0LW92bi1rdWJlcm5ldGVzX3NpZ25lci1jYUAxNzE4MTAyODIy
      MB4XDTI0MDYxMTEwNDcwMloXDTM0MDYwOTEwNDcwM1owODE2MDQGA1UEAwwtb3Bl
      bnNoaWZ0LW92bi1rdWJlcm5ldGVzX3NpZ25lci1jYUAxNzE4MTAyODIyMIIBIjAN
      BgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA6ILA95Bkqw9NX4FBfmWYozoKJYHB
      1K+QDbcDp3pWlHT/nHulWM/evCO+njPWdirZvnjM9v3RAQbxNIBMXzt74r4S6Ygf
      pkCI3ZVxt7AdX8d8C9CFrNRuynUdfWCcubNllhDoX44cIVbFA+vm9OFFMVSBq+46
      CPpvrAs1WrDPXzqXHrHNnMpGcTSkM/hZQRYwTP/nLUmdHohNuGqXpkUiFWwnxY5J
      JehT1BLa/mjmyMehoMTFDpYIZ0jmy1oUFjzhOQ8U6RbrXXfwmB7cOZ9k+kAmhBnG
      D9YurkuJVPcRcwsxIQWhFYBdEo1hPdzhJYJjWWmKq0a30hEhiOuVpw30qwIDAQAB
      o2MwYTAOBgNVHQ8BAf8EBAMCAqQwDwYDVR0TAQH/BAUwAwEB/zAdBgNVHQ4EFgQU
      /qC0HYD4QxRcOwtDZiYjY67flT8wHwYDVR0jBBgwFoAU/qC0HYD4QxRcOwtDZiYj
      Y67flT8wDQYJKoZIhvcNAQELBQADggEBAOU5SiMJE6lClmrYbTWZ1NsJeDfhfEqo
      Oo00LpJXnPaF9LiSIzfiog8H21F0E7ExXVZkVG5XIdNuybUiabqt0JPbvKLiAEU0
      QvGJJAYM2LsjUmFuJIcw71l109E6a3iQskvc4np3SffQJGtMTyJg7Mwg8IymH3RK
      dC8J28ANBIjDjCqYj0DWfaq/8vDybmXO3It2pD18Jeof0aiGw6xPbM/RwyYgme9t
      xM5Qagi3jISJ9fQQTioRJCJ7zt8L42Onb0e1jJf6BDmlVQR6BBjEJSpsOdPy0Uqw
      tAWG97S67RPdOdyTWGP744RBN32g1d8nDMMjvFHQwPCtRRs+5wR4+Mo=
      -----END CERTIFICATE-----
  kind: ConfigMap
  metadata:
    annotations:
      openshift.io/description: ""
      openshift.io/owning-component: Networking / cluster-network-operator
    creationTimestamp: "2024-06-11T10:47:03Z"
    labels:
      auth.openshift.io/managed-certificate-type: ca-bundle
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:data:
          .: {}
          f:ca-bundle.crt: {}
        f:metadata:
          f:annotations:
            .: {}
            f:openshift.io/description: {}
            f:openshift.io/owning-component: {}
          f:labels:
            .: {}
            f:auth.openshift.io/managed-certificate-type: {}
      manager: network-operator
      operation: Update
      time: "2024-06-11T10:47:03Z"
    name: signer-ca
    namespace: openshift-ovn-kubernetes
    resourceVersion: "4746"
    uid: f0583330-6609-4648-8e53-5c21ae0391dc
kind: ConfigMapList
metadata:
  resourceVersion: "40139"
