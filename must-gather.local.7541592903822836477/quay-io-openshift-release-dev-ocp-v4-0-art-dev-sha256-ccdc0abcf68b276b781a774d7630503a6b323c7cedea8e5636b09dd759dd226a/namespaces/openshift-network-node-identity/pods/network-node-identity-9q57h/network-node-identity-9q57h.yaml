---
apiVersion: v1
kind: Pod
metadata:
  annotations:
    openshift.io/scc: hostnetwork-v2
    seccomp.security.alpha.kubernetes.io/pod: runtime/default
  creationTimestamp: "2024-06-11T10:47:10Z"
  generateName: network-node-identity-
  labels:
    app: network-node-identity
    component: network
    controller-revision-hash: 9d576f5f9
    kubernetes.io/os: linux
    openshift.io/component: network
    pod-template-generation: "1"
    type: infra
  managedFields:
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .: {}
          f:target.workload.openshift.io/management: {}
        f:generateName: {}
        f:labels:
          .: {}
          f:app: {}
          f:component: {}
          f:controller-revision-hash: {}
          f:kubernetes.io/os: {}
          f:openshift.io/component: {}
          f:pod-template-generation: {}
          f:type: {}
        f:ownerReferences:
          .: {}
          k:{"uid":"b875a6a5-e8c0-40a3-ab61-b17d9bcb02ac"}: {}
      f:spec:
        f:affinity:
          .: {}
          f:nodeAffinity:
            .: {}
            f:requiredDuringSchedulingIgnoredDuringExecution: {}
        f:containers:
          k:{"name":"approver"}:
            .: {}
            f:command: {}
            f:env:
              .: {}
              k:{"name":"LOGLEVEL"}:
                .: {}
                f:name: {}
                f:value: {}
            f:image: {}
            f:imagePullPolicy: {}
            f:name: {}
            f:resources:
              .: {}
              f:requests:
                .: {}
                f:cpu: {}
                f:memory: {}
            f:terminationMessagePath: {}
            f:terminationMessagePolicy: {}
            f:volumeMounts:
              .: {}
              k:{"mountPath":"/env"}:
                .: {}
                f:mountPath: {}
                f:name: {}
              k:{"mountPath":"/var/run/ovnkube-identity-config"}:
                .: {}
                f:mountPath: {}
                f:name: {}
          k:{"name":"webhook"}:
            .: {}
            f:command: {}
            f:env:
              .: {}
              k:{"name":"KUBERNETES_NODE_NAME"}:
                .: {}
                f:name: {}
                f:valueFrom:
                  .: {}
                  f:fieldRef: {}
              k:{"name":"LOGLEVEL"}:
                .: {}
                f:name: {}
                f:value: {}
            f:image: {}
            f:imagePullPolicy: {}
            f:name: {}
            f:resources:
              .: {}
              f:requests:
                .: {}
                f:cpu: {}
                f:memory: {}
            f:terminationMessagePath: {}
            f:terminationMessagePolicy: {}
            f:volumeMounts:
              .: {}
              k:{"mountPath":"/env"}:
                .: {}
                f:mountPath: {}
                f:name: {}
              k:{"mountPath":"/etc/webhook-cert/"}:
                .: {}
                f:mountPath: {}
                f:name: {}
              k:{"mountPath":"/var/run/ovnkube-identity-config"}:
                .: {}
                f:mountPath: {}
                f:name: {}
        f:dnsPolicy: {}
        f:enableServiceLinks: {}
        f:hostNetwork: {}
        f:nodeSelector: {}
        f:priorityClassName: {}
        f:restartPolicy: {}
        f:schedulerName: {}
        f:securityContext: {}
        f:serviceAccount: {}
        f:serviceAccountName: {}
        f:terminationGracePeriodSeconds: {}
        f:tolerations: {}
        f:volumes:
          .: {}
          k:{"name":"env-overrides"}:
            .: {}
            f:configMap:
              .: {}
              f:defaultMode: {}
              f:name: {}
              f:optional: {}
            f:name: {}
          k:{"name":"ovnkube-identity-cm"}:
            .: {}
            f:configMap:
              .: {}
              f:defaultMode: {}
              f:items: {}
              f:name: {}
            f:name: {}
          k:{"name":"webhook-cert"}:
            .: {}
            f:name: {}
            f:secret:
              .: {}
              f:defaultMode: {}
              f:secretName: {}
    manager: kube-controller-manager
    operation: Update
    time: "2024-06-11T10:47:10Z"
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:status:
        f:conditions:
          k:{"type":"ContainersReady"}:
            .: {}
            f:lastProbeTime: {}
            f:lastTransitionTime: {}
            f:status: {}
            f:type: {}
          k:{"type":"Initialized"}:
            .: {}
            f:lastProbeTime: {}
            f:lastTransitionTime: {}
            f:status: {}
            f:type: {}
          k:{"type":"PodReadyToStartContainers"}:
            .: {}
            f:lastProbeTime: {}
            f:lastTransitionTime: {}
            f:status: {}
            f:type: {}
          k:{"type":"Ready"}:
            .: {}
            f:lastProbeTime: {}
            f:lastTransitionTime: {}
            f:status: {}
            f:type: {}
        f:containerStatuses: {}
        f:hostIP: {}
        f:hostIPs: {}
        f:phase: {}
        f:podIP: {}
        f:podIPs:
          .: {}
          k:{"ip":"10.0.0.6"}:
            .: {}
            f:ip: {}
        f:startTime: {}
    manager: kubelet
    operation: Update
    subresource: status
    time: "2024-06-11T10:59:27Z"
  name: network-node-identity-9q57h
  namespace: openshift-network-node-identity
  ownerReferences:
  - apiVersion: apps/v1
    blockOwnerDeletion: true
    controller: true
    kind: DaemonSet
    name: network-node-identity
    uid: b875a6a5-e8c0-40a3-ab61-b17d9bcb02ac
  resourceVersion: "20559"
  uid: 4ae2eeb6-be06-41f6-b2d1-91b0f04b74aa
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchFields:
          - key: metadata.name
            operator: In
            values:
            - ci-op-9xx71rvq-1e28e-w667k-master-1
  containers:
  - command:
    - /bin/bash
    - -c
    - |
      set -xe
      if [[ -f "/env/_master" ]]; then
        set -o allexport
        source "/env/_master"
        set +o allexport
      fi
      # OVN-K will try to remove hybrid overlay node annotations even when the hybrid overlay is not enabled.
      # https://github.com/ovn-org/ovn-kubernetes/blob/ac6820df0b338a246f10f412cd5ec903bd234694/go-controller/pkg/ovn/master.go#L791
      ho_enable="--enable-hybrid-overlay"
      echo "I$(date "+%m%d %H:%M:%S.%N") - network-node-identity - start webhook"
      # extra-allowed-user: service account `ovn-kubernetes-control-plane`
      # sets pod annotations in multi-homing layer3 network controller (cluster-manager)
      exec /usr/bin/ovnkube-identity  --k8s-apiserver=https://api-int.ci-op-9xx71rvq-1e28e.qe.azure.devcluster.openshift.com:6443 \
          --webhook-cert-dir="/etc/webhook-cert" \
          --webhook-host=127.0.0.1 \
          --webhook-port=9743 \
          ${ho_enable} \
          --enable-interconnect \
          --disable-approver \
          --extra-allowed-user="system:serviceaccount:openshift-ovn-kubernetes:ovn-kubernetes-control-plane" \
          --wait-for-kubernetes-api=200s \
          --pod-admission-conditions="/var/run/ovnkube-identity-config/additional-pod-admission-cond.json" \
          --loglevel="${LOGLEVEL}"
    env:
    - name: LOGLEVEL
      value: "2"
    - name: KUBERNETES_NODE_NAME
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: spec.nodeName
    image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f51f47793a3eda34f600e1e7eab027bc309b914eb8ea948765cf1a03549b34e4
    imagePullPolicy: IfNotPresent
    name: webhook
    resources:
      requests:
        cpu: 10m
        memory: 50Mi
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      runAsNonRoot: true
      runAsUser: 1000540000
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: FallbackToLogsOnError
    volumeMounts:
    - mountPath: /etc/webhook-cert/
      name: webhook-cert
    - mountPath: /env
      name: env-overrides
    - mountPath: /var/run/ovnkube-identity-config
      name: ovnkube-identity-cm
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-5hmpq
      readOnly: true
  - command:
    - /bin/bash
    - -c
    - |
      set -xe
      if [[ -f "/env/_master" ]]; then
        set -o allexport
        source "/env/_master"
        set +o allexport
      fi

      echo "I$(date "+%m%d %H:%M:%S.%N") - network-node-identity - start approver"
      exec /usr/bin/ovnkube-identity  --k8s-apiserver=https://api-int.ci-op-9xx71rvq-1e28e.qe.azure.devcluster.openshift.com:6443 \
          --disable-webhook \
          --csr-acceptance-conditions="/var/run/ovnkube-identity-config/additional-cert-acceptance-cond.json" \
          --loglevel="${LOGLEVEL}"
    env:
    - name: LOGLEVEL
      value: "4"
    image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f51f47793a3eda34f600e1e7eab027bc309b914eb8ea948765cf1a03549b34e4
    imagePullPolicy: IfNotPresent
    name: approver
    resources:
      requests:
        cpu: 10m
        memory: 50Mi
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      runAsNonRoot: true
      runAsUser: 1000540000
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: FallbackToLogsOnError
    volumeMounts:
    - mountPath: /env
      name: env-overrides
    - mountPath: /var/run/ovnkube-identity-config
      name: ovnkube-identity-cm
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-5hmpq
      readOnly: true
  dnsPolicy: Default
  enableServiceLinks: true
  hostNetwork: true
  nodeName: ci-op-9xx71rvq-1e28e-w667k-master-1
  nodeSelector:
    beta.kubernetes.io/os: linux
    node-role.kubernetes.io/master: ""
  preemptionPolicy: PreemptLowerPriority
  priority: 2000001000
  priorityClassName: system-node-critical
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext:
    fsGroup: 1000540000
    seLinuxOptions:
      level: s0:c23,c17
    seccompProfile:
      type: RuntimeDefault
    supplementalGroups:
    - 1000540000
  serviceAccount: network-node-identity
  serviceAccountName: network-node-identity
  terminationGracePeriodSeconds: 200
  tolerations:
  - operator: Exists
  volumes:
  - name: webhook-cert
    secret:
      defaultMode: 420
      secretName: network-node-identity-cert
  - configMap:
      defaultMode: 420
      name: env-overrides
      optional: true
    name: env-overrides
  - configMap:
      defaultMode: 420
      items:
      - key: additional-cert-acceptance-cond.json
        path: additional-cert-acceptance-cond.json
      - key: additional-pod-admission-cond.json
        path: additional-pod-admission-cond.json
      name: ovnkube-identity-cm
    name: ovnkube-identity-cm
  - name: kube-api-access-5hmpq
    projected:
      defaultMode: 420
      sources:
      - serviceAccountToken:
          expirationSeconds: 3607
          path: token
      - configMap:
          items:
          - key: ca.crt
            path: ca.crt
          name: kube-root-ca.crt
      - downwardAPI:
          items:
          - fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
            path: namespace
      - configMap:
          items:
          - key: service-ca.crt
            path: service-ca.crt
          name: openshift-service-ca.crt
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: "2024-06-11T10:47:27Z"
    status: "True"
    type: PodReadyToStartContainers
  - lastProbeTime: null
    lastTransitionTime: "2024-06-11T10:47:10Z"
    status: "True"
    type: Initialized
  - lastProbeTime: null
    lastTransitionTime: "2024-06-11T10:59:16Z"
    status: "True"
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: "2024-06-11T10:59:16Z"
    status: "True"
    type: ContainersReady
  - lastProbeTime: null
    lastTransitionTime: "2024-06-11T10:47:10Z"
    status: "True"
    type: PodScheduled
  containerStatuses:
  - containerID: cri-o://8437d7d373ad216d053cd820da32e7eaa748270740dfc2b4ddc396c95a36979a
    image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f51f47793a3eda34f600e1e7eab027bc309b914eb8ea948765cf1a03549b34e4
    imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f51f47793a3eda34f600e1e7eab027bc309b914eb8ea948765cf1a03549b34e4
    lastState:
      terminated:
        containerID: cri-o://de2506bd25124bd52ba84acb6269330ca53ec71cdade79fb74d78ecea8914105
        exitCode: 1
        finishedAt: "2024-06-11T10:59:14Z"
        message: |
          ertificates.k8s.io" controllerKind="CertificateSigningRequest" worker count=1
          I0611 10:57:34.842079       1 approver.go:230] Finished syncing CSR csr-h8v9s for unknown node in 27.903µs
          I0611 10:57:34.842143       1 approver.go:230] Finished syncing CSR csr-wfnnq for unknown node in 30.802µs
          I0611 10:57:34.842193       1 approver.go:230] Finished syncing CSR csr-zrlhg for unknown node in 29.302µs
          I0611 10:57:34.842290       1 approver.go:230] Finished syncing CSR csr-5drph for unknown node in 27.103µs
          I0611 10:57:34.842624       1 approver.go:230] Finished syncing CSR csr-hxp5n for unknown node in 34.403µs
          I0611 10:57:34.842716       1 approver.go:230] Finished syncing CSR csr-57zxb for unknown node in 28.603µs
          I0611 10:58:13.253872       1 reflector.go:800] sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:105: Watch close - *v1.CertificateSigningRequest total 0 items received
          I0611 10:58:44.255993       1 with_retry.go:234] Got a Retry-After 1s response for attempt 1 to https://api-int.ci-op-9xx71rvq-1e28e.qe.azure.devcluster.openshift.com:6443/apis/certificates.k8s.io/v1/certificatesigningrequests?allowWatchBookmarks=true&resourceVersion=19941&timeoutSeconds=389&watch=true
          E0611 10:58:44.768438       1 leaderelection.go:332] error retrieving resource lock openshift-network-node-identity/ovnkube-identity: Get "https://api-int.ci-op-9xx71rvq-1e28e.qe.azure.devcluster.openshift.com:6443/apis/coordination.k8s.io/v1/namespaces/openshift-network-node-identity/leases/ovnkube-identity": context deadline exceeded
          I0611 10:58:44.768482       1 leaderelection.go:285] failed to renew lease openshift-network-node-identity/ovnkube-identity: timed out waiting for the condition
          E0611 10:59:14.769210       1 leaderelection.go:308] Failed to release lock: Put "https://api-int.ci-op-9xx71rvq-1e28e.qe.azure.devcluster.openshift.com:6443/apis/coordination.k8s.io/v1/namespaces/openshift-network-node-identity/leases/ovnkube-identity": dial tcp 10.0.0.4:6443: i/o timeout
          error running approver: leader election lost
        reason: Error
        startedAt: "2024-06-11T10:47:27Z"
    name: approver
    ready: true
    restartCount: 1
    started: true
    state:
      running:
        startedAt: "2024-06-11T10:59:15Z"
  - containerID: cri-o://8591990efddf980a17e409d437fc67f242b96b52e9344961223941dbbdf67a1c
    image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f51f47793a3eda34f600e1e7eab027bc309b914eb8ea948765cf1a03549b34e4
    imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f51f47793a3eda34f600e1e7eab027bc309b914eb8ea948765cf1a03549b34e4
    lastState: {}
    name: webhook
    ready: true
    restartCount: 0
    started: true
    state:
      running:
        startedAt: "2024-06-11T10:47:27Z"
  hostIP: 10.0.0.6
  hostIPs:
  - ip: 10.0.0.6
  phase: Running
  podIP: 10.0.0.6
  podIPs:
  - ip: 10.0.0.6
  qosClass: Burstable
  startTime: "2024-06-11T10:47:10Z"
