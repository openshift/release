2024-06-11T10:49:44.741452175Z I0611 10:49:44.741234       1 leaderelection.go:122] The leader election gives 4 retries and allows for 30s of clock skew. The kube-apiserver downtime tolerance is 78s. Worst non-graceful lease acquisition is 2m43s. Worst graceful lease acquisition is {26s}.
2024-06-11T10:49:44.741697079Z I0611 10:49:44.741674       1 main.go:30] Go Version: go1.21.9 (Red Hat 1.21.9-1.el9_4) X:strictfipsruntime
2024-06-11T10:49:44.741734280Z I0611 10:49:44.741722       1 main.go:31] Go OS/Arch: linux/amd64
2024-06-11T10:49:44.741964784Z I0611 10:49:44.741945       1 leaderelection.go:122] The leader election gives 4 retries and allows for 13s of clock skew. The kube-apiserver downtime tolerance is 78s. Worst non-graceful lease acquisition is 2m26s. Worst graceful lease acquisition is {26s}.
2024-06-11T10:49:44.742068086Z I0611 10:49:44.742047       1 main.go:113] Watching machine-api objects only in namespace "openshift-machine-api" for reconciliation.
2024-06-11T10:49:44.743053404Z I0611 10:49:44.743026       1 main.go:121] Registering Components.
2024-06-11T10:49:44.770556505Z I0611 10:49:44.770491       1 main.go:144] Starting the Cmd.
2024-06-11T10:49:44.770844510Z I0611 10:49:44.770808       1 server.go:185] "Starting metrics server" logger="controller-runtime.metrics"
2024-06-11T10:49:44.770991413Z I0611 10:49:44.770968       1 server.go:224] "Serving metrics server" logger="controller-runtime.metrics" bindAddress=":8083" secure=false
2024-06-11T10:49:44.771102615Z I0611 10:49:44.771077       1 server.go:50] "starting server" kind="health probe" addr="[::]:9442"
2024-06-11T10:49:44.788308728Z I0611 10:49:44.788241       1 reflector.go:289] Starting reflector *v1beta1.Machine (9h55m14.065032824s) from sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:105
2024-06-11T10:49:44.788308728Z I0611 10:49:44.788287       1 reflector.go:325] Listing and watching *v1beta1.Machine from sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:105
2024-06-11T10:49:44.792984713Z I0611 10:49:44.792942       1 reflector.go:351] Caches populated for *v1beta1.Machine from sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:105
2024-06-11T10:49:44.871393941Z I0611 10:49:44.871348       1 leaderelection.go:250] attempting to acquire leader lease openshift-machine-api/cluster-api-provider-healthcheck-leader...
2024-06-11T10:49:44.878909678Z I0611 10:49:44.878869       1 leaderelection.go:260] successfully acquired lease openshift-machine-api/cluster-api-provider-healthcheck-leader
2024-06-11T10:49:44.879578590Z I0611 10:49:44.879080       1 recorder.go:104] "machine-api-controllers-857c68d88f-cpdp9_97692af3-da24-495a-92e9-f67702b961de became leader" logger="events" type="Normal" object={"kind":"Lease","namespace":"openshift-machine-api","name":"cluster-api-provider-healthcheck-leader","uid":"f085aec2-7efd-42e5-a175-c905fb5d58da","apiVersion":"coordination.k8s.io/v1","resourceVersion":"11608"} reason="LeaderElection"
2024-06-11T10:49:44.879830795Z I0611 10:49:44.879797       1 controller.go:178] "Starting EventSource" controller="machinehealthcheck-controller" source="kind source: *v1beta1.MachineHealthCheck"
2024-06-11T10:49:44.879882696Z I0611 10:49:44.879869       1 controller.go:178] "Starting EventSource" controller="machinehealthcheck-controller" source="kind source: *v1beta1.Machine"
2024-06-11T10:49:44.879917096Z I0611 10:49:44.879907       1 controller.go:178] "Starting EventSource" controller="machinehealthcheck-controller" source="kind source: *v1.Node"
2024-06-11T10:49:44.879952097Z I0611 10:49:44.879942       1 controller.go:186] "Starting Controller" controller="machinehealthcheck-controller"
2024-06-11T10:49:44.884514280Z I0611 10:49:44.884471       1 reflector.go:289] Starting reflector *v1.Node (10h17m35.019211355s) from sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:105
2024-06-11T10:49:44.884514280Z I0611 10:49:44.884499       1 reflector.go:325] Listing and watching *v1.Node from sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:105
2024-06-11T10:49:44.884546881Z I0611 10:49:44.884515       1 reflector.go:289] Starting reflector *v1beta1.MachineHealthCheck (9h30m16.946094959s) from sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:105
2024-06-11T10:49:44.884546881Z I0611 10:49:44.884530       1 reflector.go:325] Listing and watching *v1beta1.MachineHealthCheck from sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:105
2024-06-11T10:49:44.886680020Z I0611 10:49:44.886640       1 reflector.go:351] Caches populated for *v1.Node from sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:105
2024-06-11T10:49:44.887211529Z I0611 10:49:44.887172       1 reflector.go:351] Caches populated for *v1beta1.MachineHealthCheck from sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:105
2024-06-11T10:49:44.986036029Z I0611 10:49:44.985976       1 controller.go:220] "Starting workers" controller="machinehealthcheck-controller" worker count=1
2024-06-11T10:49:44.986398236Z W0611 10:49:44.986350       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-master-0": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-master-0, got: []
2024-06-11T10:54:30.997472901Z E0611 10:54:30.997401       1 leaderelection.go:332] error retrieving resource lock openshift-machine-api/cluster-api-provider-healthcheck-leader: Get "https://172.30.0.1:443/apis/coordination.k8s.io/v1/namespaces/openshift-machine-api/leases/cluster-api-provider-healthcheck-leader": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:54:56.999375229Z E0611 10:54:56.999315       1 leaderelection.go:332] error retrieving resource lock openshift-machine-api/cluster-api-provider-healthcheck-leader: Get "https://172.30.0.1:443/apis/coordination.k8s.io/v1/namespaces/openshift-machine-api/leases/cluster-api-provider-healthcheck-leader": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:55:22.999303310Z E0611 10:55:22.999239       1 leaderelection.go:332] error retrieving resource lock openshift-machine-api/cluster-api-provider-healthcheck-leader: Get "https://172.30.0.1:443/apis/coordination.k8s.io/v1/namespaces/openshift-machine-api/leases/cluster-api-provider-healthcheck-leader": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:55:48.999562757Z E0611 10:55:48.999490       1 leaderelection.go:332] error retrieving resource lock openshift-machine-api/cluster-api-provider-healthcheck-leader: Get "https://172.30.0.1:443/apis/coordination.k8s.io/v1/namespaces/openshift-machine-api/leases/cluster-api-provider-healthcheck-leader": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:57:03.432536088Z I0611 10:57:03.432382       1 reflector.go:325] Listing and watching *v1beta1.MachineHealthCheck from sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:105
2024-06-11T10:57:03.434195219Z I0611 10:57:03.434165       1 reflector.go:351] Caches populated for *v1beta1.MachineHealthCheck from sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:105
2024-06-11T10:57:14.359637420Z I0611 10:57:14.359588       1 reflector.go:325] Listing and watching *v1beta1.Machine from sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:105
2024-06-11T10:57:14.364823017Z I0611 10:57:14.364783       1 reflector.go:351] Caches populated for *v1beta1.Machine from sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:105
2024-06-11T10:57:24.571866364Z I0611 10:57:24.571803       1 reflector.go:325] Listing and watching *v1.Node from sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:105
2024-06-11T10:57:24.579473209Z I0611 10:57:24.578442       1 reflector.go:351] Caches populated for *v1.Node from sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:105
2024-06-11T10:57:24.580939337Z W0611 10:57:24.579663       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus1-k2hfp": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus1-k2hfp, got: []
2024-06-11T10:57:24.580939337Z W0611 10:57:24.579751       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus2-xnvk9": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus2-xnvk9, got: []
2024-06-11T10:57:24.580939337Z W0611 10:57:24.579819       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus3-hgn49": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus3-hgn49, got: []
2024-06-11T10:57:25.934616679Z W0611 10:57:25.934559       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus2-xnvk9": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus2-xnvk9, got: []
2024-06-11T10:57:25.934763282Z W0611 10:57:25.934740       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus2-xnvk9": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus2-xnvk9, got: []
2024-06-11T10:57:27.550927234Z W0611 10:57:27.550877       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus2-xnvk9": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus2-xnvk9, got: []
2024-06-11T10:57:27.550966735Z W0611 10:57:27.550934       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus2-xnvk9": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus2-xnvk9, got: []
2024-06-11T10:57:30.947573476Z W0611 10:57:30.947505       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus2-xnvk9": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus2-xnvk9, got: []
2024-06-11T10:57:30.947766479Z W0611 10:57:30.947686       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus2-xnvk9": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus2-xnvk9, got: []
2024-06-11T10:57:34.848262260Z W0611 10:57:34.848205       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus3-hgn49": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus3-hgn49, got: []
2024-06-11T10:57:34.848422663Z W0611 10:57:34.848399       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus3-hgn49": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus3-hgn49, got: []
2024-06-11T10:57:46.709629609Z W0611 10:57:46.709575       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus1-k2hfp": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus1-k2hfp, got: []
2024-06-11T10:57:46.709764711Z W0611 10:57:46.709729       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus1-k2hfp": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus1-k2hfp, got: []
2024-06-11T10:57:48.137962474Z W0611 10:57:48.137910       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus1-k2hfp": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus1-k2hfp, got: []
2024-06-11T10:57:48.138144077Z W0611 10:57:48.138097       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus1-k2hfp": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus1-k2hfp, got: []
2024-06-11T10:57:53.600685696Z W0611 10:57:53.600632       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus3-hgn49": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus3-hgn49, got: []
2024-06-11T10:57:53.600872600Z W0611 10:57:53.600849       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus3-hgn49": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus3-hgn49, got: []
2024-06-11T10:57:55.519642185Z W0611 10:57:55.519594       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus2-xnvk9": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus2-xnvk9, got: []
2024-06-11T10:57:55.519763187Z W0611 10:57:55.519740       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus2-xnvk9": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus2-xnvk9, got: []
2024-06-11T10:57:58.111929391Z W0611 10:57:58.111875       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus2-xnvk9": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus2-xnvk9, got: []
2024-06-11T10:57:58.111983292Z W0611 10:57:58.111949       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus2-xnvk9": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus2-xnvk9, got: []
2024-06-11T10:58:06.658671091Z W0611 10:58:06.658613       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus3-hgn49": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus3-hgn49, got: []
2024-06-11T10:58:06.658834994Z W0611 10:58:06.658805       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus3-hgn49": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus3-hgn49, got: []
2024-06-11T10:58:06.724740045Z W0611 10:58:06.724682       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus3-hgn49": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus3-hgn49, got: []
2024-06-11T10:58:06.724883847Z W0611 10:58:06.724851       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus3-hgn49": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus3-hgn49, got: []
2024-06-11T10:58:10.465864927Z W0611 10:58:10.465802       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus3-hgn49": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus3-hgn49, got: []
2024-06-11T10:58:10.466100831Z W0611 10:58:10.466070       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus3-hgn49": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus3-hgn49, got: []
2024-06-11T10:58:25.084167307Z E0611 10:58:25.084104       1 leaderelection.go:332] error retrieving resource lock openshift-machine-api/cluster-api-provider-healthcheck-leader: Get "https://172.30.0.1:443/apis/coordination.k8s.io/v1/namespaces/openshift-machine-api/leases/cluster-api-provider-healthcheck-leader": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:58:51.086120177Z E0611 10:58:51.086052       1 leaderelection.go:332] error retrieving resource lock openshift-machine-api/cluster-api-provider-healthcheck-leader: Get "https://172.30.0.1:443/apis/coordination.k8s.io/v1/namespaces/openshift-machine-api/leases/cluster-api-provider-healthcheck-leader": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:59:17.085861597Z E0611 10:59:17.085806       1 leaderelection.go:332] error retrieving resource lock openshift-machine-api/cluster-api-provider-healthcheck-leader: Get "https://172.30.0.1:443/apis/coordination.k8s.io/v1/namespaces/openshift-machine-api/leases/cluster-api-provider-healthcheck-leader": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:00:18.013536097Z I0611 11:00:18.013475       1 reflector.go:325] Listing and watching *v1.Node from sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:105
2024-06-11T11:00:18.016350951Z I0611 11:00:18.016317       1 reflector.go:351] Caches populated for *v1.Node from sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:105
2024-06-11T11:00:18.016925562Z W0611 11:00:18.016895       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus1-k2hfp": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus1-k2hfp, got: []
2024-06-11T11:00:18.016948662Z W0611 11:00:18.016932       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus1-k2hfp": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus1-k2hfp, got: []
2024-06-11T11:00:18.017042064Z W0611 11:00:18.016978       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus2-xnvk9": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus2-xnvk9, got: []
2024-06-11T11:00:18.017103165Z W0611 11:00:18.017051       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus2-xnvk9": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus2-xnvk9, got: []
2024-06-11T11:00:18.017119065Z W0611 11:00:18.017099       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus3-hgn49": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus3-hgn49, got: []
2024-06-11T11:00:18.017172466Z W0611 11:00:18.017146       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus3-hgn49": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus3-hgn49, got: []
2024-06-11T11:00:34.445909421Z I0611 11:00:34.445848       1 reflector.go:325] Listing and watching *v1beta1.MachineHealthCheck from sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:105
2024-06-11T11:00:34.447697554Z I0611 11:00:34.447657       1 reflector.go:351] Caches populated for *v1beta1.MachineHealthCheck from sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:105
2024-06-11T11:00:40.131202976Z I0611 11:00:40.131152       1 reflector.go:325] Listing and watching *v1beta1.Machine from sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:105
2024-06-11T11:00:40.137362692Z I0611 11:00:40.137326       1 reflector.go:351] Caches populated for *v1beta1.Machine from sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:105
2024-06-11T11:00:44.040886760Z W0611 11:00:44.040837       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus2-xnvk9": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus2-xnvk9, got: []
2024-06-11T11:00:44.041093264Z W0611 11:00:44.041044       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus2-xnvk9": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus2-xnvk9, got: []
2024-06-11T11:00:44.073726779Z W0611 11:00:44.073583       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus1-k2hfp": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus1-k2hfp, got: []
2024-06-11T11:00:44.073726779Z W0611 11:00:44.073649       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus1-k2hfp": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus1-k2hfp, got: []
2024-06-11T11:00:44.747166970Z W0611 11:00:44.747113       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus3-hgn49": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus3-hgn49, got: []
2024-06-11T11:00:44.747298472Z W0611 11:00:44.747271       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus3-hgn49": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus3-hgn49, got: []
2024-06-11T11:00:45.135594190Z W0611 11:00:45.134178       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus2-xnvk9": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus2-xnvk9, got: []
2024-06-11T11:00:45.135594190Z W0611 11:00:45.134251       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus2-xnvk9": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus2-xnvk9, got: []
2024-06-11T11:00:45.171591768Z W0611 11:00:45.171539       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus2-xnvk9": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus2-xnvk9, got: []
2024-06-11T11:00:45.171746071Z W0611 11:00:45.171715       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus2-xnvk9": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus2-xnvk9, got: []
2024-06-11T11:00:45.205348804Z W0611 11:00:45.205293       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus2-xnvk9": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus2-xnvk9, got: []
2024-06-11T11:00:45.205412606Z W0611 11:00:45.205351       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus2-xnvk9": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus2-xnvk9, got: []
2024-06-11T11:00:45.219878078Z W0611 11:00:45.219822       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus1-k2hfp": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus1-k2hfp, got: []
2024-06-11T11:00:45.220127183Z W0611 11:00:45.220094       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus1-k2hfp": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus1-k2hfp, got: []
2024-06-11T11:00:45.245688865Z W0611 11:00:45.245629       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus1-k2hfp": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus1-k2hfp, got: []
2024-06-11T11:00:45.245881168Z W0611 11:00:45.245691       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus1-k2hfp": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus1-k2hfp, got: []
2024-06-11T11:00:45.279435301Z W0611 11:00:45.279382       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus1-k2hfp": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus1-k2hfp, got: []
2024-06-11T11:00:45.279598104Z W0611 11:00:45.279549       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus1-k2hfp": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus1-k2hfp, got: []
2024-06-11T11:00:45.699102309Z W0611 11:00:45.699052       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus2-xnvk9": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus2-xnvk9, got: []
2024-06-11T11:00:45.699304113Z W0611 11:00:45.699274       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus2-xnvk9": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus2-xnvk9, got: []
2024-06-11T11:00:45.787200870Z W0611 11:00:45.787148       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus1-k2hfp": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus1-k2hfp, got: []
2024-06-11T11:00:45.787374573Z W0611 11:00:45.787347       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus1-k2hfp": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus1-k2hfp, got: []
2024-06-11T11:00:45.803146070Z W0611 11:00:45.803098       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus3-hgn49": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus3-hgn49, got: []
2024-06-11T11:00:45.803231272Z W0611 11:00:45.803211       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus3-hgn49": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus3-hgn49, got: []
2024-06-11T11:00:45.822850141Z W0611 11:00:45.822784       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus3-hgn49": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus3-hgn49, got: []
2024-06-11T11:00:45.823092346Z W0611 11:00:45.823055       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus3-hgn49": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus3-hgn49, got: []
2024-06-11T11:00:45.866994873Z W0611 11:00:45.866952       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus3-hgn49": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus3-hgn49, got: []
2024-06-11T11:00:45.867191377Z W0611 11:00:45.867153       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus3-hgn49": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus3-hgn49, got: []
2024-06-11T11:00:46.394345011Z W0611 11:00:46.394289       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus3-hgn49": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus3-hgn49, got: []
2024-06-11T11:00:46.394591316Z W0611 11:00:46.394560       1 machinehealthcheck_controller.go:578] No-op: Unable to retrieve machine from node "/ci-op-9xx71rvq-1e28e-w667k-worker-centralus3-hgn49": expecting one machine for node ci-op-9xx71rvq-1e28e-w667k-worker-centralus3-hgn49, got: []
2024-06-11T11:00:54.530541801Z I0611 11:00:54.530474       1 machinehealthcheck_controller.go:162] Reconciling openshift-machine-api/machine-api-termination-handler
2024-06-11T11:00:54.530595002Z I0611 11:00:54.530554       1 machinehealthcheck_controller.go:188] Reconciling openshift-machine-api/machine-api-termination-handler: finding targets
2024-06-11T11:00:54.530764705Z I0611 11:00:54.530670       1 machinehealthcheck_controller.go:251] Remediations are allowed for openshift-machine-api/machine-api-termination-handler: total targets: 0,  max unhealthy: 100%, unhealthy targets: 0
2024-06-11T11:00:54.547310616Z I0611 11:00:54.547253       1 machinehealthcheck_controller.go:279] Reconciling openshift-machine-api/machine-api-termination-handler: no more targets meet unhealthy criteria
2024-06-11T11:00:54.547356917Z I0611 11:00:54.547319       1 machinehealthcheck_controller.go:162] Reconciling openshift-machine-api/machine-api-termination-handler
2024-06-11T11:00:54.547371318Z I0611 11:00:54.547362       1 machinehealthcheck_controller.go:188] Reconciling openshift-machine-api/machine-api-termination-handler: finding targets
2024-06-11T11:00:54.547478720Z I0611 11:00:54.547449       1 machinehealthcheck_controller.go:251] Remediations are allowed for openshift-machine-api/machine-api-termination-handler: total targets: 0,  max unhealthy: 100%, unhealthy targets: 0
2024-06-11T11:00:54.555886978Z I0611 11:00:54.555844       1 machinehealthcheck_controller.go:279] Reconciling openshift-machine-api/machine-api-termination-handler: no more targets meet unhealthy criteria
2024-06-11T11:03:11.234701862Z E0611 11:03:11.234641       1 leaderelection.go:332] error retrieving resource lock openshift-machine-api/cluster-api-provider-healthcheck-leader: Get "https://172.30.0.1:443/apis/coordination.k8s.io/v1/namespaces/openshift-machine-api/leases/cluster-api-provider-healthcheck-leader": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:03:37.235805119Z E0611 11:03:37.235746       1 leaderelection.go:332] error retrieving resource lock openshift-machine-api/cluster-api-provider-healthcheck-leader: Get "https://172.30.0.1:443/apis/coordination.k8s.io/v1/namespaces/openshift-machine-api/leases/cluster-api-provider-healthcheck-leader": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:04:03.236586869Z E0611 11:04:03.236525       1 leaderelection.go:332] error retrieving resource lock openshift-machine-api/cluster-api-provider-healthcheck-leader: Get "https://172.30.0.1:443/apis/coordination.k8s.io/v1/namespaces/openshift-machine-api/leases/cluster-api-provider-healthcheck-leader": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:04:29.235842074Z E0611 11:04:29.235785       1 leaderelection.go:332] error retrieving resource lock openshift-machine-api/cluster-api-provider-healthcheck-leader: Get "https://172.30.0.1:443/apis/coordination.k8s.io/v1/namespaces/openshift-machine-api/leases/cluster-api-provider-healthcheck-leader": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:05:05.100901144Z I0611 11:05:05.100826       1 reflector.go:325] Listing and watching *v1beta1.MachineHealthCheck from sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:105
2024-06-11T11:05:05.102871981Z I0611 11:05:05.102822       1 reflector.go:351] Caches populated for *v1beta1.MachineHealthCheck from sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:105
2024-06-11T11:05:05.102944182Z I0611 11:05:05.102921       1 machinehealthcheck_controller.go:162] Reconciling openshift-machine-api/machine-api-termination-handler
2024-06-11T11:05:05.103027784Z I0611 11:05:05.102981       1 machinehealthcheck_controller.go:188] Reconciling openshift-machine-api/machine-api-termination-handler: finding targets
2024-06-11T11:05:05.103100485Z I0611 11:05:05.103075       1 machinehealthcheck_controller.go:251] Remediations are allowed for openshift-machine-api/machine-api-termination-handler: total targets: 0,  max unhealthy: 100%, unhealthy targets: 0
2024-06-11T11:05:05.111650645Z I0611 11:05:05.111601       1 machinehealthcheck_controller.go:279] Reconciling openshift-machine-api/machine-api-termination-handler: no more targets meet unhealthy criteria
2024-06-11T11:05:50.108821688Z I0611 11:05:50.108602       1 reflector.go:325] Listing and watching *v1beta1.Machine from sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:105
2024-06-11T11:05:50.113872885Z I0611 11:05:50.113840       1 reflector.go:351] Caches populated for *v1beta1.Machine from sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:105
2024-06-11T11:06:04.922104556Z I0611 11:06:04.922019       1 reflector.go:325] Listing and watching *v1.Node from sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:105
2024-06-11T11:06:04.925773926Z I0611 11:06:04.925737       1 reflector.go:351] Caches populated for *v1.Node from sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:105
2024-06-11T11:12:18.715508633Z I0611 11:12:18.715442       1 reflector.go:325] Listing and watching *v1beta1.MachineHealthCheck from sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:105
2024-06-11T11:12:18.717232865Z I0611 11:12:18.717190       1 reflector.go:351] Caches populated for *v1beta1.MachineHealthCheck from sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:105
2024-06-11T11:12:18.717591372Z I0611 11:12:18.717364       1 machinehealthcheck_controller.go:162] Reconciling openshift-machine-api/machine-api-termination-handler
2024-06-11T11:12:18.717739275Z I0611 11:12:18.717720       1 machinehealthcheck_controller.go:188] Reconciling openshift-machine-api/machine-api-termination-handler: finding targets
2024-06-11T11:12:18.717876577Z I0611 11:12:18.717857       1 machinehealthcheck_controller.go:251] Remediations are allowed for openshift-machine-api/machine-api-termination-handler: total targets: 0,  max unhealthy: 100%, unhealthy targets: 0
2024-06-11T11:12:18.736512731Z I0611 11:12:18.736461       1 machinehealthcheck_controller.go:279] Reconciling openshift-machine-api/machine-api-termination-handler: no more targets meet unhealthy criteria
2024-06-11T11:12:19.154325567Z I0611 11:12:19.154279       1 reflector.go:325] Listing and watching *v1beta1.Machine from sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:105
2024-06-11T11:12:19.159629168Z I0611 11:12:19.159587       1 reflector.go:351] Caches populated for *v1beta1.Machine from sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:105
