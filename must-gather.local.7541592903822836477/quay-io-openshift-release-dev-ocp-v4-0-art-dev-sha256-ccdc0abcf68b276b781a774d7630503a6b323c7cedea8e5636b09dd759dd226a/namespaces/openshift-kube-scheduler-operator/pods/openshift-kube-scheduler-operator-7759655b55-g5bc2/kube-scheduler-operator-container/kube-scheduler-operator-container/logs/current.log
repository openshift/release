2024-06-11T10:49:46.264174155Z I0611 10:49:46.263991       1 cmd.go:241] Using service-serving-cert provided certificates
2024-06-11T10:49:46.264367062Z I0611 10:49:46.264167       1 leaderelection.go:122] The leader election gives 4 retries and allows for 30s of clock skew. The kube-apiserver downtime tolerance is 78s. Worst non-graceful lease acquisition is 2m43s. Worst graceful lease acquisition is {26s}.
2024-06-11T10:49:46.265149194Z I0611 10:49:46.265086       1 observer_polling.go:159] Starting file observer
2024-06-11T10:49:46.293839158Z I0611 10:49:46.293781       1 builder.go:299] openshift-cluster-kube-scheduler-operator version 4.16.0-202406061206.p0.g630f63b.assembly.stream.el9-630f63b-630f63bc7a30d2662bbb5115233144079de6eef6
2024-06-11T10:49:47.065208453Z I0611 10:49:47.065138       1 secure_serving.go:57] Forcing use of http/1.1 only
2024-06-11T10:49:47.065208453Z W0611 10:49:47.065168       1 secure_serving.go:69] Use of insecure cipher 'TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256' detected.
2024-06-11T10:49:47.065208453Z W0611 10:49:47.065176       1 secure_serving.go:69] Use of insecure cipher 'TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' detected.
2024-06-11T10:49:47.067635552Z I0611 10:49:47.067582       1 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
2024-06-11T10:49:47.067635552Z I0611 10:49:47.067626       1 shared_informer.go:311] Waiting for caches to sync for RequestHeaderAuthRequestController
2024-06-11T10:49:47.067712155Z I0611 10:49:47.067675       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
2024-06-11T10:49:47.067727956Z I0611 10:49:47.067698       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
2024-06-11T10:49:47.067740456Z I0611 10:49:47.067725       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
2024-06-11T10:49:47.067752357Z I0611 10:49:47.067709       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
2024-06-11T10:49:47.068271578Z I0611 10:49:47.068227       1 leaderelection.go:250] attempting to acquire leader lease openshift-kube-scheduler-operator/openshift-cluster-kube-scheduler-operator-lock...
2024-06-11T10:49:47.068296179Z I0611 10:49:47.068264       1 dynamic_serving_content.go:132] "Starting controller" name="serving-cert::/var/run/secrets/serving-cert/tls.crt::/var/run/secrets/serving-cert/tls.key"
2024-06-11T10:49:47.069017608Z I0611 10:49:47.068977       1 secure_serving.go:213] Serving securely on [::]:8443
2024-06-11T10:49:47.069038109Z I0611 10:49:47.069011       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
2024-06-11T10:49:47.075673778Z I0611 10:49:47.075626       1 leaderelection.go:260] successfully acquired lease openshift-kube-scheduler-operator/openshift-cluster-kube-scheduler-operator-lock
2024-06-11T10:49:47.075788683Z I0611 10:49:47.075681       1 event.go:364] Event(v1.ObjectReference{Kind:"Lease", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-cluster-kube-scheduler-operator-lock", UID:"a9d37d32-9584-4786-9488-9d9473e0f5b1", APIVersion:"coordination.k8s.io/v1", ResourceVersion:"11687", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' openshift-kube-scheduler-operator-7759655b55-g5bc2_7cd660fe-0473-462d-a29b-14684d9ced17 became leader
2024-06-11T10:49:47.076397907Z I0611 10:49:47.076350       1 simple_featuregate_reader.go:171] Starting feature-gate-detector
2024-06-11T10:49:47.078940311Z I0611 10:49:47.078876       1 starter.go:80] FeatureGates initialized: knownFeatureGates=[AdminNetworkPolicy AlibabaPlatform AutomatedEtcdBackup AzureWorkloadIdentity BareMetalLoadBalancer BuildCSIVolumes CSIDriverSharedResource ChunkSizeMiB CloudDualStackNodeIPs ClusterAPIInstall ClusterAPIInstallAWS ClusterAPIInstallAzure ClusterAPIInstallGCP ClusterAPIInstallIBMCloud ClusterAPIInstallNutanix ClusterAPIInstallOpenStack ClusterAPIInstallPowerVS ClusterAPIInstallVSphere DNSNameResolver DisableKubeletCloudCredentialProviders DynamicResourceAllocation EtcdBackendQuota EventedPLEG Example ExternalCloudProvider ExternalCloudProviderAzure ExternalCloudProviderExternal ExternalCloudProviderGCP ExternalOIDC ExternalRouteCertificate GCPClusterHostedDNS GCPLabelsTags GatewayAPI HardwareSpeed ImagePolicy InsightsConfig InsightsConfigAPI InsightsOnDemandDataGather InstallAlternateInfrastructureAWS KMSv1 MachineAPIOperatorDisableMachineHealthCheckController MachineAPIProviderOpenStack MachineConfigNodes ManagedBootImages MaxUnavailableStatefulSet MetricsCollectionProfiles MetricsServer MixedCPUsAllocation NetworkDiagnosticsConfig NetworkLiveMigration NewOLM NodeDisruptionPolicy NodeSwap OnClusterBuild OpenShiftPodSecurityAdmission PinnedImages PlatformOperators PrivateHostedZoneAWS RouteExternalCertificate ServiceAccountTokenNodeBinding ServiceAccountTokenNodeBindingValidation ServiceAccountTokenPodNodeInfo SignatureStores SigstoreImageVerification TranslateStreamCloseWebsocketRequests UpgradeStatus VSphereControlPlaneMachineSet VSphereDriverConfiguration VSphereMultiVCenters VSphereStaticIPs ValidatingAdmissionPolicy VolumeGroupSnapshot]
2024-06-11T10:49:47.079046715Z I0611 10:49:47.078862       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'FeatureGatesInitialized' FeatureGates updated to featuregates.Features{Enabled:[]v1.FeatureGateName{"AdminNetworkPolicy", "AlibabaPlatform", "AzureWorkloadIdentity", "BareMetalLoadBalancer", "BuildCSIVolumes", "CloudDualStackNodeIPs", "ClusterAPIInstallAWS", "ClusterAPIInstallNutanix", "ClusterAPIInstallOpenStack", "ClusterAPIInstallVSphere", "DisableKubeletCloudCredentialProviders", "ExternalCloudProvider", "ExternalCloudProviderAzure", "ExternalCloudProviderExternal", "ExternalCloudProviderGCP", "HardwareSpeed", "KMSv1", "MetricsServer", "NetworkDiagnosticsConfig", "NetworkLiveMigration", "PrivateHostedZoneAWS", "VSphereControlPlaneMachineSet", "VSphereDriverConfiguration", "VSphereStaticIPs"}, Disabled:[]v1.FeatureGateName{"AutomatedEtcdBackup", "CSIDriverSharedResource", "ChunkSizeMiB", "ClusterAPIInstall", "ClusterAPIInstallAzure", "ClusterAPIInstallGCP", "ClusterAPIInstallIBMCloud", "ClusterAPIInstallPowerVS", "DNSNameResolver", "DynamicResourceAllocation", "EtcdBackendQuota", "EventedPLEG", "Example", "ExternalOIDC", "ExternalRouteCertificate", "GCPClusterHostedDNS", "GCPLabelsTags", "GatewayAPI", "ImagePolicy", "InsightsConfig", "InsightsConfigAPI", "InsightsOnDemandDataGather", "InstallAlternateInfrastructureAWS", "MachineAPIOperatorDisableMachineHealthCheckController", "MachineAPIProviderOpenStack", "MachineConfigNodes", "ManagedBootImages", "MaxUnavailableStatefulSet", "MetricsCollectionProfiles", "MixedCPUsAllocation", "NewOLM", "NodeDisruptionPolicy", "NodeSwap", "OnClusterBuild", "OpenShiftPodSecurityAdmission", "PinnedImages", "PlatformOperators", "RouteExternalCertificate", "ServiceAccountTokenNodeBinding", "ServiceAccountTokenNodeBindingValidation", "ServiceAccountTokenPodNodeInfo", "SignatureStores", "SigstoreImageVerification", "TranslateStreamCloseWebsocketRequests", "UpgradeStatus", "VSphereMultiVCenters", "ValidatingAdmissionPolicy", "VolumeGroupSnapshot"}}
2024-06-11T10:49:47.088661005Z I0611 10:49:47.088613       1 base_controller.go:67] Waiting for caches to sync for RemoveStaleConditionsController
2024-06-11T10:49:47.091045202Z I0611 10:49:47.090997       1 base_controller.go:67] Waiting for caches to sync for ResourceSyncController
2024-06-11T10:49:47.091220309Z I0611 10:49:47.091192       1 base_controller.go:67] Waiting for caches to sync for ConfigObserver
2024-06-11T10:49:47.091384715Z I0611 10:49:47.091353       1 base_controller.go:67] Waiting for caches to sync for MissingStaticPodController
2024-06-11T10:49:47.091501520Z I0611 10:49:47.091086       1 base_controller.go:67] Waiting for caches to sync for TargetConfigController
2024-06-11T10:49:47.091558722Z I0611 10:49:47.091525       1 base_controller.go:67] Waiting for caches to sync for KubeControllerManagerStaticResources
2024-06-11T10:49:47.091693928Z I0611 10:49:47.091621       1 base_controller.go:67] Waiting for caches to sync for RevisionController
2024-06-11T10:49:47.092338554Z I0611 10:49:47.092261       1 base_controller.go:67] Waiting for caches to sync for InstallerController
2024-06-11T10:49:47.092548163Z I0611 10:49:47.092527       1 base_controller.go:67] Waiting for caches to sync for InstallerStateController
2024-06-11T10:49:47.092636466Z I0611 10:49:47.091481       1 base_controller.go:67] Waiting for caches to sync for StatusSyncer_kube-scheduler
2024-06-11T10:49:47.092795173Z I0611 10:49:47.092716       1 base_controller.go:67] Waiting for caches to sync for UnsupportedConfigOverridesController
2024-06-11T10:49:47.093021882Z I0611 10:49:47.092913       1 base_controller.go:67] Waiting for caches to sync for PruneController
2024-06-11T10:49:47.093139987Z I0611 10:49:47.093103       1 base_controller.go:67] Waiting for caches to sync for NodeController
2024-06-11T10:49:47.093865816Z I0611 10:49:47.093839       1 base_controller.go:67] Waiting for caches to sync for StaticPodStateController
2024-06-11T10:49:47.093865816Z I0611 10:49:47.093862       1 base_controller.go:67] Waiting for caches to sync for GuardController
2024-06-11T10:49:47.093909018Z I0611 10:49:47.093873       1 base_controller.go:67] Waiting for caches to sync for LoggingSyncer
2024-06-11T10:49:47.094601746Z I0611 10:49:47.094552       1 base_controller.go:67] Waiting for caches to sync for BackingResourceController
2024-06-11T10:49:47.167861018Z I0611 10:49:47.167783       1 shared_informer.go:318] Caches are synced for RequestHeaderAuthRequestController
2024-06-11T10:49:47.167861018Z I0611 10:49:47.167808       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
2024-06-11T10:49:47.167861018Z I0611 10:49:47.167830       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
2024-06-11T10:49:47.191136462Z I0611 10:49:47.191079       1 base_controller.go:73] Caches are synced for RemoveStaleConditionsController 
2024-06-11T10:49:47.191136462Z I0611 10:49:47.191110       1 base_controller.go:110] Starting #1 worker of RemoveStaleConditionsController controller ...
2024-06-11T10:49:47.193348952Z I0611 10:49:47.193293       1 base_controller.go:73] Caches are synced for InstallerStateController 
2024-06-11T10:49:47.193348952Z I0611 10:49:47.193323       1 base_controller.go:110] Starting #1 worker of InstallerStateController controller ...
2024-06-11T10:49:47.193401254Z I0611 10:49:47.193347       1 base_controller.go:73] Caches are synced for StatusSyncer_kube-scheduler 
2024-06-11T10:49:47.193401254Z I0611 10:49:47.193356       1 base_controller.go:73] Caches are synced for InstallerController 
2024-06-11T10:49:47.193401254Z I0611 10:49:47.193365       1 base_controller.go:110] Starting #1 worker of InstallerController controller ...
2024-06-11T10:49:47.193401254Z I0611 10:49:47.193368       1 base_controller.go:110] Starting #1 worker of StatusSyncer_kube-scheduler controller ...
2024-06-11T10:49:47.194191386Z I0611 10:49:47.194152       1 base_controller.go:73] Caches are synced for LoggingSyncer 
2024-06-11T10:49:47.194191386Z I0611 10:49:47.194174       1 base_controller.go:110] Starting #1 worker of LoggingSyncer controller ...
2024-06-11T10:49:47.194215787Z I0611 10:49:47.194190       1 base_controller.go:73] Caches are synced for StaticPodStateController 
2024-06-11T10:49:47.194215787Z I0611 10:49:47.194205       1 base_controller.go:110] Starting #1 worker of StaticPodStateController controller ...
2024-06-11T10:49:47.194249189Z I0611 10:49:47.194224       1 base_controller.go:73] Caches are synced for GuardController 
2024-06-11T10:49:47.194249189Z I0611 10:49:47.194242       1 base_controller.go:110] Starting #1 worker of GuardController controller ...
2024-06-11T10:49:47.194395295Z I0611 10:49:47.194359       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' secrets: kube-scheduler-client-cert-key, configmaps: config-4,kube-scheduler-cert-syncer-kubeconfig-4,kube-scheduler-pod-4,scheduler-kubeconfig-4,serviceaccount-ca-4, secrets: localhost-recovery-client-token-4
2024-06-11T10:49:47.194527700Z I0611 10:49:47.194489       1 base_controller.go:73] Caches are synced for NodeController 
2024-06-11T10:49:47.194527700Z I0611 10:49:47.194510       1 base_controller.go:110] Starting #1 worker of NodeController controller ...
2024-06-11T10:49:47.194547801Z I0611 10:49:47.194524       1 base_controller.go:73] Caches are synced for UnsupportedConfigOverridesController 
2024-06-11T10:49:47.194547801Z I0611 10:49:47.194538       1 base_controller.go:110] Starting #1 worker of UnsupportedConfigOverridesController controller ...
2024-06-11T10:49:47.194774310Z I0611 10:49:47.194721       1 base_controller.go:73] Caches are synced for BackingResourceController 
2024-06-11T10:49:47.194856813Z I0611 10:49:47.194510       1 base_controller.go:73] Caches are synced for PruneController 
2024-06-11T10:49:47.194937217Z I0611 10:49:47.194902       1 base_controller.go:110] Starting #1 worker of PruneController controller ...
2024-06-11T10:49:47.194937217Z E0611 10:49:47.194798       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:49:47.194977818Z E0611 10:49:47.194954       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:49:47.195018420Z E0611 10:49:47.194992       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-0
2024-06-11T10:49:47.195100123Z I0611 10:49:47.194821       1 base_controller.go:110] Starting #1 worker of BackingResourceController controller ...
2024-06-11T10:49:47.195230129Z I0611 10:49:47.195143       1 prune_controller.go:269] Nothing to prune
2024-06-11T10:49:47.195330733Z E0611 10:49:47.195220       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-0, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:49:47.201570686Z E0611 10:49:47.201522       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:49:47.201570686Z E0611 10:49:47.201550       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-0
2024-06-11T10:49:47.201570686Z E0611 10:49:47.201567       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:49:47.201807895Z E0611 10:49:47.201727       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-0, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:49:47.209058190Z E0611 10:49:47.209022       1 base_controller.go:268] InstallerController reconciliation failed: missing required resources: [secrets: kube-scheduler-client-cert-key, configmaps: config-4,kube-scheduler-cert-syncer-kubeconfig-4,kube-scheduler-pod-4,scheduler-kubeconfig-4,serviceaccount-ca-4, secrets: localhost-recovery-client-token-4]
2024-06-11T10:49:47.210154234Z I0611 10:49:47.210106       1 prune_controller.go:269] Nothing to prune
2024-06-11T10:49:47.210563251Z I0611 10:49:47.210503       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' secrets: kube-scheduler-client-cert-key, configmaps: config-4,kube-scheduler-cert-syncer-kubeconfig-4,kube-scheduler-pod-4,scheduler-kubeconfig-4,serviceaccount-ca-4, secrets: localhost-recovery-client-token-4
2024-06-11T10:49:47.210635154Z E0611 10:49:47.210608       1 base_controller.go:268] InstallerController reconciliation failed: missing required resources: [secrets: kube-scheduler-client-cert-key, configmaps: config-4,kube-scheduler-cert-syncer-kubeconfig-4,kube-scheduler-pod-4,scheduler-kubeconfig-4,serviceaccount-ca-4, secrets: localhost-recovery-client-token-4]
2024-06-11T10:49:47.210875063Z I0611 10:49:47.210844       1 status_controller.go:218] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2024-06-11T10:48:20Z","message":"InstallerControllerDegraded: missing required resources: [secrets: kube-scheduler-client-cert-key, configmaps: config-4,kube-scheduler-cert-syncer-kubeconfig-4,kube-scheduler-pod-4,scheduler-kubeconfig-4,serviceaccount-ca-4, secrets: localhost-recovery-client-token-4]\nGuardControllerDegraded: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-0, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]\nNodeControllerDegraded: All master nodes are ready","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2024-06-11T10:48:34Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 4","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-06-11T10:48:20Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 4","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-06-11T10:48:31Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-06-11T10:48:20Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2024-06-11T10:49:47.212395925Z E0611 10:49:47.212361       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-0
2024-06-11T10:49:47.212395925Z E0611 10:49:47.212390       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:49:47.212428526Z E0611 10:49:47.212407       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:49:47.212559232Z E0611 10:49:47.212537       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-0, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:49:47.214657017Z I0611 10:49:47.214613       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' secrets: kube-scheduler-client-cert-key, configmaps: config-4,kube-scheduler-cert-syncer-kubeconfig-4,kube-scheduler-pod-4,scheduler-kubeconfig-4,serviceaccount-ca-4, secrets: localhost-recovery-client-token-4
2024-06-11T10:49:47.214713319Z E0611 10:49:47.214691       1 base_controller.go:268] InstallerController reconciliation failed: missing required resources: [secrets: kube-scheduler-client-cert-key, configmaps: config-4,kube-scheduler-cert-syncer-kubeconfig-4,kube-scheduler-pod-4,scheduler-kubeconfig-4,serviceaccount-ca-4, secrets: localhost-recovery-client-token-4]
2024-06-11T10:49:47.233216570Z E0611 10:49:47.233170       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-0
2024-06-11T10:49:47.233271072Z E0611 10:49:47.233214       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:49:47.233271072Z E0611 10:49:47.233239       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:49:47.233503581Z E0611 10:49:47.233468       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-0, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:49:47.235430160Z I0611 10:49:47.235379       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' secrets: kube-scheduler-client-cert-key, configmaps: config-4,kube-scheduler-cert-syncer-kubeconfig-4,kube-scheduler-pod-4,scheduler-kubeconfig-4,serviceaccount-ca-4, secrets: localhost-recovery-client-token-4
2024-06-11T10:49:47.235502962Z E0611 10:49:47.235467       1 base_controller.go:268] InstallerController reconciliation failed: missing required resources: [secrets: kube-scheduler-client-cert-key, configmaps: config-4,kube-scheduler-cert-syncer-kubeconfig-4,kube-scheduler-pod-4,scheduler-kubeconfig-4,serviceaccount-ca-4, secrets: localhost-recovery-client-token-4]
2024-06-11T10:49:47.242413643Z I0611 10:49:47.242354       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded message changed from "GuardControllerDegraded: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-0, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]\nNodeControllerDegraded: All master nodes are ready" to "InstallerControllerDegraded: missing required resources: [secrets: kube-scheduler-client-cert-key, configmaps: config-4,kube-scheduler-cert-syncer-kubeconfig-4,kube-scheduler-pod-4,scheduler-kubeconfig-4,serviceaccount-ca-4, secrets: localhost-recovery-client-token-4]\nGuardControllerDegraded: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-0, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]\nNodeControllerDegraded: All master nodes are ready"
2024-06-11T10:49:47.274559147Z E0611 10:49:47.274489       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-0
2024-06-11T10:49:47.274559147Z E0611 10:49:47.274532       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:49:47.274559147Z E0611 10:49:47.274553       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:49:47.274781256Z E0611 10:49:47.274757       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-0, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:49:47.276766037Z E0611 10:49:47.276732       1 base_controller.go:268] InstallerController reconciliation failed: missing required resources: [secrets: kube-scheduler-client-cert-key, configmaps: config-4,kube-scheduler-cert-syncer-kubeconfig-4,kube-scheduler-pod-4,scheduler-kubeconfig-4,serviceaccount-ca-4, secrets: localhost-recovery-client-token-4]
2024-06-11T10:49:47.276818739Z I0611 10:49:47.276768       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' secrets: kube-scheduler-client-cert-key, configmaps: config-4,kube-scheduler-cert-syncer-kubeconfig-4,kube-scheduler-pod-4,scheduler-kubeconfig-4,serviceaccount-ca-4, secrets: localhost-recovery-client-token-4
2024-06-11T10:49:47.292614580Z I0611 10:49:47.292546       1 reflector.go:351] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T10:49:47.356031052Z E0611 10:49:47.355970       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:49:47.356031052Z E0611 10:49:47.356021       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:49:47.356074654Z E0611 10:49:47.356050       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-0
2024-06-11T10:49:47.356366866Z E0611 10:49:47.356327       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-0, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:49:47.358469251Z I0611 10:49:47.358399       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' secrets: kube-scheduler-client-cert-key, secrets: localhost-recovery-client-token-4
2024-06-11T10:49:47.367202606Z E0611 10:49:47.367143       1 base_controller.go:268] InstallerController reconciliation failed: missing required resources: [secrets: kube-scheduler-client-cert-key, secrets: localhost-recovery-client-token-4]
2024-06-11T10:49:47.367786829Z I0611 10:49:47.367731       1 prune_controller.go:269] Nothing to prune
2024-06-11T10:49:47.367975537Z E0611 10:49:47.367918       1 base_controller.go:268] InstallerController reconciliation failed: missing required resources: [secrets: kube-scheduler-client-cert-key, secrets: localhost-recovery-client-token-4]
2024-06-11T10:49:47.367975537Z I0611 10:49:47.367957       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' secrets: kube-scheduler-client-cert-key, secrets: localhost-recovery-client-token-4
2024-06-11T10:49:47.368509859Z I0611 10:49:47.368469       1 status_controller.go:218] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2024-06-11T10:48:20Z","message":"InstallerControllerDegraded: missing required resources: [secrets: kube-scheduler-client-cert-key, secrets: localhost-recovery-client-token-4]\nGuardControllerDegraded: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-0, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]\nNodeControllerDegraded: All master nodes are ready","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2024-06-11T10:48:34Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 4","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-06-11T10:48:20Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 4","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-06-11T10:48:31Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-06-11T10:48:20Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2024-06-11T10:49:47.379292896Z I0611 10:49:47.379230       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded message changed from "InstallerControllerDegraded: missing required resources: [secrets: kube-scheduler-client-cert-key, configmaps: config-4,kube-scheduler-cert-syncer-kubeconfig-4,kube-scheduler-pod-4,scheduler-kubeconfig-4,serviceaccount-ca-4, secrets: localhost-recovery-client-token-4]\nGuardControllerDegraded: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-0, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]\nNodeControllerDegraded: All master nodes are ready" to "InstallerControllerDegraded: missing required resources: [secrets: kube-scheduler-client-cert-key, secrets: localhost-recovery-client-token-4]\nGuardControllerDegraded: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-0, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]\nNodeControllerDegraded: All master nodes are ready"
2024-06-11T10:49:47.392338225Z I0611 10:49:47.392253       1 base_controller.go:73] Caches are synced for MissingStaticPodController 
2024-06-11T10:49:47.392338225Z I0611 10:49:47.392281       1 base_controller.go:110] Starting #1 worker of MissingStaticPodController controller ...
2024-06-11T10:49:47.464077336Z E0611 10:49:47.461799       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-0
2024-06-11T10:49:47.464077336Z E0611 10:49:47.461850       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:49:47.464077336Z E0611 10:49:47.461875       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:49:47.464077336Z E0611 10:49:47.462129       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-0, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:49:47.491533450Z I0611 10:49:47.491476       1 reflector.go:351] Caches populated for *v1.Secret from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T10:49:47.492279880Z I0611 10:49:47.492204       1 base_controller.go:73] Caches are synced for RevisionController 
2024-06-11T10:49:47.492279880Z I0611 10:49:47.492243       1 base_controller.go:110] Starting #1 worker of RevisionController controller ...
2024-06-11T10:49:47.517372398Z E0611 10:49:47.517283       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-0
2024-06-11T10:49:47.517372398Z E0611 10:49:47.517340       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:49:47.517372398Z E0611 10:49:47.517364       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:49:47.517591407Z E0611 10:49:47.517563       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-0, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:49:47.692658210Z I0611 10:49:47.692604       1 reflector.go:351] Caches populated for *v1.Namespace from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T10:49:47.792239850Z I0611 10:49:47.792174       1 base_controller.go:73] Caches are synced for KubeControllerManagerStaticResources 
2024-06-11T10:49:47.792239850Z I0611 10:49:47.792212       1 base_controller.go:110] Starting #1 worker of KubeControllerManagerStaticResources controller ...
2024-06-11T10:49:47.891341171Z I0611 10:49:47.891231       1 reflector.go:351] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T10:49:47.891587181Z I0611 10:49:47.891540       1 base_controller.go:73] Caches are synced for ConfigObserver 
2024-06-11T10:49:47.891587181Z I0611 10:49:47.891569       1 base_controller.go:110] Starting #1 worker of ConfigObserver controller ...
2024-06-11T10:49:48.090783462Z I0611 10:49:48.090710       1 reflector.go:351] Caches populated for *v1.Secret from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T10:49:48.091664098Z I0611 10:49:48.091592       1 base_controller.go:73] Caches are synced for TargetConfigController 
2024-06-11T10:49:48.091664098Z I0611 10:49:48.091622       1 base_controller.go:110] Starting #1 worker of TargetConfigController controller ...
2024-06-11T10:49:48.091664098Z I0611 10:49:48.091600       1 base_controller.go:73] Caches are synced for ResourceSyncController 
2024-06-11T10:49:48.091664098Z I0611 10:49:48.091646       1 base_controller.go:110] Starting #1 worker of ResourceSyncController controller ...
2024-06-11T10:49:48.158354204Z E0611 10:49:48.158249       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:49:48.158354204Z E0611 10:49:48.158318       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-0
2024-06-11T10:49:48.158402306Z E0611 10:49:48.158348       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:49:48.158596413Z E0611 10:49:48.158564       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-0, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:49:48.289520525Z I0611 10:49:48.289436       1 request.go:697] Waited for 1.095892162s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller
2024-06-11T10:49:48.889466766Z E0611 10:49:48.889413       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-0
2024-06-11T10:49:48.889568170Z E0611 10:49:48.889552       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:49:48.889628972Z E0611 10:49:48.889615       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:49:48.889917984Z E0611 10:49:48.889899       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-0, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:49:49.439800993Z E0611 10:49:49.439727       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-0
2024-06-11T10:49:49.439800993Z E0611 10:49:49.439763       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:49:49.439800993Z E0611 10:49:49.439786       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:49:49.440008002Z E0611 10:49:49.439963       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-0, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:49:49.894347535Z I0611 10:49:49.894244       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-0" is in transition to 4, but has not made progress because installer is not finished, but in Running phase
2024-06-11T10:49:49.905906004Z I0611 10:49:49.905854       1 prune_controller.go:269] Nothing to prune
2024-06-11T10:49:49.906903644Z I0611 10:49:49.906848       1 status_controller.go:218] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2024-06-11T10:48:20Z","message":"GuardControllerDegraded: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-0, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]\nNodeControllerDegraded: All master nodes are ready","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2024-06-11T10:48:34Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 4","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-06-11T10:48:20Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 4","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-06-11T10:48:31Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-06-11T10:48:20Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2024-06-11T10:49:49.919557058Z I0611 10:49:49.919489       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded message changed from "InstallerControllerDegraded: missing required resources: [secrets: kube-scheduler-client-cert-key, secrets: localhost-recovery-client-token-4]\nGuardControllerDegraded: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-0, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]\nNodeControllerDegraded: All master nodes are ready" to "GuardControllerDegraded: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-0, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]\nNodeControllerDegraded: All master nodes are ready"
2024-06-11T10:49:50.808554625Z E0611 10:49:50.808482       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-0
2024-06-11T10:49:50.808661829Z E0611 10:49:50.808638       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:49:50.808742533Z E0611 10:49:50.808723       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:49:50.809044445Z E0611 10:49:50.809025       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-0, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:49:51.691531248Z I0611 10:49:51.691466       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-0" is in transition to 4, but has not made progress because installer is not finished, but in Running phase
2024-06-11T10:49:52.493250154Z E0611 10:49:52.493171       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-0
2024-06-11T10:49:52.493429962Z E0611 10:49:52.493406       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:49:52.493503265Z E0611 10:49:52.493488       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:49:52.493867982Z E0611 10:49:52.493826       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-0, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:49:53.244669810Z E0611 10:49:53.243643       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-0
2024-06-11T10:49:53.244669810Z E0611 10:49:53.243929       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:49:53.244669810Z E0611 10:49:53.243972       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:49:53.244669810Z E0611 10:49:53.244448       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-0, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:49:54.560926962Z E0611 10:49:54.560857       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-0
2024-06-11T10:49:54.560926962Z E0611 10:49:54.560906       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:49:54.560972564Z E0611 10:49:54.560930       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:49:54.561168572Z E0611 10:49:54.561136       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-0, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:49:55.828772384Z E0611 10:49:55.828711       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-0
2024-06-11T10:49:55.829443513Z E0611 10:49:55.829411       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:49:55.829523816Z E0611 10:49:55.829509       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:49:55.829802728Z E0611 10:49:55.829780       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-0, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:49:58.914009147Z E0611 10:49:58.913957       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-0
2024-06-11T10:49:58.914577372Z E0611 10:49:58.914541       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:49:58.914700077Z E0611 10:49:58.914682       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:49:58.914953688Z E0611 10:49:58.914934       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-0, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:50:00.839149439Z E0611 10:50:00.839091       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-0
2024-06-11T10:50:00.839253744Z E0611 10:50:00.839232       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:50:00.839345148Z E0611 10:50:00.839325       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:50:00.839675962Z E0611 10:50:00.839634       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-0, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:50:02.403291668Z E0611 10:50:02.403228       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:50:02.403472773Z E0611 10:50:02.403418       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:50:02.403472773Z E0611 10:50:02.403462       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-0
2024-06-11T10:50:02.403776381Z E0611 10:50:02.403721       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-0, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:50:09.689351080Z E0611 10:50:09.686461       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-0
2024-06-11T10:50:09.689351080Z E0611 10:50:09.686515       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:50:09.689351080Z E0611 10:50:09.686546       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:50:09.689351080Z E0611 10:50:09.686784       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-0, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:50:15.943432490Z E0611 10:50:15.943371       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:50:15.943432490Z E0611 10:50:15.943404       1 guard_controller.go:293] Missing PodIP in operand openshift-kube-scheduler-ci-op-9xx71rvq-1e28e-w667k-master-0 on node ci-op-9xx71rvq-1e28e-w667k-master-0
2024-06-11T10:50:15.943432490Z E0611 10:50:15.943426       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:50:15.964362413Z I0611 10:50:15.964275       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-0" is in transition to 4, but has not made progress because installer is not finished, but in Running phase
2024-06-11T10:50:15.964559822Z E0611 10:50:15.964452       1 base_controller.go:268] GuardController reconciliation failed: [Missing PodIP in operand openshift-kube-scheduler-ci-op-9xx71rvq-1e28e-w667k-master-0 on node ci-op-9xx71rvq-1e28e-w667k-master-0, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:50:15.965972184Z I0611 10:50:15.965922       1 prune_controller.go:269] Nothing to prune
2024-06-11T10:50:15.968846211Z I0611 10:50:15.968785       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorVersionChanged' clusteroperator/kube-scheduler version "kube-scheduler" changed from "" to "1.29.5"
2024-06-11T10:50:15.968846211Z I0611 10:50:15.968824       1 status_controller.go:218] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2024-06-11T10:48:20Z","message":"GuardControllerDegraded: [Missing PodIP in operand openshift-kube-scheduler-ci-op-9xx71rvq-1e28e-w667k-master-0 on node ci-op-9xx71rvq-1e28e-w667k-master-0, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]\nNodeControllerDegraded: All master nodes are ready","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2024-06-11T10:48:34Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 4","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-06-11T10:48:20Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 4","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-06-11T10:48:31Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-06-11T10:48:20Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}],"versions":[{"name":"raw-internal","version":"4.16.0-0.nightly-2024-06-10-211334"},{"name":"kube-scheduler","version":"1.29.5"},{"name":"operator","version":"4.16.0-0.nightly-2024-06-10-211334"}]}}
2024-06-11T10:50:15.969187126Z I0611 10:50:15.968825       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorVersionChanged' clusteroperator/kube-scheduler version "operator" changed from "" to "4.16.0-0.nightly-2024-06-10-211334"
2024-06-11T10:50:15.986203776Z I0611 10:50:15.986148       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded message changed from "GuardControllerDegraded: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-0, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]\nNodeControllerDegraded: All master nodes are ready" to "GuardControllerDegraded: [Missing PodIP in operand openshift-kube-scheduler-ci-op-9xx71rvq-1e28e-w667k-master-0 on node ci-op-9xx71rvq-1e28e-w667k-master-0, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]\nNodeControllerDegraded: All master nodes are ready",status.versions changed from [{"raw-internal" "4.16.0-0.nightly-2024-06-10-211334"}] to [{"raw-internal" "4.16.0-0.nightly-2024-06-10-211334"} {"kube-scheduler" "1.29.5"} {"operator" "4.16.0-0.nightly-2024-06-10-211334"}]
2024-06-11T10:50:16.400064225Z E0611 10:50:16.399989       1 guard_controller.go:293] Missing PodIP in operand openshift-kube-scheduler-ci-op-9xx71rvq-1e28e-w667k-master-0 on node ci-op-9xx71rvq-1e28e-w667k-master-0
2024-06-11T10:50:16.400064225Z E0611 10:50:16.400037       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:50:16.400115727Z E0611 10:50:16.400066       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:50:16.400363838Z E0611 10:50:16.400325       1 base_controller.go:268] GuardController reconciliation failed: [Missing PodIP in operand openshift-kube-scheduler-ci-op-9xx71rvq-1e28e-w667k-master-0 on node ci-op-9xx71rvq-1e28e-w667k-master-0, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:50:17.584809665Z I0611 10:50:17.584740       1 request.go:697] Waited for 1.17001869s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ci-op-9xx71rvq-1e28e-w667k-master-0
2024-06-11T10:50:18.585016154Z I0611 10:50:18.584951       1 request.go:697] Waited for 1.196923862s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller
2024-06-11T10:50:18.987780416Z I0611 10:50:18.987682       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-0" is in transition to 4, but has not made progress because static pod is pending
2024-06-11T10:50:19.784192323Z I0611 10:50:19.784122       1 request.go:697] Waited for 1.195968422s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller
2024-06-11T10:50:19.992941992Z W0611 10:50:19.992871       1 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
2024-06-11T10:50:19.993095099Z E0611 10:50:19.993053       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:50:19.993117800Z E0611 10:50:19.993091       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:50:19.994163148Z I0611 10:50:19.994105       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/openshift-kube-scheduler-guard-ci-op-9xx71rvq-1e28e-w667k-master-0 -n openshift-kube-scheduler because it was missing
2024-06-11T10:50:20.015128109Z E0611 10:50:20.015081       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:50:20.015776439Z I0611 10:50:20.015735       1 prune_controller.go:269] Nothing to prune
2024-06-11T10:50:20.017468916Z I0611 10:50:20.017432       1 status_controller.go:218] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2024-06-11T10:50:20Z","message":"GuardControllerDegraded: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-06-11T10:48:34Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 4","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-06-11T10:48:20Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 4","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-06-11T10:48:31Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-06-11T10:48:20Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2024-06-11T10:50:20.034138480Z I0611 10:50:20.032615       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded changed from False to True ("GuardControllerDegraded: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]")
2024-06-11T10:50:21.184257101Z I0611 10:50:21.184195       1 request.go:697] Waited for 1.166294062s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa
2024-06-11T10:50:22.384857935Z I0611 10:50:22.384794       1 request.go:697] Waited for 1.386520157s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler
2024-06-11T10:50:22.596621542Z I0611 10:50:22.596544       1 core.go:218] Pod "openshift-kube-scheduler/openshift-kube-scheduler-guard-ci-op-9xx71rvq-1e28e-w667k-master-0" changes: {"apiVersion":"v1","kind":"Pod","metadata":{"annotations":{"k8s.ovn.org/pod-networks":null,"k8s.v1.cni.cncf.io/network-status":null,"target.workload.openshift.io/management":"{\"effect\": \"PreferredDuringScheduling\"}"},"creationTimestamp":null,"managedFields":null,"resourceVersion":null,"uid":null},"spec":{"containers":[{"args":["-c","# properly handle TERM and exit as soon as it is signaled\nset -euo pipefail\ntrap 'jobs -p | xargs -r kill; exit 0' TERM\nsleep infinity \u0026 wait\n"],"command":["/bin/bash"],"image":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:68ae5e595cb6b6ffa3f6861f7a41a92f5db8e9cd77fabb216dd7a96b9c1b4cf5","imagePullPolicy":"IfNotPresent","name":"guard","readinessProbe":{"failureThreshold":3,"httpGet":{"host":"10.0.0.8","path":"healthz","port":10259,"scheme":"HTTPS"},"periodSeconds":5,"successThreshold":1,"timeoutSeconds":5},"resources":{"requests":{"cpu":"10m","memory":"5Mi"}},"terminationMessagePolicy":"FallbackToLogsOnError"}],"dnsPolicy":null,"enableServiceLinks":null,"preemptionPolicy":null,"priority":null,"restartPolicy":null,"schedulerName":null,"securityContext":null,"serviceAccount":null,"serviceAccountName":null,"tolerations":[{"effect":"NoSchedule","key":"node-role.kubernetes.io/master","operator":"Exists"},{"effect":"NoExecute","key":"node.kubernetes.io/not-ready","operator":"Exists"},{"effect":"NoExecute","key":"node.kubernetes.io/unreachable","operator":"Exists"},{"effect":"NoSchedule","key":"node-role.kubernetes.io/etcd","operator":"Exists"}],"volumes":null},"status":{"conditions":null,"containerStatuses":null,"hostIP":null,"hostIPs":null,"phase":null,"podIP":null,"podIPs":null,"qosClass":null,"startTime":null}}
2024-06-11T10:50:22.788482337Z I0611 10:50:22.788418       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-0" is in transition to 4, but has not made progress because static pod is pending
2024-06-11T10:50:23.384959879Z I0611 10:50:23.384861       1 request.go:697] Waited for 1.196774759s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller
2024-06-11T10:50:23.796395539Z E0611 10:50:23.796332       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:50:23.796395539Z E0611 10:50:23.796377       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:50:23.796493444Z I0611 10:50:23.796428       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodUpdated' Updated Pod/openshift-kube-scheduler-guard-ci-op-9xx71rvq-1e28e-w667k-master-0 -n openshift-kube-scheduler because it changed
2024-06-11T10:50:23.796704853Z E0611 10:50:23.796676       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:50:23.796944264Z E0611 10:50:23.796915       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:50:24.985357240Z I0611 10:50:24.984560       1 request.go:697] Waited for 1.187108116s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ci-op-9xx71rvq-1e28e-w667k-master-0
2024-06-11T10:50:25.788227443Z E0611 10:50:25.788167       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:50:25.788502856Z E0611 10:50:25.788470       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:50:25.987947711Z I0611 10:50:25.987848       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-0" is in transition to 4, but has not made progress because static pod is pending
2024-06-11T10:50:27.352670557Z I0611 10:50:27.352600       1 core.go:350] ConfigMap "openshift-kube-scheduler/serviceaccount-ca" changes: {"data":{"ca-bundle.crt":"-----BEGIN CERTIFICATE-----\nMIIDMjCCAhqgAwIBAgIIOZgkPKjiSXMwDQYJKoZIhvcNAQELBQAwNzESMBAGA1UE\nCxMJb3BlbnNoaWZ0MSEwHwYDVQQDExhrdWJlLWFwaXNlcnZlci1sYi1zaWduZXIw\nHhcNMjQwNjExMTAxOTAxWhcNMzQwNjA5MTAxOTAxWjA3MRIwEAYDVQQLEwlvcGVu\nc2hpZnQxITAfBgNVBAMTGGt1YmUtYXBpc2VydmVyLWxiLXNpZ25lcjCCASIwDQYJ\nKoZIhvcNAQEBBQADggEPADCCAQoCggEBAOMNJQ31plTY73mhN9xkeDKdBtdIMt+4\n7WAaFY8heeQdmlZfUZkZsjl1pvSugZ/hZcRXumQ3K1tdB+cKZOTKJQgPzIzOETHM\nbt0kQ6oQoul19LaT3suFT2TJ9vqNRK6fCd4IQOpLgmg51h3lBfcDz42Dd6vCWWjt\nmlGbLWsDXV0//db2PNdLlSPljFaqPdUJ60cE9eQwdzvDiTZkjN0wm4Cal/QRPmN6\nwm66FafdJAmGNhKqz23eR/KpNvf5UfZjvWXiCAgPH1okpIQswSIdPBCp3lrkftga\nXLqeX48BEJAU2+X5IZjzihzDyPngyHxiH4cXz/x6WKPpoEvdsabiCB8CAwEAAaNC\nMEAwDgYDVR0PAQH/BAQDAgKkMA8GA1UdEwEB/wQFMAMBAf8wHQYDVR0OBBYEFAFe\n+hvToOpBQ1OjZmFKflYn87mGMA0GCSqGSIb3DQEBCwUAA4IBAQDdgDIWYEqHNCNx\nfVGa+5Sgh4qE/rKsILCAPKjFtvSWg5jkfTyTyZgig6ikby1xA8gVjL1dHy6jGv2g\n2RV8zavt9HBLSSHM/Gz+85dADsot5aE+fypMx3TQuFvxXNytcqE8TI1GUroQnwp4\nCEXcRaPJ3Tt9bqS3yJuPcwdREhUFjEdMO9ZFbVUk94tPsZ07tfPvaCTCrikf8Occ\nEmIBN0ahylZk8dw9OmKw3wtYgoLAeLfTFxNYTdVb1VYei4AK/yC29f8/skrwAPBX\n/9TNA9p5xnRtyaa9mYbO20LdfED7m4PsHzCw0U0XjqDvOosN/Xi1xD5ZxKsDRuxj\nEpPdHinx\n-----END CERTIFICATE-----\n-----BEGIN CERTIFICATE-----\nMIIDQDCCAiigAwIBAgIIK/muZYsht/cwDQYJKoZIhvcNAQELBQAwPjESMBAGA1UE\nCxMJb3BlbnNoaWZ0MSgwJgYDVQQDEx9rdWJlLWFwaXNlcnZlci1sb2NhbGhvc3Qt\nc2lnbmVyMB4XDTI0MDYxMTEwMTkwMVoXDTM0MDYwOTEwMTkwMVowPjESMBAGA1UE\nCxMJb3BlbnNoaWZ0MSgwJgYDVQQDEx9rdWJlLWFwaXNlcnZlci1sb2NhbGhvc3Qt\nc2lnbmVyMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA4iXHckm/7n1U\nQUlmnOPjm6Co/1q/cl/cWJRtsdAMvd9V5hsKehnPJ4Q4mDCQ3rkpz7fagNbHSEvO\nP9rAjEI6Bgzwbo+JAwMqDelN7CbEO9p2AW2ciuXQX2OhMow+13NBbbZxa8UHZcqG\n2rh+yRv/SIYT+A/WotWCQZwQJc9JiGsK1q1oLe5kNNr/2sCjVe3o63Tf74TTMC52\nAY1rDbnhrsx2VKB3JMV2XcQS5k9IxZbFPLJWzvGM2t4zZBnu0U9hN4JtzArsNSgl\nb3LiKAC6PCLV3kCVGbXlOiTykOTyNq3lhARHJzd5Lm6cN+uXywmVtj+QE45pgfbJ\nyroFyOiOZwIDAQABo0IwQDAOBgNVHQ8BAf8EBAMCAqQwDwYDVR0TAQH/BAUwAwEB\n/zAdBgNVHQ4EFgQUUZKSa8GUZ9qjiL/E7pSges35z4AwDQYJKoZIhvcNAQELBQAD\nggEBAC6QwBuPhM8F+wB5zqt9ldzYXOzzlVy7u1+4agEvTlfz0vB7l2Bi22rFEimw\nhEXsmuZhVIJxgGNWVqLbg2dr8ax1QZ/gPeXqZXJY8wz0kzlNQB3tERBlxCQhIIoa\ncuQipYTA51C7WEoIQQc68+HrPmmHnhBbFw5LZbN6k6jRmLFotg7RCtjZLDS+2bxw\nqZT0YencAZMBVpvDoDZyW4HEdUd4bzrmK2p1DUNkMlZuSR+Rjjp8CLz+0ikveesK\ncGanBxN+oW1OzlVyaCI2Epu+aagPwHbUUFdlw7hsyNc5lofab+nfm2ViHZhZwBcj\nA0LSVVmltDG7jfi0Xj49ug51mUA=\n-----END CERTIFICATE-----\n-----BEGIN CERTIFICATE-----\nMIIDTDCCAjSgAwIBAgIINO0yAMSHeFcwDQYJKoZIhvcNAQELBQAwRDESMBAGA1UE\nCxMJb3BlbnNoaWZ0MS4wLAYDVQQDEyVrdWJlLWFwaXNlcnZlci1zZXJ2aWNlLW5l\ndHdvcmstc2lnbmVyMB4XDTI0MDYxMTEwMTkwMVoXDTM0MDYwOTEwMTkwMVowRDES\nMBAGA1UECxMJb3BlbnNoaWZ0MS4wLAYDVQQDEyVrdWJlLWFwaXNlcnZlci1zZXJ2\naWNlLW5ldHdvcmstc2lnbmVyMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKC\nAQEA3PLsu6vfUb4qCKyXxaExvUXWGRiUrV3b1BiEvyER08XIovGzpHDQ41aD7mxT\nw1+mjW19MsSWD8XXLHaj8ad6GiZ3aNJ2iIAp6PGATQrKWY+LlOFnSrk21XArYOx3\n8HNx6kjgFdQOs3A4unbbCBNQh3MTbnJXfUaSDNBEYY+QMWaHSAoQPVThS4VcD6rI\nF8d7GG726ogUuAM4eDum3b7Dw6h9xZWFHS9A3L/05S1jD/p6r/E7BghRNRvvJsSm\nM78c68fQmygMIwAHYV0iQTXYzFl4qeMovCUTlm933P56flpghXTLbV858pJEiVKq\neInZ6QWrH4oEp1Wvduxsm6I3AQIDAQABo0IwQDAOBgNVHQ8BAf8EBAMCAqQwDwYD\nVR0TAQH/BAUwAwEB/zAdBgNVHQ4EFgQUxgNzyToLk4YHrryxwHRdCi1YVZ4wDQYJ\nKoZIhvcNAQELBQADggEBACMEI0/eYMPlJf/TKVlQh+8zuFaqsmV4SEo4bcz2QR/5\nytw7cC3W87lCAJ+yhWLF0c8ddjn2EgckASE97MVEvdY7z/JqD48tzS4MTBzKu5V6\n7g4qX0N8sPG8i7fKeB5VIdtpvk1THhwpJBgAYeRs2Vt3oR3hts38Hunf1Fm1Iq9a\nuC2a63NTfI5SFAg2jiICuokbdPcRSJWQhufS5vwBR5rTAoyP2iVJLtHrtPp+2+Fq\nO7G4qMDY8M8sVOsmTLnuXdoS7z+j9bwMGhYZG3xW3nvgEFLUEtB62OjM/8YQGzKj\n8PBR8CmLd/3Zz8/Bf4nIwJ9bB9acCCYHXh2Vr23X0sE=\n-----END CERTIFICATE-----\n-----BEGIN CERTIFICATE-----\nMIIDlzCCAn+gAwIBAgIIEtJWch74/5wwDQYJKoZIhvcNAQELBQAwWTFXMFUGA1UE\nAwxOb3BlbnNoaWZ0LWt1YmUtYXBpc2VydmVyLW9wZXJhdG9yX2xvY2FsaG9zdC1y\nZWNvdmVyeS1zZXJ2aW5nLXNpZ25lckAxNzE4MTAyOTAzMB4XDTI0MDYxMTEwNDgy\nMloXDTM0MDYwOTEwNDgyM1owWTFXMFUGA1UEAwxOb3BlbnNoaWZ0LWt1YmUtYXBp\nc2VydmVyLW9wZXJhdG9yX2xvY2FsaG9zdC1yZWNvdmVyeS1zZXJ2aW5nLXNpZ25l\nckAxNzE4MTAyOTAzMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA2gfY\nieeA1s5yIPIyr4nreolJtWJU316iq04zgL77CwCJSCLdqovBxvSFgaprS5Cd+V3+\n7w3t5JrZ+/o1DeVfvMGcQdeYG0ruYp6tPAqzbV9huKjXNC4B1tipWTWL0xKfjfol\nVJJ+idbKQQm5cFg0q4MgwG8gT0B8sfJr7Tnz9pfjmUffcTqHk9xkq8A6ba+HnWc+\nAA5iZl7fmGUAtIRwaFHDZgMTGER7Hy7iY6DlaZhga2tHwk2srdj80gxFDutzIDXm\nDQla8I2ZEoEVbWZndub5JPpl0OrVc1l6JZ1Zo4dKz0Dg9ef9vd/AKTinVx+9sFUy\nXKBsbJnqWssuUNMYbwIDAQABo2MwYTAOBgNVHQ8BAf8EBAMCAqQwDwYDVR0TAQH/\nBAUwAwEB/zAdBgNVHQ4EFgQUVC5qGhSPlvXcUxjPlGQx5kRN+JAwHwYDVR0jBBgw\nFoAUVC5qGhSPlvXcUxjPlGQx5kRN+JAwDQYJKoZIhvcNAQELBQADggEBADlCo4YF\naa3VGBLnkAvygstPOaeCRAeUXTVPeFeo6IgkrNBrkR2l1lP3cmH2XhGfxEbTILxT\nenFFwDjhM1zm4LsygLqA9FC08TzWuQyS1uDOajvLaXxtam6s6d3N6nDAElZzuwh2\n19XYLGEFWxgs1Pt71KoN16kVdPW/rVFgCPFyn0CVTjbPeoPSmqgJbsqyK/Rs2VOR\n4segDUoBmTmGYAsBx0Kq9BuOAAtfaA7UlluIQ8q5qrGd9xQ/3s8qmCV+syah0ljJ\nX0/Z2S0DrWBqX5bzkR6i6vzUg/ACMt+5B/j++NCyqs03ls862GD7AeiP8jaS51a2\nVBPno2MjcbkakiI=\n-----END CERTIFICATE-----\n-----BEGIN CERTIFICATE-----\nMIIDsTCCApmgAwIBAgIIdCvH9RE3lzgwDQYJKoZIhvcNAQELBQAwJjEkMCIGA1UE\nAwwbaW5ncmVzcy1vcGVyYXRvckAxNzE4MTAyOTYzMB4XDTI0MDYxMTEwNDkyNFoX\nDTI2MDYxMTEwNDkyNVowSDFGMEQGA1UEAww9Ki5hcHBzLmNpLW9wLTl4eDcxcnZx\nLTFlMjhlLnFlLmF6dXJlLmRldmNsdXN0ZXIub3BlbnNoaWZ0LmNvbTCCASIwDQYJ\nKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMEXt7td5wBAHJLibHXL3j6/zeypnUlZ\nzFwfnYhxHFpmHdDXkj4+8yXVj/asJOtL9gTHh9VqNwnSr1slC+yq16+KEQPMGiwc\nZ94MpCUV4zNHTHqxCXOHWFCmQ+gXZXExRpWnZXN6jzUv29dCax/XJ+xA+Rre2Aly\nB45hkmfnDx3tggIcypQoFG3MMJVZxktKJZA/W9R/rRcQyYHRvIaWD96CNzaApjnR\nZ9l+FJ3U8zXEb2jUeNH9oa2/t2zPc5JOwaHFG0xaTPyTIif77qES6XbaqJgco6zr\ncCfim6/w6/9oaYcz9MD6cR1hGAoxokGphkazISMxL3UroLA+SFZ/cj8CAwEAAaOB\nwDCBvTAOBgNVHQ8BAf8EBAMCBaAwEwYDVR0lBAwwCgYIKwYBBQUHAwEwDAYDVR0T\nAQH/BAIwADAdBgNVHQ4EFgQUxxTcJW1j1SIm1srEoxJRfxoYXv0wHwYDVR0jBBgw\nFoAU0EJZY45sDzwQHJ9Fbs4J0+5YQXAwSAYDVR0RBEEwP4I9Ki5hcHBzLmNpLW9w\nLTl4eDcxcnZxLTFlMjhlLnFlLmF6dXJlLmRldmNsdXN0ZXIub3BlbnNoaWZ0LmNv\nbTANBgkqhkiG9w0BAQsFAAOCAQEAJyrM8ojm2I0nTliQ9ax/wRX5mFQ9rB00O/Oo\nGIeyObuNTQPkyxEjAr7s/Gdh0FriZMZbfYj1LeEg9kwjRsufzZqxvpLmJxMPFZxp\nol4n8LNctWlYIWWG4KH2edc6c/pvBaxOJC9R0MgnjuNqPfvrKAk1OcNada1QQF4H\nPHJGeDSR2nBUBJj8WTLD9EbAaXkuiK2hf6xBBKsn3+Eu8wOVtphxs7CpPj3bwZsM\nn5N7IqLMaE9exNFXVMd+0tALBpte6UmpRG7YpAzNkDSCVHVTR9H+KdpdPmTyZ8Al\n+LGjhyal9KyTiDFwlwUa3hnK2P3w4Q0EjH7jRBj85bALW2454w==\n-----END CERTIFICATE-----\n-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIBATANBgkqhkiG9w0BAQsFADAmMSQwIgYDVQQDDBtpbmdy\nZXNzLW9wZXJhdG9yQDE3MTgxMDI5NjMwHhcNMjQwNjExMTA0OTIzWhcNMjYwNjEx\nMTA0OTI0WjAmMSQwIgYDVQQDDBtpbmdyZXNzLW9wZXJhdG9yQDE3MTgxMDI5NjMw\nggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDWuPsQsmr2xRHEfP71JdYl\n0GBQDYw3/OmIHjvzE6uOfM1PAGtVQ482iExzYG0AIJq5AFMr7L3+XJuPdaDiudWu\n2+x32AOlFayWaWzBnbIqkdT1Gmotprgj5qXnCVI2maXzsQbJwefEFMfjkq0+4hs0\nK1y5aKQKqgNP8cl4OdENL3J7B1AJT4i3zqHB7Uu1li72TYRs0K68+IVN37GdBFi4\nkpdpOLTjE+akA2mg7A9MbC5gZsIN0hfbBNxhkPRf8137YfajsfCCxMLbjb94mW5M\npcNZAfMnrFTMgAX3ADOckuw2EndjLmPyRjQHDplJrcjF4TxQ83IReEZRNcvvXN1v\nAgMBAAGjRTBDMA4GA1UdDwEB/wQEAwICpDASBgNVHRMBAf8ECDAGAQH/AgEAMB0G\nA1UdDgQWBBTQQlljjmwPPBAcn0VuzgnT7lhBcDANBgkqhkiG9w0BAQsFAAOCAQEA\nUjk7rdAYEprDClAOKLCbnuv1FiAFrZgSp+JHHxujTTupCisSmcLeShXtg9g0DJ+l\nae2bW9ukn4UVTrXjIXgurE98HhOXf+7+VGDlFehPcfBNvwHr9vbOI1TXfyB0bPov\nm4YAfVVY7XEH9yDCqzPIWuEfPyBlY1/k+XPezEoMukrAT5d49nuBdtqsIN3cQEN9\n+0HTffr3BEZcxBdDAhp2/tpmxwPWcC4G4W9V/ptCKBoYDarSzCUViTvEV2VpC8Yl\nv24kgR5RgQWWaLN1jUI6lxZtcUVelHV98UBTpjhNPXPEVjzWhUzxLcPjwJXfLPyA\nCCnuScmLsxK+fnsBo6WM0Q==\n-----END CERTIFICATE-----\n"},"metadata":{"creationTimestamp":null,"managedFields":null,"resourceVersion":null,"uid":null}}
2024-06-11T10:50:27.356834754Z I0611 10:50:27.356759       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapUpdated' Updated ConfigMap/serviceaccount-ca -n openshift-kube-scheduler:
2024-06-11T10:50:27.356834754Z cause by changes in data.ca-bundle.crt
2024-06-11T10:50:27.359608186Z I0611 10:50:27.359493       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'StartingNewRevision' new revision 5 triggered by "required configmap/serviceaccount-ca has changed"
2024-06-11T10:50:28.187811339Z E0611 10:50:28.187706       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:50:28.187811339Z E0611 10:50:28.187749       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:50:28.188060050Z E0611 10:50:28.188024       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:50:28.594734174Z I0611 10:50:28.594667       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/kube-scheduler-pod-5 -n openshift-kube-scheduler because it was missing
2024-06-11T10:50:28.784424287Z I0611 10:50:28.784368       1 request.go:697] Waited for 1.012876528s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler
2024-06-11T10:50:28.991096707Z I0611 10:50:28.991032       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-0" is in transition to 4, but has not made progress because static pod is pending
2024-06-11T10:50:29.592478182Z I0611 10:50:29.592405       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/config-5 -n openshift-kube-scheduler because it was missing
2024-06-11T10:50:30.197620136Z I0611 10:50:30.197292       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/serviceaccount-ca-5 -n openshift-kube-scheduler because it was missing
2024-06-11T10:50:30.468939527Z E0611 10:50:30.468868       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:50:30.468939527Z E0611 10:50:30.468917       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:50:30.790436603Z I0611 10:50:30.790344       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/scheduler-kubeconfig-5 -n openshift-kube-scheduler because it was missing
2024-06-11T10:50:31.584174918Z I0611 10:50:31.584107       1 request.go:697] Waited for 1.114572459s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ci-op-9xx71rvq-1e28e-w667k-master-0
2024-06-11T10:50:32.190226315Z I0611 10:50:32.190151       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/kube-scheduler-cert-syncer-kubeconfig-5 -n openshift-kube-scheduler because it was missing
2024-06-11T10:50:32.584797563Z I0611 10:50:32.584741       1 request.go:697] Waited for 1.192522263s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:50:32.988134628Z E0611 10:50:32.988068       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:50:32.988466244Z E0611 10:50:32.988435       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:50:33.584880583Z I0611 10:50:33.584806       1 request.go:697] Waited for 1.394690669s due to client-side throttling, not priority and fairness, request: POST:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/secrets
2024-06-11T10:50:33.594007916Z I0611 10:50:33.593941       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/serving-cert-5 -n openshift-kube-scheduler because it was missing
2024-06-11T10:50:34.387209726Z I0611 10:50:34.387143       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-0" is in transition to 4, but has not made progress because static pod is pending
2024-06-11T10:50:34.784006762Z I0611 10:50:34.783936       1 request.go:697] Waited for 1.189966045s due to client-side throttling, not priority and fairness, request: POST:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/secrets
2024-06-11T10:50:34.794332605Z I0611 10:50:34.794260       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/localhost-recovery-client-token-5 -n openshift-kube-scheduler because it was missing
2024-06-11T10:50:35.398497645Z E0611 10:50:35.398382       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:50:35.398727454Z E0611 10:50:35.398670       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:50:35.784770529Z I0611 10:50:35.784692       1 request.go:697] Waited for 1.197363408s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/secrets/localhost-recovery-client-token
2024-06-11T10:50:36.001018213Z I0611 10:50:36.000944       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionTriggered' new revision 5 triggered by "required configmap/serviceaccount-ca has changed"
2024-06-11T10:50:36.014256982Z I0611 10:50:36.014186       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionCreate' Revision 5 created because required configmap/serviceaccount-ca has changed
2024-06-11T10:50:36.015837549Z I0611 10:50:36.015771       1 prune_controller.go:269] Nothing to prune
2024-06-11T10:50:36.032895982Z W0611 10:50:36.032838       1 staticpod.go:38] revision 5 is unexpectedly already the latest available revision. This is a possible race!
2024-06-11T10:50:36.032895982Z E0611 10:50:36.032879       1 base_controller.go:268] RevisionController reconciliation failed: conflicting latestAvailableRevision 5
2024-06-11T10:50:36.785137879Z I0611 10:50:36.785033       1 request.go:697] Waited for 1.385310977s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ci-op-9xx71rvq-1e28e-w667k-master-0
2024-06-11T10:50:37.984178858Z I0611 10:50:37.984105       1 request.go:697] Waited for 1.195647734s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ci-op-9xx71rvq-1e28e-w667k-master-0
2024-06-11T10:50:37.987659008Z E0611 10:50:37.987618       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:50:37.988084726Z E0611 10:50:37.988045       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:50:37.988676051Z E0611 10:50:37.988621       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:50:38.188274221Z I0611 10:50:38.188216       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-0" is in transition to 4, but has not made progress because static pod is pending
2024-06-11T10:50:38.206729113Z I0611 10:50:38.206669       1 prune_controller.go:269] Nothing to prune
2024-06-11T10:50:38.207388342Z I0611 10:50:38.207349       1 status_controller.go:218] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2024-06-11T10:50:20Z","message":"GuardControllerDegraded: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-06-11T10:48:34Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 5","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-06-11T10:48:20Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 5","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-06-11T10:48:31Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-06-11T10:48:20Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2024-06-11T10:50:38.220729314Z I0611 10:50:38.220670       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Progressing message changed from "NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 4" to "NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 5",Available message changed from "StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 4" to "StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 5"
2024-06-11T10:50:38.988362472Z I0611 10:50:38.987474       1 request.go:697] Waited for 1.096641583s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa
2024-06-11T10:50:39.188026644Z I0611 10:50:39.187957       1 installer_controller.go:500] "ci-op-9xx71rvq-1e28e-w667k-master-0" moving to (v1.NodeStatus) {
2024-06-11T10:50:39.188026644Z  NodeName: (string) (len=35) "ci-op-9xx71rvq-1e28e-w667k-master-0",
2024-06-11T10:50:39.188026644Z  CurrentRevision: (int32) 0,
2024-06-11T10:50:39.188026644Z  TargetRevision: (int32) 5,
2024-06-11T10:50:39.188026644Z  LastFailedRevision: (int32) 0,
2024-06-11T10:50:39.188026644Z  LastFailedTime: (*v1.Time)(<nil>),
2024-06-11T10:50:39.188026644Z  LastFailedReason: (string) "",
2024-06-11T10:50:39.188026644Z  LastFailedCount: (int) 0,
2024-06-11T10:50:39.188026644Z  LastFallbackCount: (int) 0,
2024-06-11T10:50:39.188026644Z  LastFailedRevisionErrors: ([]string) <nil>
2024-06-11T10:50:39.188026644Z }
2024-06-11T10:50:39.188026644Z  because new revision pending
2024-06-11T10:50:39.220507639Z I0611 10:50:39.220451       1 prune_controller.go:269] Nothing to prune
2024-06-11T10:50:41.184680769Z I0611 10:50:41.184614       1 request.go:697] Waited for 1.115142877s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ci-op-9xx71rvq-1e28e-w667k-master-0
2024-06-11T10:50:41.405278540Z I0611 10:50:41.405197       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/installer-5-ci-op-9xx71rvq-1e28e-w667k-master-0 -n openshift-kube-scheduler because it was missing
2024-06-11T10:50:42.187764050Z E0611 10:50:42.187691       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:50:42.187764050Z E0611 10:50:42.187729       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:50:42.187985560Z E0611 10:50:42.187940       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:50:42.388124999Z I0611 10:50:42.388060       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-0" is in transition to 5, but has not made progress because installer is not finished, but in Pending phase
2024-06-11T10:50:42.584477069Z I0611 10:50:42.584404       1 request.go:697] Waited for 1.179122626s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller
2024-06-11T10:50:43.584586538Z I0611 10:50:43.584527       1 request.go:697] Waited for 1.1955831s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-5-ci-op-9xx71rvq-1e28e-w667k-master-0
2024-06-11T10:50:44.591254300Z E0611 10:50:44.591200       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:50:44.591254300Z E0611 10:50:44.591231       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:50:44.591555913Z E0611 10:50:44.591497       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:50:44.591844826Z E0611 10:50:44.591808       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:50:44.591865727Z E0611 10:50:44.591846       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:50:44.784809545Z I0611 10:50:44.784747       1 request.go:697] Waited for 1.184607209s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-5-ci-op-9xx71rvq-1e28e-w667k-master-0
2024-06-11T10:50:44.793415729Z I0611 10:50:44.793348       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-0" is in transition to 5, but has not made progress because installer is not finished, but in Running phase
2024-06-11T10:50:46.390036541Z E0611 10:50:46.389972       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:50:46.587238348Z I0611 10:50:46.587175       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-0" is in transition to 5, but has not made progress because installer is not finished, but in Running phase
2024-06-11T10:50:48.188694676Z E0611 10:50:48.188505       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:50:48.188694676Z E0611 10:50:48.188567       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:50:48.188774279Z E0611 10:50:48.188743       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:50:52.395730717Z E0611 10:50:52.395669       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:50:52.395730717Z E0611 10:50:52.395717       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:50:52.396042631Z E0611 10:50:52.395996       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:50:54.371541856Z E0611 10:50:54.371473       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:50:54.371541856Z E0611 10:50:54.371518       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:50:54.371833169Z E0611 10:50:54.371790       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:50:57.396189799Z E0611 10:50:57.396137       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:50:57.396189799Z E0611 10:50:57.396168       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:50:57.396418609Z E0611 10:50:57.396386       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:51:03.831006911Z E0611 10:51:03.830741       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:51:03.831006911Z E0611 10:51:03.830791       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:51:03.831076814Z E0611 10:51:03.831062       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:51:08.487350539Z E0611 10:51:08.487257       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:51:08.487350539Z E0611 10:51:08.487320       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:51:08.487587750Z E0611 10:51:08.487554       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:51:08.495649507Z E0611 10:51:08.495606       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:51:08.495649507Z E0611 10:51:08.495639       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:51:08.495871217Z E0611 10:51:08.495841       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:51:09.617540025Z E0611 10:51:09.617479       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:51:09.617540025Z E0611 10:51:09.617524       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:51:09.617795436Z E0611 10:51:09.617752       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:51:10.842916828Z E0611 10:51:10.842857       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:51:10.842916828Z E0611 10:51:10.842892       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:51:10.843101436Z E0611 10:51:10.843072       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:51:14.713147551Z I0611 10:51:14.713088       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-0" is in transition to 5, but has not made progress because installer is not finished, but in Running phase
2024-06-11T10:51:14.714445510Z E0611 10:51:14.714404       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:51:14.714445510Z E0611 10:51:14.714434       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:51:14.714666720Z E0611 10:51:14.714638       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:51:15.773172126Z E0611 10:51:15.773113       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:51:15.773172126Z E0611 10:51:15.773148       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:51:15.773406236Z E0611 10:51:15.773353       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:51:15.773660348Z E0611 10:51:15.773629       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:51:15.773774553Z E0611 10:51:15.773752       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:51:15.786371627Z E0611 10:51:15.786319       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:51:16.111549036Z E0611 10:51:16.111473       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:51:16.111549036Z E0611 10:51:16.111507       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:51:16.111741845Z E0611 10:51:16.111687       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:51:16.711934178Z I0611 10:51:16.711870       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-0" is in transition to 5, but has not made progress because waiting for static pod of revision 5, found 4
2024-06-11T10:51:17.711698809Z E0611 10:51:17.711634       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:51:17.711698809Z E0611 10:51:17.711668       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:51:17.711916719Z E0611 10:51:17.711871       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:51:17.712123328Z E0611 10:51:17.712099       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:51:18.711817855Z I0611 10:51:18.711751       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-0" is in transition to 5, but has not made progress because waiting for static pod of revision 5, found 4
2024-06-11T10:51:18.913010718Z E0611 10:51:18.912926       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:51:18.913142924Z E0611 10:51:18.913119       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:51:25.349394742Z E0611 10:51:25.349177       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:51:25.349394742Z E0611 10:51:25.349238       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:51:25.349610784Z E0611 10:51:25.349516       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:51:25.352191588Z I0611 10:51:25.352141       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-0" is in transition to 5, but has not made progress because waiting for static pod of revision 5, found 4
2024-06-11T10:51:26.096878987Z E0611 10:51:26.096814       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:51:26.096878987Z E0611 10:51:26.096852       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:51:26.130340320Z E0611 10:51:26.130285       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:51:26.130564864Z E0611 10:51:26.130536       1 guard_controller.go:293] Missing PodIP in operand openshift-kube-scheduler-ci-op-9xx71rvq-1e28e-w667k-master-0 on node ci-op-9xx71rvq-1e28e-w667k-master-0
2024-06-11T10:51:26.130585468Z E0611 10:51:26.130565       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:51:26.130585468Z E0611 10:51:26.130582       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:51:26.151401732Z E0611 10:51:26.151347       1 base_controller.go:268] GuardController reconciliation failed: [Missing PodIP in operand openshift-kube-scheduler-ci-op-9xx71rvq-1e28e-w667k-master-0 on node ci-op-9xx71rvq-1e28e-w667k-master-0, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:51:26.155344502Z I0611 10:51:26.155281       1 prune_controller.go:269] Nothing to prune
2024-06-11T10:51:26.155978326Z I0611 10:51:26.155925       1 status_controller.go:218] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2024-06-11T10:50:20Z","message":"GuardControllerDegraded: [Missing PodIP in operand openshift-kube-scheduler-ci-op-9xx71rvq-1e28e-w667k-master-0 on node ci-op-9xx71rvq-1e28e-w667k-master-0, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-06-11T10:48:34Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 5","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-06-11T10:48:20Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 5","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-06-11T10:48:31Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-06-11T10:48:20Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2024-06-11T10:51:26.169115191Z I0611 10:51:26.169045       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded message changed from "GuardControllerDegraded: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]" to "GuardControllerDegraded: [Missing PodIP in operand openshift-kube-scheduler-ci-op-9xx71rvq-1e28e-w667k-master-0 on node ci-op-9xx71rvq-1e28e-w667k-master-0, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]"
2024-06-11T10:51:27.326564480Z I0611 10:51:27.326500       1 request.go:697] Waited for 1.170761589s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler
2024-06-11T10:51:27.731388422Z I0611 10:51:27.731293       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-0" is in transition to 5, but has not made progress because static pod is pending
2024-06-11T10:51:28.326619739Z I0611 10:51:28.326523       1 request.go:697] Waited for 1.196503015s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/configmaps/serviceaccount-ca
2024-06-11T10:51:29.330635830Z E0611 10:51:29.330571       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:51:29.330635830Z E0611 10:51:29.330604       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:51:29.342107540Z E0611 10:51:29.342016       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:51:29.342787170Z I0611 10:51:29.342736       1 prune_controller.go:269] Nothing to prune
2024-06-11T10:51:29.343618407Z I0611 10:51:29.343578       1 status_controller.go:218] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2024-06-11T10:50:20Z","message":"GuardControllerDegraded: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-06-11T10:48:34Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 5","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-06-11T10:48:20Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 5","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2024-06-11T10:48:31Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-06-11T10:48:20Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2024-06-11T10:51:29.355969357Z I0611 10:51:29.355913       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded message changed from "GuardControllerDegraded: [Missing PodIP in operand openshift-kube-scheduler-ci-op-9xx71rvq-1e28e-w667k-master-0 on node ci-op-9xx71rvq-1e28e-w667k-master-0, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]" to "GuardControllerDegraded: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]"
2024-06-11T10:51:29.526805059Z I0611 10:51:29.526736       1 request.go:697] Waited for 1.196609338s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/localhost-recovery-client
2024-06-11T10:51:30.727203365Z I0611 10:51:30.727122       1 request.go:697] Waited for 1.197200209s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/secrets/localhost-recovery-client-token
2024-06-11T10:51:31.331845818Z I0611 10:51:31.331784       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-0" is in transition to 5, but has not made progress because static pod is pending
2024-06-11T10:51:31.730546816Z E0611 10:51:31.730488       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:51:31.730546816Z E0611 10:51:31.730522       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:51:31.730722604Z E0611 10:51:31.730696       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:51:31.926894760Z I0611 10:51:31.926812       1 request.go:697] Waited for 1.196378134s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/configmaps/scheduler-kubeconfig
2024-06-11T10:51:33.730194189Z E0611 10:51:33.730135       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:51:33.730194189Z E0611 10:51:33.730174       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:51:33.730486436Z E0611 10:51:33.730450       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:51:34.530433476Z I0611 10:51:34.530369       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-0" is in transition to 5, but has not made progress because static pod is pending
2024-06-11T10:51:35.529857655Z E0611 10:51:35.529799       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:51:35.529857655Z E0611 10:51:35.529831       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:51:35.530069964Z E0611 10:51:35.530035       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:51:41.492986738Z E0611 10:51:41.492903       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:51:41.492986738Z E0611 10:51:41.492954       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:51:41.493284752Z E0611 10:51:41.493237       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:51:55.268350038Z E0611 10:51:55.268259       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:51:55.268399149Z E0611 10:51:55.268342       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:51:55.268612295Z E0611 10:51:55.268581       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:52:09.507094278Z I0611 10:52:09.507030       1 tlsconfig.go:178] "Loaded client CA" index=0 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"admin-kubeconfig-signer\" [] issuer=\"<self>\" (2024-06-11 10:19:00 +0000 UTC to 2034-06-09 10:19:00 +0000 UTC (now=2024-06-11 10:52:09.506977657 +0000 UTC))"
2024-06-11T10:52:09.509251682Z I0611 10:52:09.509174       1 tlsconfig.go:178] "Loaded client CA" index=1 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kubelet-signer\" [] issuer=\"<self>\" (2024-06-11 10:19:05 +0000 UTC to 2024-06-12 10:19:05 +0000 UTC (now=2024-06-11 10:52:09.509123658 +0000 UTC))"
2024-06-11T10:52:09.509251682Z I0611 10:52:09.509235       1 tlsconfig.go:178] "Loaded client CA" index=2 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kube-control-plane-signer\" [] issuer=\"<self>\" (2024-06-11 10:19:05 +0000 UTC to 2025-06-11 10:19:05 +0000 UTC (now=2024-06-11 10:52:09.509207474 +0000 UTC))"
2024-06-11T10:52:09.509288889Z I0611 10:52:09.509271       1 tlsconfig.go:178] "Loaded client CA" index=3 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kube-apiserver-to-kubelet-signer\" [] issuer=\"<self>\" (2024-06-11 10:19:05 +0000 UTC to 2025-06-11 10:19:05 +0000 UTC (now=2024-06-11 10:52:09.509249381 +0000 UTC))"
2024-06-11T10:52:09.509349700Z I0611 10:52:09.509336       1 tlsconfig.go:178] "Loaded client CA" index=4 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kubelet-bootstrap-kubeconfig-signer\" [] issuer=\"<self>\" (2024-06-11 10:19:01 +0000 UTC to 2034-06-09 10:19:01 +0000 UTC (now=2024-06-11 10:52:09.509284188 +0000 UTC))"
2024-06-11T10:52:09.509407911Z I0611 10:52:09.509373       1 tlsconfig.go:178] "Loaded client CA" index=5 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kube-csr-signer_@1718102903\" [] issuer=\"kubelet-signer\" (2024-06-11 10:48:22 +0000 UTC to 2024-06-12 10:19:05 +0000 UTC (now=2024-06-11 10:52:09.5093508 +0000 UTC))"
2024-06-11T10:52:09.509512631Z I0611 10:52:09.509469       1 tlsconfig.go:178] "Loaded client CA" index=6 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"openshift-kube-apiserver-operator_node-system-admin-signer@1718102900\" [] issuer=\"<self>\" (2024-06-11 10:48:20 +0000 UTC to 2025-06-11 10:48:21 +0000 UTC (now=2024-06-11 10:52:09.509419613 +0000 UTC))"
2024-06-11T10:52:09.509556739Z I0611 10:52:09.509516       1 tlsconfig.go:178] "Loaded client CA" index=7 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"aggregator-signer\" [] issuer=\"<self>\" (2024-06-11 10:19:02 +0000 UTC to 2024-06-12 10:19:02 +0000 UTC (now=2024-06-11 10:52:09.509491027 +0000 UTC))"
2024-06-11T10:52:09.510263071Z I0611 10:52:09.510206       1 tlsconfig.go:200] "Loaded serving cert" certName="serving-cert::/var/run/secrets/serving-cert/tls.crt::/var/run/secrets/serving-cert/tls.key" certDetail="\"metrics.openshift-kube-scheduler-operator.svc\" [serving] validServingFor=[metrics.openshift-kube-scheduler-operator.svc,metrics.openshift-kube-scheduler-operator.svc.cluster.local] issuer=\"openshift-service-serving-signer@1718102902\" (2024-06-11 10:48:33 +0000 UTC to 2026-06-11 10:48:34 +0000 UTC (now=2024-06-11 10:52:09.510174755 +0000 UTC))"
2024-06-11T10:52:09.510885187Z I0611 10:52:09.510845       1 named_certificates.go:53] "Loaded SNI cert" index=0 certName="self-signed loopback" certDetail="\"apiserver-loopback-client@1718102987\" [serving] validServingFor=[apiserver-loopback-client] issuer=\"apiserver-loopback-client-ca@1718102986\" (2024-06-11 09:49:46 +0000 UTC to 2025-06-11 09:49:46 +0000 UTC (now=2024-06-11 10:52:09.510804172 +0000 UTC))"
2024-06-11T10:52:16.167383149Z E0611 10:52:16.167293       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:52:16.167383149Z E0611 10:52:16.167350       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:52:16.167558180Z E0611 10:52:16.167524       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:52:16.172577873Z I0611 10:52:16.172532       1 installer_controller.go:500] "ci-op-9xx71rvq-1e28e-w667k-master-0" moving to (v1.NodeStatus) {
2024-06-11T10:52:16.172577873Z  NodeName: (string) (len=35) "ci-op-9xx71rvq-1e28e-w667k-master-0",
2024-06-11T10:52:16.172577873Z  CurrentRevision: (int32) 5,
2024-06-11T10:52:16.172577873Z  TargetRevision: (int32) 0,
2024-06-11T10:52:16.172577873Z  LastFailedRevision: (int32) 0,
2024-06-11T10:52:16.172577873Z  LastFailedTime: (*v1.Time)(<nil>),
2024-06-11T10:52:16.172577873Z  LastFailedReason: (string) "",
2024-06-11T10:52:16.172577873Z  LastFailedCount: (int) 0,
2024-06-11T10:52:16.172577873Z  LastFallbackCount: (int) 0,
2024-06-11T10:52:16.172577873Z  LastFailedRevisionErrors: ([]string) <nil>
2024-06-11T10:52:16.172577873Z }
2024-06-11T10:52:16.172577873Z  because static pod is ready
2024-06-11T10:52:16.186355724Z I0611 10:52:16.186281       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeCurrentRevisionChanged' Updated node "ci-op-9xx71rvq-1e28e-w667k-master-0" from revision 0 to 5 because static pod is ready
2024-06-11T10:52:16.186446640Z I0611 10:52:16.186358       1 prune_controller.go:269] Nothing to prune
2024-06-11T10:52:16.187705664Z I0611 10:52:16.187649       1 status_controller.go:218] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2024-06-11T10:50:20Z","message":"GuardControllerDegraded: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-06-11T10:48:34Z","message":"NodeInstallerProgressing: 2 nodes are at revision 0; 1 node is at revision 5","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-06-11T10:52:16Z","message":"StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 node is at revision 5","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2024-06-11T10:48:31Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-06-11T10:48:20Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2024-06-11T10:52:16.200997529Z I0611 10:52:16.200900       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Progressing message changed from "NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 5" to "NodeInstallerProgressing: 2 nodes are at revision 0; 1 node is at revision 5",Available changed from False to True ("StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 node is at revision 5")
2024-06-11T10:52:17.554716758Z I0611 10:52:17.554660       1 installer_controller.go:524] node ci-op-9xx71rvq-1e28e-w667k-master-1 static pod not found and needs new revision 5
2024-06-11T10:52:17.554764866Z I0611 10:52:17.554723       1 installer_controller.go:532] "ci-op-9xx71rvq-1e28e-w667k-master-1" moving to (v1.NodeStatus) {
2024-06-11T10:52:17.554764866Z  NodeName: (string) (len=35) "ci-op-9xx71rvq-1e28e-w667k-master-1",
2024-06-11T10:52:17.554764866Z  CurrentRevision: (int32) 0,
2024-06-11T10:52:17.554764866Z  TargetRevision: (int32) 5,
2024-06-11T10:52:17.554764866Z  LastFailedRevision: (int32) 0,
2024-06-11T10:52:17.554764866Z  LastFailedTime: (*v1.Time)(<nil>),
2024-06-11T10:52:17.554764866Z  LastFailedReason: (string) "",
2024-06-11T10:52:17.554764866Z  LastFailedCount: (int) 0,
2024-06-11T10:52:17.554764866Z  LastFallbackCount: (int) 0,
2024-06-11T10:52:17.554764866Z  LastFailedRevisionErrors: ([]string) <nil>
2024-06-11T10:52:17.554764866Z }
2024-06-11T10:52:17.566552763Z I0611 10:52:17.566468       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeTargetRevisionChanged' Updating node "ci-op-9xx71rvq-1e28e-w667k-master-1" from revision 0 to 5 because node ci-op-9xx71rvq-1e28e-w667k-master-1 static pod not found
2024-06-11T10:52:17.567651059Z I0611 10:52:17.567597       1 prune_controller.go:269] Nothing to prune
2024-06-11T10:52:19.364891655Z I0611 10:52:19.364804       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/installer-5-ci-op-9xx71rvq-1e28e-w667k-master-1 -n openshift-kube-scheduler because it was missing
2024-06-11T10:52:19.365014977Z E0611 10:52:19.364976       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:52:19.365038382Z E0611 10:52:19.365028       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:52:20.155960157Z I0611 10:52:20.155854       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-1" is in transition to 5, but has not made progress because installer is not finished, but in Pending phase
2024-06-11T10:52:20.552637520Z I0611 10:52:20.552564       1 request.go:697] Waited for 1.186945216s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ci-op-9xx71rvq-1e28e-w667k-master-0
2024-06-11T10:52:21.751812111Z I0611 10:52:21.751751       1 request.go:697] Waited for 1.187649648s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ci-op-9xx71rvq-1e28e-w667k-master-0
2024-06-11T10:52:21.755336266Z E0611 10:52:21.755273       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:52:21.755626620Z E0611 10:52:21.755578       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:52:21.755626620Z E0611 10:52:21.755618       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:52:22.555465753Z I0611 10:52:22.555403       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-1" is in transition to 5, but has not made progress because installer is not finished, but in Running phase
2024-06-11T10:52:22.751991598Z I0611 10:52:22.751925       1 request.go:697] Waited for 1.195523812s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller
2024-06-11T10:52:23.955457787Z E0611 10:52:23.955392       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:52:24.591409944Z I0611 10:52:24.590921       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-1" is in transition to 5, but has not made progress because installer is not finished, but in Running phase
2024-06-11T10:52:36.015863852Z E0611 10:52:36.015789       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:52:36.025815481Z E0611 10:52:36.025762       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:52:36.026153540Z E0611 10:52:36.026117       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:52:36.330238892Z E0611 10:52:36.330061       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:52:36.330238892Z E0611 10:52:36.330112       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:52:36.340642000Z E0611 10:52:36.340591       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:52:42.792626654Z E0611 10:52:42.792567       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:52:42.792755576Z E0611 10:52:42.792738       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:52:42.801444872Z E0611 10:52:42.801389       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:52:51.804266096Z E0611 10:52:51.804209       1 guard_controller.go:293] Missing PodIP in operand openshift-kube-scheduler-ci-op-9xx71rvq-1e28e-w667k-master-1 on node ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:52:51.804266096Z E0611 10:52:51.804245       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:52:51.804962111Z I0611 10:52:51.804906       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-1" is in transition to 5, but has not made progress because installer is not finished, but in Running phase
2024-06-11T10:52:51.815706483Z E0611 10:52:51.815657       1 base_controller.go:268] GuardController reconciliation failed: [Missing PodIP in operand openshift-kube-scheduler-ci-op-9xx71rvq-1e28e-w667k-master-1 on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:52:51.815856908Z I0611 10:52:51.815817       1 prune_controller.go:269] Nothing to prune
2024-06-11T10:52:51.817958554Z I0611 10:52:51.817916       1 status_controller.go:218] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2024-06-11T10:50:20Z","message":"GuardControllerDegraded: [Missing PodIP in operand openshift-kube-scheduler-ci-op-9xx71rvq-1e28e-w667k-master-1 on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-06-11T10:48:34Z","message":"NodeInstallerProgressing: 2 nodes are at revision 0; 1 node is at revision 5","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-06-11T10:52:16Z","message":"StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 node is at revision 5","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2024-06-11T10:48:31Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-06-11T10:48:20Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2024-06-11T10:52:51.830103757Z I0611 10:52:51.830049       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded message changed from "GuardControllerDegraded: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]" to "GuardControllerDegraded: [Missing PodIP in operand openshift-kube-scheduler-ci-op-9xx71rvq-1e28e-w667k-master-1 on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]"
2024-06-11T10:52:52.802930794Z I0611 10:52:52.802857       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-1" is in transition to 5, but has not made progress because installer is not finished, but in Running phase
2024-06-11T10:52:53.398436003Z I0611 10:52:53.398377       1 request.go:697] Waited for 1.119092259s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller
2024-06-11T10:52:54.398550240Z I0611 10:52:54.398486       1 request.go:697] Waited for 1.196887789s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ci-op-9xx71rvq-1e28e-w667k-master-0
2024-06-11T10:52:55.398695082Z I0611 10:52:55.398593       1 request.go:697] Waited for 1.196420712s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ci-op-9xx71rvq-1e28e-w667k-master-0
2024-06-11T10:52:56.201200930Z I0611 10:52:56.201124       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-1" is in transition to 5, but has not made progress because static pod is pending
2024-06-11T10:52:57.405219194Z W0611 10:52:57.405152       1 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
2024-06-11T10:52:57.405355516Z E0611 10:52:57.405316       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:52:57.407408255Z I0611 10:52:57.407332       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/openshift-kube-scheduler-guard-ci-op-9xx71rvq-1e28e-w667k-master-1 -n openshift-kube-scheduler because it was missing
2024-06-11T10:52:57.420827468Z E0611 10:52:57.420765       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:52:57.422674673Z I0611 10:52:57.422628       1 prune_controller.go:269] Nothing to prune
2024-06-11T10:52:57.425033562Z I0611 10:52:57.424986       1 status_controller.go:218] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2024-06-11T10:50:20Z","message":"GuardControllerDegraded: Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-06-11T10:48:34Z","message":"NodeInstallerProgressing: 2 nodes are at revision 0; 1 node is at revision 5","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-06-11T10:52:16Z","message":"StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 node is at revision 5","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2024-06-11T10:48:31Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-06-11T10:48:20Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2024-06-11T10:52:57.443636030Z I0611 10:52:57.443494       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded message changed from "GuardControllerDegraded: [Missing PodIP in operand openshift-kube-scheduler-ci-op-9xx71rvq-1e28e-w667k-master-1 on node ci-op-9xx71rvq-1e28e-w667k-master-1, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]" to "GuardControllerDegraded: Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2"
2024-06-11T10:52:58.597997315Z I0611 10:52:58.597930       1 request.go:697] Waited for 1.174979796s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa
2024-06-11T10:52:59.201689552Z I0611 10:52:59.201618       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-1" is in transition to 5, but has not made progress because static pod is pending
2024-06-11T10:52:59.598464300Z I0611 10:52:59.598398       1 request.go:697] Waited for 1.393159088s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller
2024-06-11T10:53:01.398330514Z I0611 10:53:01.398254       1 request.go:697] Waited for 1.086959871s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller
2024-06-11T10:53:02.001338182Z I0611 10:53:02.001235       1 core.go:218] Pod "openshift-kube-scheduler/openshift-kube-scheduler-guard-ci-op-9xx71rvq-1e28e-w667k-master-1" changes: {"apiVersion":"v1","kind":"Pod","metadata":{"annotations":{"k8s.ovn.org/pod-networks":null,"k8s.v1.cni.cncf.io/network-status":null,"target.workload.openshift.io/management":"{\"effect\": \"PreferredDuringScheduling\"}"},"creationTimestamp":null,"managedFields":null,"resourceVersion":null,"uid":null},"spec":{"containers":[{"args":["-c","# properly handle TERM and exit as soon as it is signaled\nset -euo pipefail\ntrap 'jobs -p | xargs -r kill; exit 0' TERM\nsleep infinity \u0026 wait\n"],"command":["/bin/bash"],"image":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:68ae5e595cb6b6ffa3f6861f7a41a92f5db8e9cd77fabb216dd7a96b9c1b4cf5","imagePullPolicy":"IfNotPresent","name":"guard","readinessProbe":{"failureThreshold":3,"httpGet":{"host":"10.0.0.6","path":"healthz","port":10259,"scheme":"HTTPS"},"periodSeconds":5,"successThreshold":1,"timeoutSeconds":5},"resources":{"requests":{"cpu":"10m","memory":"5Mi"}},"terminationMessagePolicy":"FallbackToLogsOnError"}],"dnsPolicy":null,"enableServiceLinks":null,"preemptionPolicy":null,"priority":null,"restartPolicy":null,"schedulerName":null,"securityContext":null,"serviceAccount":null,"serviceAccountName":null,"tolerations":[{"effect":"NoSchedule","key":"node-role.kubernetes.io/master","operator":"Exists"},{"effect":"NoExecute","key":"node.kubernetes.io/not-ready","operator":"Exists"},{"effect":"NoExecute","key":"node.kubernetes.io/unreachable","operator":"Exists"},{"effect":"NoSchedule","key":"node-role.kubernetes.io/etcd","operator":"Exists"}],"volumes":null},"status":{"conditions":null,"containerStatuses":null,"hostIP":null,"hostIPs":null,"phase":null,"podIP":null,"podIPs":null,"qosClass":null,"startTime":null}}
2024-06-11T10:53:02.398539192Z I0611 10:53:02.398478       1 request.go:697] Waited for 1.19665631s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/localhost-recovery-client
2024-06-11T10:53:02.601235009Z I0611 10:53:02.601176       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-1" is in transition to 5, but has not made progress because static pod is pending
2024-06-11T10:53:03.005502647Z E0611 10:53:03.005436       1 guard_controller.go:359] Unable to apply pod openshift-kube-scheduler-guard-ci-op-9xx71rvq-1e28e-w667k-master-1 changes: Operation cannot be fulfilled on pods "openshift-kube-scheduler-guard-ci-op-9xx71rvq-1e28e-w667k-master-1": the object has been modified; please apply your changes to the latest version and try again
2024-06-11T10:53:03.005502647Z E0611 10:53:03.005468       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:53:03.005590863Z I0611 10:53:03.005556       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'PodUpdateFailed' Failed to update Pod/openshift-kube-scheduler-guard-ci-op-9xx71rvq-1e28e-w667k-master-1 -n openshift-kube-scheduler: Operation cannot be fulfilled on pods "openshift-kube-scheduler-guard-ci-op-9xx71rvq-1e28e-w667k-master-1": the object has been modified; please apply your changes to the latest version and try again
2024-06-11T10:53:03.016775706Z E0611 10:53:03.016707       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2, Unable to apply pod openshift-kube-scheduler-guard-ci-op-9xx71rvq-1e28e-w667k-master-1 changes: Operation cannot be fulfilled on pods "openshift-kube-scheduler-guard-ci-op-9xx71rvq-1e28e-w667k-master-1": the object has been modified; please apply your changes to the latest version and try again]
2024-06-11T10:53:03.017060255Z E0611 10:53:03.017014       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:53:03.017271092Z I0611 10:53:03.017239       1 prune_controller.go:269] Nothing to prune
2024-06-11T10:53:03.018231759Z I0611 10:53:03.018187       1 status_controller.go:218] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2024-06-11T10:50:20Z","message":"GuardControllerDegraded: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2, Unable to apply pod openshift-kube-scheduler-guard-ci-op-9xx71rvq-1e28e-w667k-master-1 changes: Operation cannot be fulfilled on pods \"openshift-kube-scheduler-guard-ci-op-9xx71rvq-1e28e-w667k-master-1\": the object has been modified; please apply your changes to the latest version and try again]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-06-11T10:48:34Z","message":"NodeInstallerProgressing: 2 nodes are at revision 0; 1 node is at revision 5","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-06-11T10:52:16Z","message":"StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 node is at revision 5","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2024-06-11T10:48:31Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-06-11T10:48:20Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2024-06-11T10:53:03.028973825Z I0611 10:53:03.028170       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded message changed from "GuardControllerDegraded: Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2" to "GuardControllerDegraded: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2, Unable to apply pod openshift-kube-scheduler-guard-ci-op-9xx71rvq-1e28e-w667k-master-1 changes: Operation cannot be fulfilled on pods \"openshift-kube-scheduler-guard-ci-op-9xx71rvq-1e28e-w667k-master-1\": the object has been modified; please apply your changes to the latest version and try again]"
2024-06-11T10:53:04.197919320Z I0611 10:53:04.197854       1 request.go:697] Waited for 1.180159144s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ci-op-9xx71rvq-1e28e-w667k-master-0
2024-06-11T10:53:05.801469725Z I0611 10:53:05.801391       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-1" is in transition to 5, but has not made progress because static pod is pending
2024-06-11T10:53:07.201422950Z I0611 10:53:07.201354       1 core.go:218] Pod "openshift-kube-scheduler/openshift-kube-scheduler-guard-ci-op-9xx71rvq-1e28e-w667k-master-1" changes: {"apiVersion":"v1","kind":"Pod","metadata":{"annotations":{"k8s.ovn.org/pod-networks":null,"k8s.v1.cni.cncf.io/network-status":null,"target.workload.openshift.io/management":"{\"effect\": \"PreferredDuringScheduling\"}"},"creationTimestamp":null,"managedFields":null,"resourceVersion":null,"uid":null},"spec":{"containers":[{"args":["-c","# properly handle TERM and exit as soon as it is signaled\nset -euo pipefail\ntrap 'jobs -p | xargs -r kill; exit 0' TERM\nsleep infinity \u0026 wait\n"],"command":["/bin/bash"],"image":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:68ae5e595cb6b6ffa3f6861f7a41a92f5db8e9cd77fabb216dd7a96b9c1b4cf5","imagePullPolicy":"IfNotPresent","name":"guard","readinessProbe":{"failureThreshold":3,"httpGet":{"host":"10.0.0.6","path":"healthz","port":10259,"scheme":"HTTPS"},"periodSeconds":5,"successThreshold":1,"timeoutSeconds":5},"resources":{"requests":{"cpu":"10m","memory":"5Mi"}},"terminationMessagePolicy":"FallbackToLogsOnError"}],"dnsPolicy":null,"enableServiceLinks":null,"preemptionPolicy":null,"priority":null,"restartPolicy":null,"schedulerName":null,"securityContext":null,"serviceAccount":null,"serviceAccountName":null,"tolerations":[{"effect":"NoSchedule","key":"node-role.kubernetes.io/master","operator":"Exists"},{"effect":"NoExecute","key":"node.kubernetes.io/not-ready","operator":"Exists"},{"effect":"NoExecute","key":"node.kubernetes.io/unreachable","operator":"Exists"},{"effect":"NoSchedule","key":"node-role.kubernetes.io/etcd","operator":"Exists"}],"volumes":null},"status":{"conditions":null,"containerStatuses":null,"hostIP":null,"hostIPs":null,"phase":null,"podIP":null,"podIPs":null,"qosClass":null,"startTime":null}}
2024-06-11T10:53:08.008632651Z I0611 10:53:08.008559       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodUpdated' Updated Pod/openshift-kube-scheduler-guard-ci-op-9xx71rvq-1e28e-w667k-master-1 -n openshift-kube-scheduler because it changed
2024-06-11T10:53:08.020575234Z E0611 10:53:08.020527       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:53:08.021265925Z I0611 10:53:08.021216       1 prune_controller.go:269] Nothing to prune
2024-06-11T10:53:08.021476953Z I0611 10:53:08.021420       1 status_controller.go:218] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2024-06-11T10:50:20Z","message":"GuardControllerDegraded: Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-06-11T10:48:34Z","message":"NodeInstallerProgressing: 2 nodes are at revision 0; 1 node is at revision 5","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-06-11T10:52:16Z","message":"StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 node is at revision 5","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2024-06-11T10:48:31Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-06-11T10:48:20Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2024-06-11T10:53:08.033743779Z I0611 10:53:08.032051       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded message changed from "GuardControllerDegraded: [Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2, Unable to apply pod openshift-kube-scheduler-guard-ci-op-9xx71rvq-1e28e-w667k-master-1 changes: Operation cannot be fulfilled on pods \"openshift-kube-scheduler-guard-ci-op-9xx71rvq-1e28e-w667k-master-1\": the object has been modified; please apply your changes to the latest version and try again]" to "GuardControllerDegraded: Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2"
2024-06-11T10:53:08.401921184Z I0611 10:53:08.401839       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-1" is in transition to 5, but has not made progress because static pod is pending
2024-06-11T10:53:09.197777580Z I0611 10:53:09.197705       1 request.go:697] Waited for 1.176598366s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ci-op-9xx71rvq-1e28e-w667k-master-0
2024-06-11T10:53:11.801769905Z I0611 10:53:11.801708       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-1" is in transition to 5, but has not made progress because static pod is pending
2024-06-11T10:53:12.202008040Z E0611 10:53:12.201944       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:53:12.202187770Z E0611 10:53:12.202131       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:53:20.015808587Z I0611 10:53:20.015747       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'StartingNewRevision' new revision 6 triggered by "required secret/localhost-recovery-client-token has changed"
2024-06-11T10:53:20.041487800Z I0611 10:53:20.041421       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/kube-scheduler-pod-6 -n openshift-kube-scheduler because it was missing
2024-06-11T10:53:20.051724439Z I0611 10:53:20.051668       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/config-6 -n openshift-kube-scheduler because it was missing
2024-06-11T10:53:20.061394788Z I0611 10:53:20.061329       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/serviceaccount-ca-6 -n openshift-kube-scheduler because it was missing
2024-06-11T10:53:20.068719761Z I0611 10:53:20.068660       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/scheduler-kubeconfig-6 -n openshift-kube-scheduler because it was missing
2024-06-11T10:53:20.422064748Z I0611 10:53:20.421998       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/kube-scheduler-cert-syncer-kubeconfig-6 -n openshift-kube-scheduler because it was missing
2024-06-11T10:53:21.024692957Z I0611 10:53:21.024623       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/serving-cert-6 -n openshift-kube-scheduler because it was missing
2024-06-11T10:53:21.623463949Z I0611 10:53:21.623397       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/localhost-recovery-client-token-6 -n openshift-kube-scheduler because it was missing
2024-06-11T10:53:22.220136818Z I0611 10:53:22.220057       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionTriggered' new revision 6 triggered by "required secret/localhost-recovery-client-token has changed"
2024-06-11T10:53:22.236495792Z I0611 10:53:22.236436       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionCreate' Revision 6 created because required secret/localhost-recovery-client-token has changed
2024-06-11T10:53:22.236985066Z I0611 10:53:22.236919       1 prune_controller.go:269] Nothing to prune
2024-06-11T10:53:23.017928250Z I0611 10:53:23.017874       1 installer_controller.go:500] "ci-op-9xx71rvq-1e28e-w667k-master-1" moving to (v1.NodeStatus) {
2024-06-11T10:53:23.017928250Z  NodeName: (string) (len=35) "ci-op-9xx71rvq-1e28e-w667k-master-1",
2024-06-11T10:53:23.017928250Z  CurrentRevision: (int32) 0,
2024-06-11T10:53:23.017928250Z  TargetRevision: (int32) 6,
2024-06-11T10:53:23.017928250Z  LastFailedRevision: (int32) 0,
2024-06-11T10:53:23.017928250Z  LastFailedTime: (*v1.Time)(<nil>),
2024-06-11T10:53:23.017928250Z  LastFailedReason: (string) "",
2024-06-11T10:53:23.017928250Z  LastFailedCount: (int) 0,
2024-06-11T10:53:23.017928250Z  LastFallbackCount: (int) 0,
2024-06-11T10:53:23.017928250Z  LastFailedRevisionErrors: ([]string) <nil>
2024-06-11T10:53:23.017928250Z }
2024-06-11T10:53:23.017928250Z  because new revision pending
2024-06-11T10:53:23.055727965Z I0611 10:53:23.055664       1 prune_controller.go:269] Nothing to prune
2024-06-11T10:53:23.056206038Z I0611 10:53:23.056150       1 status_controller.go:218] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2024-06-11T10:50:20Z","message":"GuardControllerDegraded: Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-06-11T10:48:34Z","message":"NodeInstallerProgressing: 2 nodes are at revision 0; 1 node is at revision 5; 0 nodes have achieved new revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-06-11T10:52:16Z","message":"StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 node is at revision 5; 0 nodes have achieved new revision 6","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2024-06-11T10:48:31Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-06-11T10:48:20Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2024-06-11T10:53:23.070642320Z I0611 10:53:23.070545       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Progressing message changed from "NodeInstallerProgressing: 2 nodes are at revision 0; 1 node is at revision 5" to "NodeInstallerProgressing: 2 nodes are at revision 0; 1 node is at revision 5; 0 nodes have achieved new revision 6",Available message changed from "StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 node is at revision 5" to "StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 node is at revision 5; 0 nodes have achieved new revision 6"
2024-06-11T10:53:25.028248623Z I0611 10:53:25.028180       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/installer-6-ci-op-9xx71rvq-1e28e-w667k-master-1 -n openshift-kube-scheduler because it was missing
2024-06-11T10:53:26.017951773Z I0611 10:53:26.017887       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-1" is in transition to 6, but has not made progress because installer is not finished, but in Pending phase
2024-06-11T10:53:26.214247854Z I0611 10:53:26.214152       1 request.go:697] Waited for 1.184928869s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ci-op-9xx71rvq-1e28e-w667k-master-0
2024-06-11T10:53:27.214757738Z I0611 10:53:27.214690       1 request.go:697] Waited for 1.195740103s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-6-ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:53:28.414932612Z I0611 10:53:28.414871       1 request.go:697] Waited for 1.19716942s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-6-ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T10:53:28.421250667Z I0611 10:53:28.421196       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-1" is in transition to 6, but has not made progress because installer is not finished, but in Running phase
2024-06-11T10:53:29.618397684Z E0611 10:53:29.618279       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:53:29.618889858Z E0611 10:53:29.618834       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:53:30.417804500Z I0611 10:53:30.417747       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-1" is in transition to 6, but has not made progress because installer is not finished, but in Running phase
2024-06-11T10:53:31.217996795Z E0611 10:53:31.217928       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:53:32.018202892Z E0611 10:53:32.018145       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:53:36.968111590Z E0611 10:53:36.968041       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:53:36.968277616Z E0611 10:53:36.968232       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:53:42.133284175Z I0611 10:53:42.133212       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-1" is in transition to 6, but has not made progress because installer is not finished, but in Running phase
2024-06-11T10:53:42.139966869Z E0611 10:53:42.139895       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:53:42.140169793Z E0611 10:53:42.140143       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:53:57.643613210Z I0611 10:53:57.643540       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-1" is in transition to 6, but has not made progress because installer is not finished, but in Running phase
2024-06-11T10:53:57.649327392Z E0611 10:53:57.649240       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:53:57.649524712Z E0611 10:53:57.649482       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:53:59.237903711Z I0611 10:53:59.237827       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-1" is in transition to 6, but has not made progress because waiting for static pod of revision 6, found 5
2024-06-11T10:53:59.837149828Z E0611 10:53:59.837059       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:53:59.837487577Z E0611 10:53:59.837439       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:54:01.637103896Z I0611 10:54:01.637032       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-1" is in transition to 6, but has not made progress because waiting for static pod of revision 6, found 5
2024-06-11T10:54:02.041557483Z E0611 10:54:02.041497       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:54:02.042233579Z E0611 10:54:02.042177       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:54:07.754555077Z E0611 10:54:07.754491       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:54:07.769465339Z I0611 10:54:07.769396       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-1" is in transition to 6, but has not made progress because waiting for static pod of revision 6, found 5
2024-06-11T10:54:07.957474829Z E0611 10:54:07.957410       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:54:10.157816476Z I0611 10:54:10.157730       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-1" is in transition to 6, but has not made progress because static pod is pending
2024-06-11T10:54:10.557279285Z E0611 10:54:10.557206       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:54:10.557525716Z E0611 10:54:10.557483       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:54:11.957474526Z E0611 10:54:11.957397       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:54:12.357991066Z I0611 10:54:12.357925       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-1" is in transition to 6, but has not made progress because static pod is pending
2024-06-11T10:54:13.157854401Z E0611 10:54:13.157797       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:54:14.157667818Z I0611 10:54:14.157600       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-1" is in transition to 6, but has not made progress because static pod is pending
2024-06-11T10:54:19.558250644Z E0611 10:54:19.558183       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:54:19.559485565Z W0611 10:54:19.559421       1 base_controller.go:232] Updating status of "GuardController" failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:54:19.559523869Z E0611 10:54:19.559487       1 base_controller.go:268] GuardController reconciliation failed: [Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ci-op-9xx71rvq-1e28e-w667k-master-1": dial tcp 172.30.0.1:443: connect: connection refused, Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2]
2024-06-11T10:54:33.203829440Z E0611 10:54:33.203745       1 leaderelection.go:332] error retrieving resource lock openshift-kube-scheduler-operator/openshift-cluster-kube-scheduler-operator-lock: Get "https://172.30.0.1:443/apis/coordination.k8s.io/v1/namespaces/openshift-kube-scheduler-operator/leases/openshift-cluster-kube-scheduler-operator-lock?timeout=1m47s": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:54:47.197118289Z E0611 10:54:47.197058       1 base_controller.go:268] InstallerStateController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:54:47.199758411Z E0611 10:54:47.199709       1 base_controller.go:268] StaticPodStateController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:54:47.202419035Z E0611 10:54:47.202385       1 base_controller.go:268] BackingResourceController reconciliation failed: ["manifests/installer-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa": dial tcp 172.30.0.1:443: connect: connection refused, "manifests/installer-cluster-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-scheduler-installer": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T10:54:47.202959501Z E0611 10:54:47.202927       1 base_controller.go:268] InstallerStateController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:54:47.208073124Z E0611 10:54:47.208033       1 base_controller.go:268] StaticPodStateController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:54:47.210455214Z E0611 10:54:47.210422       1 base_controller.go:268] BackingResourceController reconciliation failed: ["manifests/installer-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa": dial tcp 172.30.0.1:443: connect: connection refused, "manifests/installer-cluster-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-scheduler-installer": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T10:54:47.397106862Z E0611 10:54:47.397046       1 base_controller.go:268] InstallerStateController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:54:47.799999364Z E0611 10:54:47.799934       1 base_controller.go:268] BackingResourceController reconciliation failed: ["manifests/installer-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa": dial tcp 172.30.0.1:443: connect: connection refused, "manifests/installer-cluster-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-scheduler-installer": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T10:54:47.997528937Z E0611 10:54:47.997468       1 base_controller.go:268] InstallerStateController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:54:48.599227868Z E0611 10:54:48.599163       1 base_controller.go:268] BackingResourceController reconciliation failed: ["manifests/installer-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa": dial tcp 172.30.0.1:443: connect: connection refused, "manifests/installer-cluster-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-scheduler-installer": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T10:54:48.797838873Z E0611 10:54:48.797779       1 base_controller.go:268] InstallerStateController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:54:49.198926255Z E0611 10:54:49.198873       1 base_controller.go:268] StaticPodStateController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:54:49.599718701Z E0611 10:54:49.599662       1 base_controller.go:268] BackingResourceController reconciliation failed: ["manifests/installer-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa": dial tcp 172.30.0.1:443: connect: connection refused, "manifests/installer-cluster-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-scheduler-installer": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T10:54:49.797736234Z E0611 10:54:49.797683       1 base_controller.go:268] InstallerStateController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:54:50.600335361Z E0611 10:54:50.600241       1 base_controller.go:268] BackingResourceController reconciliation failed: ["manifests/installer-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa": dial tcp 172.30.0.1:443: connect: connection refused, "manifests/installer-cluster-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-scheduler-installer": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T10:54:50.798046232Z E0611 10:54:50.797978       1 base_controller.go:268] InstallerStateController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:54:51.399016938Z E0611 10:54:51.398950       1 base_controller.go:268] KubeControllerManagerStaticResources reconciliation failed: ["assets/kube-scheduler/ns.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/leader-election-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:leader-locking-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/scheduler-clusterrolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler:public-2": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-role.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/roles/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/rolebindings/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/svc.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/openshift-kube-scheduler-sa": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-client-crb.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler-recovery": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/localhost-recovery-client": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T10:54:51.599340607Z E0611 10:54:51.599270       1 base_controller.go:268] BackingResourceController reconciliation failed: ["manifests/installer-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa": dial tcp 172.30.0.1:443: connect: connection refused, "manifests/installer-cluster-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-scheduler-installer": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T10:54:51.998266749Z E0611 10:54:51.998190       1 base_controller.go:268] InstallerStateController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:54:52.198619021Z E0611 10:54:52.198557       1 base_controller.go:268] StaticPodStateController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:54:52.598975825Z E0611 10:54:52.598897       1 base_controller.go:268] TargetConfigController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:54:52.799991572Z E0611 10:54:52.799931       1 base_controller.go:268] BackingResourceController reconciliation failed: ["manifests/installer-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa": dial tcp 172.30.0.1:443: connect: connection refused, "manifests/installer-cluster-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-scheduler-installer": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T10:54:53.598091984Z E0611 10:54:53.598019       1 base_controller.go:268] InstallerStateController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:54:54.399587681Z E0611 10:54:54.399527       1 base_controller.go:268] BackingResourceController reconciliation failed: ["manifests/installer-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa": dial tcp 172.30.0.1:443: connect: connection refused, "manifests/installer-cluster-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-scheduler-installer": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T10:54:54.599446197Z E0611 10:54:54.599372       1 base_controller.go:268] StaticPodStateController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:54:54.799587245Z E0611 10:54:54.799518       1 base_controller.go:268] KubeControllerManagerStaticResources reconciliation failed: ["assets/kube-scheduler/ns.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/leader-election-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:leader-locking-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/scheduler-clusterrolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler:public-2": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-role.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/roles/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/rolebindings/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/svc.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/openshift-kube-scheduler-sa": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-client-crb.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler-recovery": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/localhost-recovery-client": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T10:54:55.597674755Z E0611 10:54:55.597618       1 base_controller.go:268] InstallerStateController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:54:56.400850080Z E0611 10:54:56.400785       1 base_controller.go:268] BackingResourceController reconciliation failed: ["manifests/installer-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa": dial tcp 172.30.0.1:443: connect: connection refused, "manifests/installer-cluster-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-scheduler-installer": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T10:54:56.598524379Z E0611 10:54:56.598445       1 base_controller.go:268] TargetConfigController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:54:56.799772716Z E0611 10:54:56.799709       1 base_controller.go:268] StaticPodStateController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:54:57.598932049Z E0611 10:54:57.598861       1 base_controller.go:268] KubeControllerManagerStaticResources reconciliation failed: ["assets/kube-scheduler/ns.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/leader-election-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:leader-locking-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/scheduler-clusterrolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler:public-2": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-role.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/roles/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/rolebindings/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/svc.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/openshift-kube-scheduler-sa": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-client-crb.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler-recovery": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/localhost-recovery-client": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T10:54:58.598704393Z E0611 10:54:58.598644       1 base_controller.go:268] StaticPodStateController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:54:58.797749939Z E0611 10:54:58.797699       1 base_controller.go:268] InstallerStateController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:54:59.206004487Z E0611 10:54:59.205935       1 leaderelection.go:332] error retrieving resource lock openshift-kube-scheduler-operator/openshift-cluster-kube-scheduler-operator-lock: Get "https://172.30.0.1:443/apis/coordination.k8s.io/v1/namespaces/openshift-kube-scheduler-operator/leases/openshift-cluster-kube-scheduler-operator-lock?timeout=1m47s": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:54:59.599562751Z E0611 10:54:59.599490       1 base_controller.go:268] BackingResourceController reconciliation failed: ["manifests/installer-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa": dial tcp 172.30.0.1:443: connect: connection refused, "manifests/installer-cluster-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-scheduler-installer": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T10:54:59.998711942Z E0611 10:54:59.998656       1 base_controller.go:268] TargetConfigController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:55:00.400351467Z E0611 10:55:00.400256       1 base_controller.go:268] KubeControllerManagerStaticResources reconciliation failed: ["assets/kube-scheduler/ns.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/leader-election-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:leader-locking-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/scheduler-clusterrolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler:public-2": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-role.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/roles/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/rolebindings/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/svc.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/openshift-kube-scheduler-sa": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-client-crb.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler-recovery": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/localhost-recovery-client": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T10:55:00.799657773Z E0611 10:55:00.799582       1 base_controller.go:268] StaticPodStateController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:55:02.400177601Z E0611 10:55:02.399652       1 base_controller.go:268] KubeControllerManagerStaticResources reconciliation failed: ["assets/kube-scheduler/ns.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/leader-election-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:leader-locking-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/scheduler-clusterrolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler:public-2": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-role.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/roles/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/rolebindings/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/svc.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/openshift-kube-scheduler-sa": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-client-crb.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler-recovery": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/localhost-recovery-client": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T10:55:02.799814704Z E0611 10:55:02.799749       1 base_controller.go:268] TargetConfigController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:55:03.200340614Z E0611 10:55:03.200263       1 base_controller.go:268] StaticPodStateController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:55:04.398373817Z E0611 10:55:04.398319       1 base_controller.go:268] InstallerStateController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:55:04.599608339Z E0611 10:55:04.599548       1 base_controller.go:268] KubeControllerManagerStaticResources reconciliation failed: ["assets/kube-scheduler/ns.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/leader-election-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:leader-locking-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/scheduler-clusterrolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler:public-2": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-role.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/roles/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/rolebindings/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/svc.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/openshift-kube-scheduler-sa": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-client-crb.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler-recovery": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/localhost-recovery-client": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T10:55:05.200722294Z E0611 10:55:05.200654       1 base_controller.go:268] BackingResourceController reconciliation failed: ["manifests/installer-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa": dial tcp 172.30.0.1:443: connect: connection refused, "manifests/installer-cluster-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-scheduler-installer": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T10:55:05.598874318Z E0611 10:55:05.598815       1 base_controller.go:268] TargetConfigController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:55:06.398684261Z E0611 10:55:06.398624       1 base_controller.go:268] StaticPodStateController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:55:06.999877895Z E0611 10:55:06.999801       1 base_controller.go:268] KubeControllerManagerStaticResources reconciliation failed: ["assets/kube-scheduler/ns.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/leader-election-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:leader-locking-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/scheduler-clusterrolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler:public-2": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-role.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/roles/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/rolebindings/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/svc.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/openshift-kube-scheduler-sa": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-client-crb.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler-recovery": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/localhost-recovery-client": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T10:55:07.799588499Z E0611 10:55:07.799535       1 base_controller.go:268] TargetConfigController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:55:08.799311837Z E0611 10:55:08.799236       1 base_controller.go:268] KubeControllerManagerStaticResources reconciliation failed: ["assets/kube-scheduler/ns.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/leader-election-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:leader-locking-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/scheduler-clusterrolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler:public-2": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-role.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/roles/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/rolebindings/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/svc.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/openshift-kube-scheduler-sa": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-client-crb.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler-recovery": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/localhost-recovery-client": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T10:55:09.799177092Z E0611 10:55:09.799117       1 base_controller.go:268] TargetConfigController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:55:10.198924031Z E0611 10:55:10.198849       1 base_controller.go:268] StaticPodStateController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:55:11.199841203Z E0611 10:55:11.199760       1 base_controller.go:268] KubeControllerManagerStaticResources reconciliation failed: ["assets/kube-scheduler/ns.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/leader-election-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:leader-locking-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/scheduler-clusterrolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler:public-2": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-role.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/roles/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/rolebindings/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/svc.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/openshift-kube-scheduler-sa": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-client-crb.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler-recovery": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/localhost-recovery-client": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T10:55:11.799118124Z E0611 10:55:11.799052       1 base_controller.go:268] TargetConfigController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:55:12.999379257Z E0611 10:55:12.999322       1 base_controller.go:268] TargetConfigController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:55:13.599877112Z E0611 10:55:13.599812       1 base_controller.go:268] KubeControllerManagerStaticResources reconciliation failed: ["assets/kube-scheduler/ns.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/leader-election-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:leader-locking-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/scheduler-clusterrolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler:public-2": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-role.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/roles/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/rolebindings/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/svc.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/openshift-kube-scheduler-sa": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-client-crb.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler-recovery": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/localhost-recovery-client": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T10:55:14.598961024Z E0611 10:55:14.598908       1 base_controller.go:268] TargetConfigController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:55:14.797210082Z E0611 10:55:14.797159       1 base_controller.go:268] InstallerStateController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:55:15.399965759Z E0611 10:55:15.399917       1 base_controller.go:268] StaticPodStateController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:55:15.600149363Z E0611 10:55:15.600005       1 base_controller.go:268] BackingResourceController reconciliation failed: ["manifests/installer-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa": dial tcp 172.30.0.1:443: connect: connection refused, "manifests/installer-cluster-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-scheduler-installer": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T10:55:16.400144822Z E0611 10:55:16.400072       1 base_controller.go:268] KubeControllerManagerStaticResources reconciliation failed: ["assets/kube-scheduler/ns.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/leader-election-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:leader-locking-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/scheduler-clusterrolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler:public-2": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-role.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/roles/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/rolebindings/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/svc.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/openshift-kube-scheduler-sa": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-client-crb.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler-recovery": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/localhost-recovery-client": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T10:55:17.399336798Z E0611 10:55:17.399255       1 base_controller.go:268] TargetConfigController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:55:21.528834084Z E0611 10:55:21.528760       1 base_controller.go:268] KubeControllerManagerStaticResources reconciliation failed: ["assets/kube-scheduler/ns.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/leader-election-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:leader-locking-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/scheduler-clusterrolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler:public-2": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-role.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/roles/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/rolebindings/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/svc.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/openshift-kube-scheduler-sa": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-client-crb.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler-recovery": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/localhost-recovery-client": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T10:55:22.530632204Z E0611 10:55:22.530544       1 base_controller.go:268] TargetConfigController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:55:25.205641974Z E0611 10:55:25.205570       1 leaderelection.go:332] error retrieving resource lock openshift-kube-scheduler-operator/openshift-cluster-kube-scheduler-operator-lock: Get "https://172.30.0.1:443/apis/coordination.k8s.io/v1/namespaces/openshift-kube-scheduler-operator/leases/openshift-cluster-kube-scheduler-operator-lock?timeout=1m47s": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:55:25.643629167Z E0611 10:55:25.643578       1 base_controller.go:268] StaticPodStateController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:55:31.780375672Z E0611 10:55:31.780288       1 base_controller.go:268] KubeControllerManagerStaticResources reconciliation failed: ["assets/kube-scheduler/ns.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/leader-election-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:leader-locking-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/scheduler-clusterrolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler:public-2": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-role.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/roles/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/rolebindings/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/svc.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/openshift-kube-scheduler-sa": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-client-crb.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler-recovery": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/localhost-recovery-client": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T10:55:32.778868114Z E0611 10:55:32.778819       1 base_controller.go:268] TargetConfigController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:55:35.279134024Z E0611 10:55:35.279061       1 base_controller.go:268] InstallerStateController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:55:36.083906854Z E0611 10:55:36.083852       1 base_controller.go:268] BackingResourceController reconciliation failed: ["manifests/installer-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa": dial tcp 172.30.0.1:443: connect: connection refused, "manifests/installer-cluster-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-scheduler-installer": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T10:55:46.127931543Z E0611 10:55:46.127806       1 base_controller.go:268] StaticPodStateController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:55:47.197889606Z E0611 10:55:47.197822       1 base_controller.go:268] InstallerStateController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:55:47.200712999Z E0611 10:55:47.200654       1 base_controller.go:268] StaticPodStateController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:55:47.202598194Z E0611 10:55:47.202556       1 base_controller.go:268] BackingResourceController reconciliation failed: ["manifests/installer-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa": dial tcp 172.30.0.1:443: connect: connection refused, "manifests/installer-cluster-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-scheduler-installer": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T10:55:47.804553707Z E0611 10:55:47.804500       1 base_controller.go:268] KubeControllerManagerStaticResources reconciliation failed: ["assets/kube-scheduler/ns.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/leader-election-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:leader-locking-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/scheduler-clusterrolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler:public-2": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-role.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/roles/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/rolebindings/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/svc.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/openshift-kube-scheduler-sa": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-client-crb.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler-recovery": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/localhost-recovery-client": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T10:55:48.101595456Z E0611 10:55:48.101536       1 base_controller.go:268] TargetConfigController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:55:51.206122727Z E0611 10:55:51.206038       1 leaderelection.go:332] error retrieving resource lock openshift-kube-scheduler-operator/openshift-cluster-kube-scheduler-operator-lock: Get "https://172.30.0.1:443/apis/coordination.k8s.io/v1/namespaces/openshift-kube-scheduler-operator/leases/openshift-cluster-kube-scheduler-operator-lock?timeout=1m47s": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:55:52.268798232Z E0611 10:55:52.268741       1 base_controller.go:268] KubeControllerManagerStaticResources reconciliation failed: ["assets/kube-scheduler/ns.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/leader-election-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:leader-locking-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/scheduler-clusterrolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler:public-2": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-role.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/roles/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/rolebindings/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/svc.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/openshift-kube-scheduler-sa": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-client-crb.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler-recovery": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/localhost-recovery-client": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T10:55:53.265752134Z E0611 10:55:53.265696       1 base_controller.go:268] TargetConfigController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:56:44.295806410Z I0611 10:56:44.295723       1 reflector.go:351] Caches populated for *v1.ClusterVersion from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T10:56:44.750328257Z I0611 10:56:44.750254       1 reflector.go:351] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T10:56:47.533471323Z I0611 10:56:47.533398       1 reflector.go:351] Caches populated for *v1.APIServer from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T10:56:54.586729025Z I0611 10:56:54.586666       1 reflector.go:351] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T10:56:55.984104960Z I0611 10:56:55.984029       1 reflector.go:351] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T10:56:56.227978274Z I0611 10:56:56.227890       1 reflector.go:351] Caches populated for *v1.RoleBinding from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T10:56:56.399771063Z I0611 10:56:56.399686       1 reflector.go:351] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T10:56:57.184085247Z I0611 10:56:57.184033       1 reflector.go:351] Caches populated for *v1.Secret from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T10:57:02.016327693Z I0611 10:57:02.016236       1 reflector.go:351] Caches populated for *v1.Service from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T10:57:02.162859112Z I0611 10:57:02.162795       1 reflector.go:351] Caches populated for *v1.Secret from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T10:57:05.850835542Z I0611 10:57:05.850779       1 reflector.go:351] Caches populated for *v1.Pod from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T10:57:06.220055614Z I0611 10:57:06.219985       1 installer_controller.go:500] "ci-op-9xx71rvq-1e28e-w667k-master-1" moving to (v1.NodeStatus) {
2024-06-11T10:57:06.220055614Z  NodeName: (string) (len=35) "ci-op-9xx71rvq-1e28e-w667k-master-1",
2024-06-11T10:57:06.220055614Z  CurrentRevision: (int32) 6,
2024-06-11T10:57:06.220055614Z  TargetRevision: (int32) 0,
2024-06-11T10:57:06.220055614Z  LastFailedRevision: (int32) 0,
2024-06-11T10:57:06.220055614Z  LastFailedTime: (*v1.Time)(<nil>),
2024-06-11T10:57:06.220055614Z  LastFailedReason: (string) "",
2024-06-11T10:57:06.220055614Z  LastFailedCount: (int) 0,
2024-06-11T10:57:06.220055614Z  LastFallbackCount: (int) 0,
2024-06-11T10:57:06.220055614Z  LastFailedRevisionErrors: ([]string) <nil>
2024-06-11T10:57:06.220055614Z }
2024-06-11T10:57:06.220055614Z  because static pod is ready
2024-06-11T10:57:06.240170732Z I0611 10:57:06.240091       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeCurrentRevisionChanged' Updated node "ci-op-9xx71rvq-1e28e-w667k-master-1" from revision 0 to 6 because static pod is ready
2024-06-11T10:57:07.222566499Z E0611 10:57:07.222506       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:57:07.222793619Z E0611 10:57:07.222761       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:57:08.223560102Z I0611 10:57:08.223486       1 installer_controller.go:500] "ci-op-9xx71rvq-1e28e-w667k-master-1" moving to (v1.NodeStatus) {
2024-06-11T10:57:08.223560102Z  NodeName: (string) (len=35) "ci-op-9xx71rvq-1e28e-w667k-master-1",
2024-06-11T10:57:08.223560102Z  CurrentRevision: (int32) 6,
2024-06-11T10:57:08.223560102Z  TargetRevision: (int32) 0,
2024-06-11T10:57:08.223560102Z  LastFailedRevision: (int32) 0,
2024-06-11T10:57:08.223560102Z  LastFailedTime: (*v1.Time)(<nil>),
2024-06-11T10:57:08.223560102Z  LastFailedReason: (string) "",
2024-06-11T10:57:08.223560102Z  LastFailedCount: (int) 0,
2024-06-11T10:57:08.223560102Z  LastFallbackCount: (int) 0,
2024-06-11T10:57:08.223560102Z  LastFailedRevisionErrors: ([]string) <nil>
2024-06-11T10:57:08.223560102Z }
2024-06-11T10:57:08.223560102Z  because static pod is ready
2024-06-11T10:57:08.355184916Z I0611 10:57:08.355135       1 helpers.go:260] lister was stale at resourceVersion=16942, live get showed resourceVersion=19421
2024-06-11T10:57:08.419710411Z E0611 10:57:08.419657       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:57:08.631592685Z I0611 10:57:08.631511       1 reflector.go:351] Caches populated for *v1.FeatureGate from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T10:57:09.419345396Z E0611 10:57:09.419275       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:57:14.185686015Z I0611 10:57:14.185624       1 reflector.go:351] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T10:57:14.409893103Z I0611 10:57:14.409831       1 reflector.go:351] Caches populated for *v1.RoleBinding from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T10:57:16.010334167Z I0611 10:57:16.010258       1 reflector.go:351] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T10:57:16.385805715Z I0611 10:57:16.385748       1 reflector.go:351] Caches populated for *v1.Secret from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T10:57:16.524136296Z I0611 10:57:16.524061       1 reflector.go:351] Caches populated for *v1.ClusterRoleBinding from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T10:57:18.787551616Z I0611 10:57:18.787494       1 reflector.go:351] Caches populated for *v1.Secret from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T10:57:18.985809919Z I0611 10:57:18.985747       1 reflector.go:351] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T10:57:21.184913157Z I0611 10:57:21.184831       1 reflector.go:351] Caches populated for *v1.ServiceAccount from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T10:57:21.985294214Z I0611 10:57:21.985233       1 reflector.go:351] Caches populated for *v1.Secret from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T10:57:22.066467899Z I0611 10:57:22.066409       1 reflector.go:351] Caches populated for *v1.Scheduler from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T10:57:23.266492781Z I0611 10:57:23.266415       1 reflector.go:351] Caches populated for *v1.ClusterOperator from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T10:57:23.388023587Z I0611 10:57:23.387968       1 reflector.go:351] Caches populated for *v1.Namespace from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T10:57:24.079833876Z I0611 10:57:24.079755       1 reflector.go:351] Caches populated for *v1.Infrastructure from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T10:57:25.130508516Z I0611 10:57:25.130441       1 reflector.go:351] Caches populated for operator.openshift.io/v1, Resource=kubeschedulers from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T10:57:25.131255877Z I0611 10:57:25.131190       1 prune_controller.go:269] Nothing to prune
2024-06-11T10:57:25.131538200Z I0611 10:57:25.131496       1 status_controller.go:218] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2024-06-11T10:50:20Z","message":"GuardControllerDegraded: Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-06-11T10:48:34Z","message":"NodeInstallerProgressing: 1 node is at revision 0; 1 node is at revision 5; 1 node is at revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-06-11T10:52:16Z","message":"StaticPodsAvailable: 2 nodes are active; 1 node is at revision 0; 1 node is at revision 5; 1 node is at revision 6","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2024-06-11T10:48:31Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-06-11T10:48:20Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2024-06-11T10:57:25.145509139Z I0611 10:57:25.145435       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Progressing message changed from "NodeInstallerProgressing: 2 nodes are at revision 0; 1 node is at revision 5; 0 nodes have achieved new revision 6" to "NodeInstallerProgressing: 1 node is at revision 0; 1 node is at revision 5; 1 node is at revision 6",Available message changed from "StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 node is at revision 5; 0 nodes have achieved new revision 6" to "StaticPodsAvailable: 2 nodes are active; 1 node is at revision 0; 1 node is at revision 5; 1 node is at revision 6"
2024-06-11T10:57:25.586071349Z I0611 10:57:25.586002       1 reflector.go:351] Caches populated for *v1.Node from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T10:57:25.586486783Z E0611 10:57:25.586442       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:57:26.183660658Z I0611 10:57:26.183588       1 request.go:697] Waited for 1.052320774s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ci-op-9xx71rvq-1e28e-w667k-master-0
2024-06-11T10:57:28.187593998Z I0611 10:57:28.187482       1 installer_controller.go:524] node ci-op-9xx71rvq-1e28e-w667k-master-2 static pod not found and needs new revision 6
2024-06-11T10:57:28.187708707Z I0611 10:57:28.187594       1 installer_controller.go:532] "ci-op-9xx71rvq-1e28e-w667k-master-2" moving to (v1.NodeStatus) {
2024-06-11T10:57:28.187708707Z  NodeName: (string) (len=35) "ci-op-9xx71rvq-1e28e-w667k-master-2",
2024-06-11T10:57:28.187708707Z  CurrentRevision: (int32) 0,
2024-06-11T10:57:28.187708707Z  TargetRevision: (int32) 6,
2024-06-11T10:57:28.187708707Z  LastFailedRevision: (int32) 0,
2024-06-11T10:57:28.187708707Z  LastFailedTime: (*v1.Time)(<nil>),
2024-06-11T10:57:28.187708707Z  LastFailedReason: (string) "",
2024-06-11T10:57:28.187708707Z  LastFailedCount: (int) 0,
2024-06-11T10:57:28.187708707Z  LastFallbackCount: (int) 0,
2024-06-11T10:57:28.187708707Z  LastFailedRevisionErrors: ([]string) <nil>
2024-06-11T10:57:28.187708707Z }
2024-06-11T10:57:28.191592724Z I0611 10:57:28.191535       1 reflector.go:351] Caches populated for *v1.Role from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T10:57:28.205258838Z I0611 10:57:28.205195       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeTargetRevisionChanged' Updating node "ci-op-9xx71rvq-1e28e-w667k-master-2" from revision 0 to 6 because node ci-op-9xx71rvq-1e28e-w667k-master-2 static pod not found
2024-06-11T10:57:28.205749678Z I0611 10:57:28.205718       1 prune_controller.go:269] Nothing to prune
2024-06-11T10:57:29.383355783Z I0611 10:57:29.383281       1 request.go:697] Waited for 1.175978472s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa
2024-06-11T10:57:29.987865326Z E0611 10:57:29.987806       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:57:30.202902925Z I0611 10:57:30.202791       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/installer-6-ci-op-9xx71rvq-1e28e-w667k-master-2 -n openshift-kube-scheduler because it was missing
2024-06-11T10:57:31.187653703Z I0611 10:57:31.187573       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-2" is in transition to 6, but has not made progress because installer is not finished, but in Pending phase
2024-06-11T10:57:31.383605672Z I0611 10:57:31.383495       1 request.go:697] Waited for 1.180488629s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller
2024-06-11T10:57:32.582999389Z I0611 10:57:32.582933       1 request.go:697] Waited for 1.193663632s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller
2024-06-11T10:57:33.589989437Z I0611 10:57:33.583750       1 request.go:697] Waited for 1.195803614s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-6-ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:57:33.592607759Z I0611 10:57:33.592549       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-2" is in transition to 6, but has not made progress because installer is not finished, but in Pending phase
2024-06-11T10:57:34.387527175Z E0611 10:57:34.387445       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:57:34.387752494Z E0611 10:57:34.387710       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:57:34.388029317Z E0611 10:57:34.387991       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:57:36.588358770Z E0611 10:57:36.588163       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:57:37.587119723Z E0611 10:57:37.587050       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:57:37.587283736Z E0611 10:57:37.587263       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:57:39.203353145Z I0611 10:57:39.203284       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-2" is in transition to 6, but has not made progress because installer is not finished, but in Running phase
2024-06-11T10:57:39.587608675Z E0611 10:57:39.587553       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:57:39.587781386Z E0611 10:57:39.587747       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:57:42.224618260Z I0611 10:57:42.224562       1 reflector.go:351] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T10:57:46.707940035Z E0611 10:57:46.707881       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:57:46.708077246Z E0611 10:57:46.708057       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:57:48.695168853Z E0611 10:57:48.695103       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:57:49.490348003Z E0611 10:57:49.490266       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:57:50.806637994Z I0611 10:57:50.806571       1 reflector.go:351] Caches populated for *v1.PodDisruptionBudget from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T10:57:53.594376692Z E0611 10:57:53.594322       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:57:53.594525804Z E0611 10:57:53.594506       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:57:55.504724889Z E0611 10:57:55.504651       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:57:55.514007153Z E0611 10:57:55.513942       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:57:58.096498152Z E0611 10:57:58.096441       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:57:58.105086347Z E0611 10:57:58.105044       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:58:06.703383332Z E0611 10:58:06.703287       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:58:06.745580805Z E0611 10:58:06.745517       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:58:06.765865827Z E0611 10:58:06.765809       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:58:06.766017239Z E0611 10:58:06.765979       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:58:07.207226208Z E0611 10:58:07.207174       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:58:07.207464328Z E0611 10:58:07.207411       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:58:07.845941466Z E0611 10:58:07.845881       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:58:07.846138482Z E0611 10:58:07.846110       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:58:10.460162939Z E0611 10:58:10.460096       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:58:10.460378755Z E0611 10:58:10.460349       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T10:58:27.275374541Z E0611 10:58:27.275287       1 leaderelection.go:332] error retrieving resource lock openshift-kube-scheduler-operator/openshift-cluster-kube-scheduler-operator-lock: Get "https://172.30.0.1:443/apis/coordination.k8s.io/v1/namespaces/openshift-kube-scheduler-operator/leases/openshift-cluster-kube-scheduler-operator-lock?timeout=1m47s": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:58:47.203843569Z E0611 10:58:47.203769       1 base_controller.go:268] InstallerStateController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:58:47.206498054Z E0611 10:58:47.206433       1 base_controller.go:268] BackingResourceController reconciliation failed: ["manifests/installer-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa": dial tcp 172.30.0.1:443: connect: connection refused, "manifests/installer-cluster-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-scheduler-installer": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T10:58:47.206686367Z E0611 10:58:47.206654       1 base_controller.go:268] StaticPodStateController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:58:47.209732879Z E0611 10:58:47.209696       1 base_controller.go:268] InstallerStateController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:58:47.214424006Z E0611 10:58:47.214383       1 base_controller.go:268] BackingResourceController reconciliation failed: ["manifests/installer-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa": dial tcp 172.30.0.1:443: connect: connection refused, "manifests/installer-cluster-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-scheduler-installer": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T10:58:47.215798101Z E0611 10:58:47.215764       1 base_controller.go:268] StaticPodStateController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:58:47.401489422Z E0611 10:58:47.401435       1 base_controller.go:268] InstallerStateController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:58:47.605092189Z E0611 10:58:47.605022       1 base_controller.go:268] BackingResourceController reconciliation failed: ["manifests/installer-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa": dial tcp 172.30.0.1:443: connect: connection refused, "manifests/installer-cluster-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-scheduler-installer": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T10:58:48.002672253Z E0611 10:58:48.002600       1 base_controller.go:268] InstallerStateController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:58:48.205044035Z E0611 10:58:48.204966       1 base_controller.go:268] BackingResourceController reconciliation failed: ["manifests/installer-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa": dial tcp 172.30.0.1:443: connect: connection refused, "manifests/installer-cluster-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-scheduler-installer": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T10:58:48.802389499Z E0611 10:58:48.802310       1 base_controller.go:268] InstallerStateController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:58:49.203793093Z E0611 10:58:49.203737       1 base_controller.go:268] BackingResourceController reconciliation failed: ["manifests/installer-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa": dial tcp 172.30.0.1:443: connect: connection refused, "manifests/installer-cluster-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-scheduler-installer": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T10:58:49.602951208Z E0611 10:58:49.602902       1 base_controller.go:268] StaticPodStateController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:58:49.801441986Z E0611 10:58:49.801376       1 base_controller.go:268] InstallerStateController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:58:50.203781833Z E0611 10:58:50.203715       1 base_controller.go:268] BackingResourceController reconciliation failed: ["manifests/installer-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa": dial tcp 172.30.0.1:443: connect: connection refused, "manifests/installer-cluster-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-scheduler-installer": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T10:58:50.802178095Z E0611 10:58:50.802122       1 base_controller.go:268] InstallerStateController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:58:51.204890351Z E0611 10:58:51.204835       1 base_controller.go:268] BackingResourceController reconciliation failed: ["manifests/installer-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa": dial tcp 172.30.0.1:443: connect: connection refused, "manifests/installer-cluster-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-scheduler-installer": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T10:58:51.404281036Z E0611 10:58:51.404201       1 base_controller.go:268] KubeControllerManagerStaticResources reconciliation failed: ["assets/kube-scheduler/ns.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/leader-election-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:leader-locking-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/scheduler-clusterrolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler:public-2": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-role.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/roles/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/rolebindings/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/svc.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/openshift-kube-scheduler-sa": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-client-crb.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler-recovery": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/localhost-recovery-client": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T10:58:52.001572326Z E0611 10:58:52.001514       1 base_controller.go:268] InstallerStateController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:58:52.403810247Z E0611 10:58:52.403751       1 base_controller.go:268] BackingResourceController reconciliation failed: ["manifests/installer-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa": dial tcp 172.30.0.1:443: connect: connection refused, "manifests/installer-cluster-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-scheduler-installer": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T10:58:52.603385545Z E0611 10:58:52.603331       1 base_controller.go:268] StaticPodStateController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:58:52.806114772Z E0611 10:58:52.806054       1 base_controller.go:268] TargetConfigController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:58:53.202352157Z E0611 10:58:53.202269       1 base_controller.go:268] InstallerStateController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:58:53.277278900Z E0611 10:58:53.277209       1 leaderelection.go:332] error retrieving resource lock openshift-kube-scheduler-operator/openshift-cluster-kube-scheduler-operator-lock: Get "https://172.30.0.1:443/apis/coordination.k8s.io/v1/namespaces/openshift-kube-scheduler-operator/leases/openshift-cluster-kube-scheduler-operator-lock?timeout=1m47s": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:58:54.004018494Z E0611 10:58:54.003956       1 base_controller.go:268] BackingResourceController reconciliation failed: ["manifests/installer-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa": dial tcp 172.30.0.1:443: connect: connection refused, "manifests/installer-cluster-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-scheduler-installer": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T10:58:54.602890000Z E0611 10:58:54.602825       1 base_controller.go:268] KubeControllerManagerStaticResources reconciliation failed: ["assets/kube-scheduler/ns.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/leader-election-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:leader-locking-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/scheduler-clusterrolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler:public-2": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-role.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/roles/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/rolebindings/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/svc.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/openshift-kube-scheduler-sa": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-client-crb.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler-recovery": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/localhost-recovery-client": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T10:58:54.803100344Z E0611 10:58:54.803044       1 base_controller.go:268] StaticPodStateController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:58:55.202190636Z E0611 10:58:55.202128       1 base_controller.go:268] InstallerStateController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:58:56.003773868Z E0611 10:58:56.003714       1 base_controller.go:268] BackingResourceController reconciliation failed: ["manifests/installer-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa": dial tcp 172.30.0.1:443: connect: connection refused, "manifests/installer-cluster-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-scheduler-installer": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T10:58:56.603882963Z E0611 10:58:56.603800       1 base_controller.go:268] TargetConfigController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:58:57.003740811Z E0611 10:58:57.003660       1 base_controller.go:268] StaticPodStateController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:58:57.403278435Z E0611 10:58:57.403211       1 base_controller.go:268] KubeControllerManagerStaticResources reconciliation failed: ["assets/kube-scheduler/ns.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/leader-election-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:leader-locking-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/scheduler-clusterrolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler:public-2": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-role.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/roles/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/rolebindings/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/svc.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/openshift-kube-scheduler-sa": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-client-crb.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler-recovery": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/localhost-recovery-client": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T10:58:58.402056592Z E0611 10:58:58.401915       1 base_controller.go:268] InstallerStateController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:58:59.004030706Z E0611 10:58:59.003964       1 base_controller.go:268] StaticPodStateController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:58:59.203320152Z E0611 10:58:59.203244       1 base_controller.go:268] BackingResourceController reconciliation failed: ["manifests/installer-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa": dial tcp 172.30.0.1:443: connect: connection refused, "manifests/installer-cluster-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-scheduler-installer": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T10:59:00.004053051Z E0611 10:59:00.003992       1 base_controller.go:268] TargetConfigController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:59:00.204832086Z E0611 10:59:00.204731       1 base_controller.go:268] KubeControllerManagerStaticResources reconciliation failed: ["assets/kube-scheduler/ns.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/leader-election-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:leader-locking-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/scheduler-clusterrolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler:public-2": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-role.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/roles/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/rolebindings/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/svc.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/openshift-kube-scheduler-sa": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-client-crb.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler-recovery": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/localhost-recovery-client": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T10:59:01.003180542Z E0611 10:59:01.003088       1 base_controller.go:268] StaticPodStateController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:59:02.403210065Z E0611 10:59:02.403138       1 base_controller.go:268] KubeControllerManagerStaticResources reconciliation failed: ["assets/kube-scheduler/ns.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/leader-election-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:leader-locking-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/scheduler-clusterrolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler:public-2": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-role.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/roles/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/rolebindings/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/svc.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/openshift-kube-scheduler-sa": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-client-crb.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler-recovery": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/localhost-recovery-client": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T10:59:02.603516772Z E0611 10:59:02.603450       1 base_controller.go:268] TargetConfigController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:59:03.404272363Z E0611 10:59:03.404199       1 base_controller.go:268] StaticPodStateController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:59:04.002218796Z E0611 10:59:04.002155       1 base_controller.go:268] InstallerStateController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:59:04.604499040Z E0611 10:59:04.604438       1 base_controller.go:268] KubeControllerManagerStaticResources reconciliation failed: ["assets/kube-scheduler/ns.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/leader-election-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:leader-locking-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/scheduler-clusterrolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler:public-2": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-role.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/roles/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/rolebindings/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/svc.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/openshift-kube-scheduler-sa": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-client-crb.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler-recovery": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/localhost-recovery-client": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T10:59:04.804649344Z E0611 10:59:04.804578       1 base_controller.go:268] BackingResourceController reconciliation failed: ["manifests/installer-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa": dial tcp 172.30.0.1:443: connect: connection refused, "manifests/installer-cluster-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-scheduler-installer": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T10:59:05.603042102Z E0611 10:59:05.602981       1 base_controller.go:268] TargetConfigController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:59:06.404125254Z E0611 10:59:06.404067       1 base_controller.go:268] StaticPodStateController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:59:07.003918783Z E0611 10:59:07.003839       1 base_controller.go:268] KubeControllerManagerStaticResources reconciliation failed: ["assets/kube-scheduler/ns.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/leader-election-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:leader-locking-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/scheduler-clusterrolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler:public-2": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-role.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/roles/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/rolebindings/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/svc.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/openshift-kube-scheduler-sa": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-client-crb.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler-recovery": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/localhost-recovery-client": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T10:59:07.803274969Z E0611 10:59:07.803217       1 base_controller.go:268] TargetConfigController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:59:08.803390717Z E0611 10:59:08.803319       1 base_controller.go:268] KubeControllerManagerStaticResources reconciliation failed: ["assets/kube-scheduler/ns.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/leader-election-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:leader-locking-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/scheduler-clusterrolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler:public-2": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-role.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/roles/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/rolebindings/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/svc.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/openshift-kube-scheduler-sa": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-client-crb.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler-recovery": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/localhost-recovery-client": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T10:59:09.803961697Z E0611 10:59:09.803898       1 base_controller.go:268] TargetConfigController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:59:10.203581386Z E0611 10:59:10.203514       1 base_controller.go:268] StaticPodStateController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:59:11.203803741Z E0611 10:59:11.203730       1 base_controller.go:268] KubeControllerManagerStaticResources reconciliation failed: ["assets/kube-scheduler/ns.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/leader-election-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:leader-locking-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/scheduler-clusterrolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler:public-2": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-role.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/roles/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/rolebindings/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/svc.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/openshift-kube-scheduler-sa": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-client-crb.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler-recovery": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/localhost-recovery-client": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T10:59:11.803547947Z E0611 10:59:11.803487       1 base_controller.go:268] TargetConfigController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:59:13.002993555Z E0611 10:59:13.002930       1 base_controller.go:268] TargetConfigController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:59:13.604036053Z E0611 10:59:13.603968       1 base_controller.go:268] KubeControllerManagerStaticResources reconciliation failed: ["assets/kube-scheduler/ns.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/leader-election-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:leader-locking-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/scheduler-clusterrolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler:public-2": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-role.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/roles/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/rolebindings/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/svc.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/openshift-kube-scheduler-sa": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-client-crb.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler-recovery": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/localhost-recovery-client": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T10:59:14.243683394Z E0611 10:59:14.243602       1 base_controller.go:268] InstallerStateController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:59:14.802899476Z E0611 10:59:14.802838       1 base_controller.go:268] TargetConfigController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:59:15.048091601Z E0611 10:59:15.048039       1 base_controller.go:268] BackingResourceController reconciliation failed: ["manifests/installer-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa": dial tcp 172.30.0.1:443: connect: connection refused, "manifests/installer-cluster-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-scheduler-installer": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T10:59:15.602979626Z E0611 10:59:15.602921       1 base_controller.go:268] StaticPodStateController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:59:16.406760899Z E0611 10:59:16.406699       1 base_controller.go:268] KubeControllerManagerStaticResources reconciliation failed: ["assets/kube-scheduler/ns.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/leader-election-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:leader-locking-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/scheduler-clusterrolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler:public-2": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-role.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/roles/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/rolebindings/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/svc.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/openshift-kube-scheduler-sa": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-client-crb.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler-recovery": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/localhost-recovery-client": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T10:59:17.403457158Z E0611 10:59:17.403389       1 base_controller.go:268] TargetConfigController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:59:19.277702120Z E0611 10:59:19.277606       1 leaderelection.go:332] error retrieving resource lock openshift-kube-scheduler-operator/openshift-cluster-kube-scheduler-operator-lock: Get "https://172.30.0.1:443/apis/coordination.k8s.io/v1/namespaces/openshift-kube-scheduler-operator/leases/openshift-cluster-kube-scheduler-operator-lock?timeout=1m47s": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:59:21.535617979Z E0611 10:59:21.535561       1 base_controller.go:268] KubeControllerManagerStaticResources reconciliation failed: ["assets/kube-scheduler/ns.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/leader-election-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:leader-locking-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/scheduler-clusterrolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler:public-2": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-role.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/roles/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/rolebindings/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/svc.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/openshift-kube-scheduler-sa": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-client-crb.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler-recovery": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/localhost-recovery-client": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T10:59:22.531849202Z E0611 10:59:22.531795       1 base_controller.go:268] TargetConfigController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T10:59:25.851809448Z E0611 10:59:25.851754       1 base_controller.go:268] StaticPodStateController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:00:06.152949134Z I0611 11:00:06.152874       1 reflector.go:351] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:00:08.130719720Z I0611 11:00:08.130664       1 reflector.go:351] Caches populated for *v1.Secret from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:00:10.792700641Z I0611 11:00:10.792630       1 reflector.go:351] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:00:11.969022567Z I0611 11:00:11.968965       1 reflector.go:351] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:00:24.619028458Z I0611 11:00:24.618969       1 reflector.go:351] Caches populated for *v1.ServiceAccount from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:00:26.216070585Z I0611 11:00:26.216005       1 reflector.go:351] Caches populated for *v1.Pod from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:00:27.412584579Z I0611 11:00:27.412521       1 request.go:697] Waited for 1.19535562s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-6-ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T11:00:28.414468263Z I0611 11:00:28.414403       1 request.go:697] Waited for 1.197337094s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ci-op-9xx71rvq-1e28e-w667k-master-0
2024-06-11T11:00:28.617222328Z I0611 11:00:28.617147       1 installer_controller.go:491] Will retry "ci-op-9xx71rvq-1e28e-w667k-master-2" for revision 6 for the 1st time because installer pod failed: installer:    1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:00:28.617222328Z W0611 10:57:59.941259       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:00:28.617222328Z W0611 10:58:09.940822       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:00:28.617222328Z W0611 10:58:19.940980       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:00:28.617222328Z W0611 10:58:29.941484       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:00:28.617222328Z W0611 10:58:39.940922       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:00:28.617222328Z W0611 10:58:39.941659       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:00:28.617222328Z F0611 10:58:39.941689       1 cmd.go:106] timed out waiting for the condition
2024-06-11T11:00:28.617327935Z I0611 11:00:28.617267       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'InstallerPodFailed' installer errors: installer:    1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:00:28.617327935Z W0611 10:57:59.941259       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:00:28.617327935Z W0611 10:58:09.940822       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:00:28.617327935Z W0611 10:58:19.940980       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:00:28.617327935Z W0611 10:58:29.941484       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:00:28.617327935Z W0611 10:58:39.940922       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:00:28.617327935Z W0611 10:58:39.941659       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:00:28.617327935Z F0611 10:58:39.941689       1 cmd.go:106] timed out waiting for the condition
2024-06-11T11:00:28.617369338Z I0611 11:00:28.617346       1 installer_controller.go:500] "ci-op-9xx71rvq-1e28e-w667k-master-2" moving to (v1.NodeStatus) {
2024-06-11T11:00:28.617369338Z  NodeName: (string) (len=35) "ci-op-9xx71rvq-1e28e-w667k-master-2",
2024-06-11T11:00:28.617369338Z  CurrentRevision: (int32) 0,
2024-06-11T11:00:28.617369338Z  TargetRevision: (int32) 6,
2024-06-11T11:00:28.617369338Z  LastFailedRevision: (int32) 6,
2024-06-11T11:00:28.617369338Z  LastFailedTime: (*v1.Time)(0xc0000140d8)(2024-06-11 11:00:28.617130423 +0000 UTC m=+642.427956512),
2024-06-11T11:00:28.617369338Z  LastFailedReason: (string) (len=15) "InstallerFailed",
2024-06-11T11:00:28.617369338Z  LastFailedCount: (int) 1,
2024-06-11T11:00:28.617369338Z  LastFallbackCount: (int) 0,
2024-06-11T11:00:28.617369338Z  LastFailedRevisionErrors: ([]string) (len=1 cap=1) {
2024-06-11T11:00:28.617369338Z   (string) (len=2059) "installer:    1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nW0611 10:57:59.941259       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nW0611 10:58:09.940822       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nW0611 10:58:19.940980       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nW0611 10:58:29.941484       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nW0611 10:58:39.940922       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nW0611 10:58:39.941659       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nF0611 10:58:39.941689       1 cmd.go:106] timed out waiting for the condition\n"
2024-06-11T11:00:28.617369338Z  }
2024-06-11T11:00:28.617369338Z }
2024-06-11T11:00:28.617369338Z  because installer pod failed: installer:    1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:00:28.617369338Z W0611 10:57:59.941259       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:00:28.617369338Z W0611 10:58:09.940822       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:00:28.617369338Z W0611 10:58:19.940980       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:00:28.617369338Z W0611 10:58:29.941484       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:00:28.617369338Z W0611 10:58:39.940922       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:00:28.617369338Z W0611 10:58:39.941659       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:00:28.617369338Z F0611 10:58:39.941689       1 cmd.go:106] timed out waiting for the condition
2024-06-11T11:00:30.217876016Z E0611 11:00:30.217811       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T11:00:30.218079929Z E0611 11:00:30.218051       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T11:00:30.421941463Z I0611 11:00:30.421881       1 installer_controller.go:491] Will retry "ci-op-9xx71rvq-1e28e-w667k-master-2" for revision 6 for the 1st time because installer pod failed: installer:    1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:00:30.421941463Z W0611 10:57:59.941259       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:00:30.421941463Z W0611 10:58:09.940822       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:00:30.421941463Z W0611 10:58:19.940980       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:00:30.421941463Z W0611 10:58:29.941484       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:00:30.421941463Z W0611 10:58:39.940922       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:00:30.421941463Z W0611 10:58:39.941659       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:00:30.421941463Z F0611 10:58:39.941689       1 cmd.go:106] timed out waiting for the condition
2024-06-11T11:00:30.422716812Z I0611 11:00:30.422669       1 installer_controller.go:500] "ci-op-9xx71rvq-1e28e-w667k-master-2" moving to (v1.NodeStatus) {
2024-06-11T11:00:30.422716812Z  NodeName: (string) (len=35) "ci-op-9xx71rvq-1e28e-w667k-master-2",
2024-06-11T11:00:30.422716812Z  CurrentRevision: (int32) 0,
2024-06-11T11:00:30.422716812Z  TargetRevision: (int32) 6,
2024-06-11T11:00:30.422716812Z  LastFailedRevision: (int32) 6,
2024-06-11T11:00:30.422716812Z  LastFailedTime: (*v1.Time)(0xc0020d3518)(2024-06-11 11:00:30.421858358 +0000 UTC m=+644.232684547),
2024-06-11T11:00:30.422716812Z  LastFailedReason: (string) (len=15) "InstallerFailed",
2024-06-11T11:00:30.422716812Z  LastFailedCount: (int) 1,
2024-06-11T11:00:30.422716812Z  LastFallbackCount: (int) 0,
2024-06-11T11:00:30.422716812Z  LastFailedRevisionErrors: ([]string) (len=1 cap=1) {
2024-06-11T11:00:30.422716812Z   (string) (len=2059) "installer:    1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nW0611 10:57:59.941259       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nW0611 10:58:09.940822       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nW0611 10:58:19.940980       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nW0611 10:58:29.941484       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nW0611 10:58:39.940922       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nW0611 10:58:39.941659       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nF0611 10:58:39.941689       1 cmd.go:106] timed out waiting for the condition\n"
2024-06-11T11:00:30.422716812Z  }
2024-06-11T11:00:30.422716812Z }
2024-06-11T11:00:30.422716812Z  because installer pod failed: installer:    1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:00:30.422716812Z W0611 10:57:59.941259       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:00:30.422716812Z W0611 10:58:09.940822       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:00:30.422716812Z W0611 10:58:19.940980       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:00:30.422716812Z W0611 10:58:29.941484       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:00:30.422716812Z W0611 10:58:39.940922       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:00:30.422716812Z W0611 10:58:39.941659       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:00:30.422716812Z F0611 10:58:39.941689       1 cmd.go:106] timed out waiting for the condition
2024-06-11T11:00:30.423817681Z I0611 11:00:30.422500       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'InstallerPodFailed' installer errors: installer:    1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:00:30.423817681Z W0611 10:57:59.941259       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:00:30.423817681Z W0611 10:58:09.940822       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:00:30.423817681Z W0611 10:58:19.940980       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:00:30.423817681Z W0611 10:58:29.941484       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:00:30.423817681Z W0611 10:58:39.940922       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:00:30.423817681Z W0611 10:58:39.941659       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:00:30.423817681Z F0611 10:58:39.941689       1 cmd.go:106] timed out waiting for the condition
2024-06-11T11:00:30.452241056Z I0611 11:00:30.452183       1 helpers.go:260] lister was stale at resourceVersion=19891, live get showed resourceVersion=21277
2024-06-11T11:00:31.215055607Z I0611 11:00:31.214970       1 reflector.go:351] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:00:32.016977700Z I0611 11:00:32.016916       1 reflector.go:351] Caches populated for *v1.Namespace from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:00:32.820435990Z E0611 11:00:32.820372       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T11:00:32.820674405Z E0611 11:00:32.820646       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T11:00:32.860028263Z I0611 11:00:32.859969       1 reflector.go:351] Caches populated for *v1.ClusterOperator from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:00:33.215665578Z I0611 11:00:33.215595       1 reflector.go:351] Caches populated for *v1.Secret from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:00:33.816187691Z I0611 11:00:33.816120       1 reflector.go:351] Caches populated for *v1.Secret from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:00:34.975206751Z I0611 11:00:34.975135       1 reflector.go:351] Caches populated for *v1.FeatureGate from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:00:37.932383513Z I0611 11:00:37.932254       1 reflector.go:351] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:00:38.447207562Z I0611 11:00:38.447147       1 reflector.go:351] Caches populated for *v1.Secret from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:00:38.826211462Z I0611 11:00:38.826136       1 reflector.go:351] Caches populated for *v1.APIServer from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:00:39.933448329Z I0611 11:00:39.933370       1 reflector.go:351] Caches populated for *v1.RoleBinding from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:00:40.583363083Z I0611 11:00:40.583233       1 reflector.go:351] Caches populated for *v1.PodDisruptionBudget from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:00:43.622293764Z I0611 11:00:43.622225       1 reflector.go:351] Caches populated for operator.openshift.io/v1, Resource=kubeschedulers from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:00:43.623394730Z I0611 11:00:43.623261       1 prune_controller.go:269] Nothing to prune
2024-06-11T11:00:43.623575540Z I0611 11:00:43.623529       1 status_controller.go:218] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2024-06-11T10:50:20Z","message":"GuardControllerDegraded: Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2\nNodeInstallerDegraded: 1 nodes are failing on revision 6:\nNodeInstallerDegraded: installer:    1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:57:59.941259       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:09.940822       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:19.940980       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:29.941484       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:39.940922       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:39.941659       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: F0611 10:58:39.941689       1 cmd.go:106] timed out waiting for the condition\nNodeInstallerDegraded: ","reason":"GuardController_SyncError::NodeInstaller_InstallerPodFailed","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-06-11T10:48:34Z","message":"NodeInstallerProgressing: 1 node is at revision 0; 1 node is at revision 5; 1 node is at revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-06-11T10:52:16Z","message":"StaticPodsAvailable: 2 nodes are active; 1 node is at revision 0; 1 node is at revision 5; 1 node is at revision 6","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2024-06-11T10:48:31Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-06-11T10:48:20Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2024-06-11T11:00:43.637045041Z I0611 11:00:43.636977       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded message changed from "GuardControllerDegraded: Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2" to "GuardControllerDegraded: Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2\nNodeInstallerDegraded: 1 nodes are failing on revision 6:\nNodeInstallerDegraded: installer:    1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:57:59.941259       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:09.940822       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:19.940980       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:29.941484       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:39.940922       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:39.941659       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: F0611 10:58:39.941689       1 cmd.go:106] timed out waiting for the condition\nNodeInstallerDegraded: "
2024-06-11T11:00:44.608670113Z I0611 11:00:44.608611       1 reflector.go:351] Caches populated for *v1.ClusterRoleBinding from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:00:44.792039515Z I0611 11:00:44.791975       1 reflector.go:351] Caches populated for *v1.Role from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:00:45.014529844Z I0611 11:00:45.014477       1 reflector.go:351] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:00:46.631333077Z I0611 11:00:46.631213       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/installer-6-retry-1-ci-op-9xx71rvq-1e28e-w667k-master-2 -n openshift-kube-scheduler because it was missing
2024-06-11T11:00:47.228192465Z I0611 11:00:47.228114       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-2" is in transition to 6, but has not made progress because installer is not finished, but in Pending phase
2024-06-11T11:00:47.812256093Z I0611 11:00:47.812137       1 request.go:697] Waited for 1.180195772s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ci-op-9xx71rvq-1e28e-w667k-master-0
2024-06-11T11:00:48.812757181Z I0611 11:00:48.812688       1 request.go:697] Waited for 1.395372166s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T11:00:49.812892847Z I0611 11:00:49.812835       1 request.go:697] Waited for 1.53401631s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/configmaps?resourceVersion=20287
2024-06-11T11:00:49.815838022Z I0611 11:00:49.815796       1 reflector.go:351] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:00:50.015832814Z I0611 11:00:50.015776       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-2" is in transition to 6, but has not made progress because installer is not finished, but in Pending phase
2024-06-11T11:00:50.216784662Z I0611 11:00:50.214836       1 reflector.go:351] Caches populated for *v1.Secret from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:00:51.012107312Z I0611 11:00:51.011965       1 request.go:697] Waited for 1.394773293s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler
2024-06-11T11:00:51.417867870Z E0611 11:00:51.417801       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T11:00:51.418154587Z E0611 11:00:51.418095       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T11:00:52.814863919Z I0611 11:00:52.814795       1 reflector.go:351] Caches populated for *v1.Node from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:00:53.535185014Z I0611 11:00:53.535134       1 reflector.go:351] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:00:54.416674264Z E0611 11:00:54.416609       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T11:00:54.416908178Z E0611 11:00:54.416859       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T11:00:56.411913721Z I0611 11:00:56.411853       1 request.go:697] Waited for 1.080934221s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-6-retry-1-ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T11:00:56.579684575Z I0611 11:00:56.579620       1 reflector.go:351] Caches populated for *v1.RoleBinding from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:00:57.394226172Z I0611 11:00:57.394153       1 reflector.go:351] Caches populated for *v1.ClusterVersion from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:00:57.416471319Z I0611 11:00:57.416419       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-2" is in transition to 6, but has not made progress because installer is not finished, but in Running phase
2024-06-11T11:00:57.816762545Z E0611 11:00:57.816705       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T11:00:57.816933456Z E0611 11:00:57.816912       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T11:01:00.224176315Z E0611 11:01:00.224113       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T11:01:00.224408629Z E0611 11:01:00.224378       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T11:01:01.217477571Z E0611 11:01:01.217288       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T11:01:01.217679083Z E0611 11:01:01.217650       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T11:01:02.022479265Z E0611 11:01:02.022411       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T11:01:02.022736180Z E0611 11:01:02.022686       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T11:01:02.781733271Z E0611 11:01:02.781662       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T11:01:02.832858175Z E0611 11:01:02.832794       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T11:01:03.217114550Z E0611 11:01:03.217052       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T11:01:03.617741586Z E0611 11:01:03.617680       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T11:01:05.638996735Z E0611 11:01:05.638936       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T11:01:05.639154344Z E0611 11:01:05.639130       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T11:01:05.866280388Z I0611 11:01:05.866226       1 reflector.go:351] Caches populated for *v1.Scheduler from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:01:06.616517864Z E0611 11:01:06.616451       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T11:01:06.616699375Z E0611 11:01:06.616669       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T11:01:09.107770313Z I0611 11:01:09.107674       1 reflector.go:351] Caches populated for *v1.Infrastructure from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:01:09.180754191Z I0611 11:01:09.180680       1 reflector.go:351] Caches populated for *v1.Service from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:01:19.905791092Z E0611 11:01:19.905738       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T11:01:19.905970102Z E0611 11:01:19.905943       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T11:01:20.339450228Z E0611 11:01:20.339355       1 guard_controller.go:287] Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T11:01:20.339580236Z E0611 11:01:20.339557       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T11:01:21.263468704Z I0611 11:01:21.263403       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-2" is in transition to 6, but has not made progress because installer is not finished, but in Running phase
2024-06-11T11:01:21.489583880Z E0611 11:01:21.489517       1 guard_controller.go:293] Missing PodIP in operand openshift-kube-scheduler-ci-op-9xx71rvq-1e28e-w667k-master-2 on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T11:01:21.522528358Z E0611 11:01:21.522463       1 base_controller.go:268] GuardController reconciliation failed: Missing PodIP in operand openshift-kube-scheduler-ci-op-9xx71rvq-1e28e-w667k-master-2 on node ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T11:01:21.524620483Z I0611 11:01:21.524556       1 prune_controller.go:269] Nothing to prune
2024-06-11T11:01:21.525339126Z I0611 11:01:21.525269       1 status_controller.go:218] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2024-06-11T10:50:20Z","message":"GuardControllerDegraded: Missing PodIP in operand openshift-kube-scheduler-ci-op-9xx71rvq-1e28e-w667k-master-2 on node ci-op-9xx71rvq-1e28e-w667k-master-2\nNodeInstallerDegraded: 1 nodes are failing on revision 6:\nNodeInstallerDegraded: installer:    1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:57:59.941259       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:09.940822       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:19.940980       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:29.941484       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:39.940922       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:39.941659       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: F0611 10:58:39.941689       1 cmd.go:106] timed out waiting for the condition\nNodeInstallerDegraded: ","reason":"GuardController_SyncError::NodeInstaller_InstallerPodFailed","status":"True","type":"Degraded"},{"lastTransitionTime":"2024-06-11T10:48:34Z","message":"NodeInstallerProgressing: 1 node is at revision 0; 1 node is at revision 5; 1 node is at revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-06-11T10:52:16Z","message":"StaticPodsAvailable: 2 nodes are active; 1 node is at revision 0; 1 node is at revision 5; 1 node is at revision 6","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2024-06-11T10:48:31Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-06-11T10:48:20Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2024-06-11T11:01:21.542595462Z I0611 11:01:21.542507       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded message changed from "GuardControllerDegraded: Missing operand on node ci-op-9xx71rvq-1e28e-w667k-master-2\nNodeInstallerDegraded: 1 nodes are failing on revision 6:\nNodeInstallerDegraded: installer:    1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:57:59.941259       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:09.940822       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:19.940980       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:29.941484       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:39.940922       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:39.941659       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: F0611 10:58:39.941689       1 cmd.go:106] timed out waiting for the condition\nNodeInstallerDegraded: " to "GuardControllerDegraded: Missing PodIP in operand openshift-kube-scheduler-ci-op-9xx71rvq-1e28e-w667k-master-2 on node ci-op-9xx71rvq-1e28e-w667k-master-2\nNodeInstallerDegraded: 1 nodes are failing on revision 6:\nNodeInstallerDegraded: installer:    1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:57:59.941259       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:09.940822       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:19.940980       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:29.941484       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:39.940922       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:39.941659       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: F0611 10:58:39.941689       1 cmd.go:106] timed out waiting for the condition\nNodeInstallerDegraded: "
2024-06-11T11:01:23.087008528Z I0611 11:01:23.086935       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-2" is in transition to 6, but has not made progress because installer is not finished, but in Running phase
2024-06-11T11:01:23.282190110Z I0611 11:01:23.282134       1 request.go:697] Waited for 1.039679126s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller
2024-06-11T11:01:24.482169014Z I0611 11:01:24.482106       1 request.go:697] Waited for 1.393889211s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-6-retry-1-ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T11:01:25.682295527Z I0611 11:01:25.682229       1 request.go:697] Waited for 1.395272693s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ci-op-9xx71rvq-1e28e-w667k-master-0
2024-06-11T11:01:26.882215128Z I0611 11:01:26.882153       1 request.go:697] Waited for 1.191461699s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T11:01:27.086783666Z I0611 11:01:27.086369       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-2" is in transition to 6, but has not made progress because static pod is pending
2024-06-11T11:01:29.081704841Z I0611 11:01:29.081630       1 request.go:697] Waited for 1.07823088s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller
2024-06-11T11:01:30.081802884Z I0611 11:01:30.081739       1 request.go:697] Waited for 1.195651847s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T11:01:30.486334988Z I0611 11:01:30.486244       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-2" is in transition to 6, but has not made progress because static pod is pending
2024-06-11T11:01:30.691496162Z W0611 11:01:30.691354       1 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
2024-06-11T11:01:30.691922287Z I0611 11:01:30.691865       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/openshift-kube-scheduler-guard-ci-op-9xx71rvq-1e28e-w667k-master-2 -n openshift-kube-scheduler because it was missing
2024-06-11T11:01:30.716594851Z I0611 11:01:30.716537       1 prune_controller.go:269] Nothing to prune
2024-06-11T11:01:30.717185886Z I0611 11:01:30.717151       1 status_controller.go:218] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2024-06-11T11:01:30Z","message":"NodeInstallerDegraded: 1 nodes are failing on revision 6:\nNodeInstallerDegraded: installer:    1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:57:59.941259       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:09.940822       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:19.940980       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:29.941484       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:39.940922       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:39.941659       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: F0611 10:58:39.941689       1 cmd.go:106] timed out waiting for the condition\nNodeInstallerDegraded: \nNodeControllerDegraded: All master nodes are ready","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2024-06-11T10:48:34Z","message":"NodeInstallerProgressing: 1 node is at revision 0; 1 node is at revision 5; 1 node is at revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-06-11T10:52:16Z","message":"StaticPodsAvailable: 2 nodes are active; 1 node is at revision 0; 1 node is at revision 5; 1 node is at revision 6","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2024-06-11T10:48:31Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-06-11T10:48:20Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2024-06-11T11:01:30.738417934Z I0611 11:01:30.738352       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded changed from True to False ("NodeInstallerDegraded: 1 nodes are failing on revision 6:\nNodeInstallerDegraded: installer:    1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:57:59.941259       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:09.940822       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:19.940980       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:29.941484       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:39.940922       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:39.941659       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: F0611 10:58:39.941689       1 cmd.go:106] timed out waiting for the condition\nNodeInstallerDegraded: \nNodeControllerDegraded: All master nodes are ready")
2024-06-11T11:01:31.281573508Z I0611 11:01:31.281511       1 request.go:697] Waited for 1.195370263s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T11:01:31.304951654Z I0611 11:01:31.304894       1 prune_controller.go:269] Nothing to prune
2024-06-11T11:01:31.305781202Z I0611 11:01:31.305684       1 status_controller.go:218] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2024-06-11T11:01:30Z","message":"NodeInstallerDegraded: 1 nodes are failing on revision 6:\nNodeInstallerDegraded: installer:    1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:57:59.941259       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:09.940822       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:19.940980       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:29.941484       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:39.940922       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:39.941659       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: F0611 10:58:39.941689       1 cmd.go:106] timed out waiting for the condition\nNodeInstallerDegraded: \nStaticPodsDegraded: pod/openshift-kube-scheduler-ci-op-9xx71rvq-1e28e-w667k-master-0 container \"kube-scheduler-cert-syncer\" is terminated: Error: 9.1/tools/cache/reflector.go:229: failed to list *v1.ConfigMap: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-scheduler/configmaps?limit=500\u0026resourceVersion=0\": tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded: E0611 11:00:30.697705       1 reflector.go:147] k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-scheduler/configmaps?limit=500\u0026resourceVersion=0\": tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded: W0611 11:00:33.307678       1 reflector.go:539] k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229: failed to list *v1.Secret: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-scheduler/secrets?limit=500\u0026resourceVersion=0\": tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded: E0611 11:00:33.307748       1 reflector.go:147] k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229: Failed to watch *v1.Secret: failed to list *v1.Secret: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-scheduler/secrets?limit=500\u0026resourceVersion=0\": tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded: W0611 11:01:26.353360       1 reflector.go:539] k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229: failed to list *v1.Secret: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-scheduler/secrets?limit=500\u0026resourceVersion=0\": tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded: E0611 11:01:26.353426       1 reflector.go:147] k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229: Failed to watch *v1.Secret: failed to list *v1.Secret: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-scheduler/secrets?limit=500\u0026resourceVersion=0\": tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded: F0611 11:01:27.382663       1 base_controller.go:96] unable to sync caches for CertSyncController\nStaticPodsDegraded: \nNodeControllerDegraded: All master nodes are ready","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2024-06-11T10:48:34Z","message":"NodeInstallerProgressing: 1 node is at revision 0; 1 node is at revision 5; 1 node is at revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-06-11T10:52:16Z","message":"StaticPodsAvailable: 2 nodes are active; 1 node is at revision 0; 1 node is at revision 5; 1 node is at revision 6","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2024-06-11T10:48:31Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-06-11T10:48:20Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2024-06-11T11:01:31.319970819Z I0611 11:01:31.319893       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded message changed from "NodeInstallerDegraded: 1 nodes are failing on revision 6:\nNodeInstallerDegraded: installer:    1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:57:59.941259       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:09.940822       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:19.940980       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:29.941484       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:39.940922       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:39.941659       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: F0611 10:58:39.941689       1 cmd.go:106] timed out waiting for the condition\nNodeInstallerDegraded: \nNodeControllerDegraded: All master nodes are ready" to "NodeInstallerDegraded: 1 nodes are failing on revision 6:\nNodeInstallerDegraded: installer:    1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:57:59.941259       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:09.940822       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:19.940980       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:29.941484       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:39.940922       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:39.941659       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: F0611 10:58:39.941689       1 cmd.go:106] timed out waiting for the condition\nNodeInstallerDegraded: \nStaticPodsDegraded: pod/openshift-kube-scheduler-ci-op-9xx71rvq-1e28e-w667k-master-0 container \"kube-scheduler-cert-syncer\" is terminated: Error: 9.1/tools/cache/reflector.go:229: failed to list *v1.ConfigMap: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-scheduler/configmaps?limit=500&resourceVersion=0\": tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded: E0611 11:00:30.697705       1 reflector.go:147] k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-scheduler/configmaps?limit=500&resourceVersion=0\": tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded: W0611 11:00:33.307678       1 reflector.go:539] k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229: failed to list *v1.Secret: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-scheduler/secrets?limit=500&resourceVersion=0\": tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded: E0611 11:00:33.307748       1 reflector.go:147] k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229: Failed to watch *v1.Secret: failed to list *v1.Secret: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-scheduler/secrets?limit=500&resourceVersion=0\": tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded: W0611 11:01:26.353360       1 reflector.go:539] k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229: failed to list *v1.Secret: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-scheduler/secrets?limit=500&resourceVersion=0\": tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded: E0611 11:01:26.353426       1 reflector.go:147] k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229: Failed to watch *v1.Secret: failed to list *v1.Secret: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-scheduler/secrets?limit=500&resourceVersion=0\": tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded: F0611 11:01:27.382663       1 base_controller.go:96] unable to sync caches for CertSyncController\nStaticPodsDegraded: \nNodeControllerDegraded: All master nodes are ready"
2024-06-11T11:01:32.281748497Z I0611 11:01:32.281679       1 request.go:697] Waited for 1.386176314s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/localhost-recovery-client
2024-06-11T11:01:33.481935203Z I0611 11:01:33.481855       1 request.go:697] Waited for 1.391401215s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa
2024-06-11T11:01:34.286742342Z I0611 11:01:34.286678       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-2" is in transition to 6, but has not made progress because static pod is pending
2024-06-11T11:01:34.482176795Z I0611 11:01:34.482106       1 request.go:697] Waited for 1.392675188s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller
2024-06-11T11:01:35.307785033Z I0611 11:01:35.307722       1 prune_controller.go:269] Nothing to prune
2024-06-11T11:01:35.308466472Z I0611 11:01:35.308420       1 status_controller.go:218] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2024-06-11T11:01:30Z","message":"NodeInstallerDegraded: 1 nodes are failing on revision 6:\nNodeInstallerDegraded: installer:    1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:57:59.941259       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:09.940822       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:19.940980       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:29.941484       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:39.940922       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:39.941659       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: F0611 10:58:39.941689       1 cmd.go:106] timed out waiting for the condition\nNodeInstallerDegraded: \nNodeControllerDegraded: All master nodes are ready","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2024-06-11T10:48:34Z","message":"NodeInstallerProgressing: 1 node is at revision 0; 1 node is at revision 5; 1 node is at revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-06-11T10:52:16Z","message":"StaticPodsAvailable: 2 nodes are active; 1 node is at revision 0; 1 node is at revision 5; 1 node is at revision 6","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2024-06-11T10:48:31Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-06-11T10:48:20Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2024-06-11T11:01:35.327671578Z I0611 11:01:35.327582       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded message changed from "NodeInstallerDegraded: 1 nodes are failing on revision 6:\nNodeInstallerDegraded: installer:    1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:57:59.941259       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:09.940822       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:19.940980       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:29.941484       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:39.940922       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:39.941659       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: F0611 10:58:39.941689       1 cmd.go:106] timed out waiting for the condition\nNodeInstallerDegraded: \nStaticPodsDegraded: pod/openshift-kube-scheduler-ci-op-9xx71rvq-1e28e-w667k-master-0 container \"kube-scheduler-cert-syncer\" is terminated: Error: 9.1/tools/cache/reflector.go:229: failed to list *v1.ConfigMap: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-scheduler/configmaps?limit=500&resourceVersion=0\": tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded: E0611 11:00:30.697705       1 reflector.go:147] k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-scheduler/configmaps?limit=500&resourceVersion=0\": tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded: W0611 11:00:33.307678       1 reflector.go:539] k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229: failed to list *v1.Secret: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-scheduler/secrets?limit=500&resourceVersion=0\": tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded: E0611 11:00:33.307748       1 reflector.go:147] k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229: Failed to watch *v1.Secret: failed to list *v1.Secret: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-scheduler/secrets?limit=500&resourceVersion=0\": tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded: W0611 11:01:26.353360       1 reflector.go:539] k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229: failed to list *v1.Secret: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-scheduler/secrets?limit=500&resourceVersion=0\": tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded: E0611 11:01:26.353426       1 reflector.go:147] k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229: Failed to watch *v1.Secret: failed to list *v1.Secret: Get \"https://localhost:6443/api/v1/namespaces/openshift-kube-scheduler/secrets?limit=500&resourceVersion=0\": tls: failed to verify certificate: x509: certificate signed by unknown authority\nStaticPodsDegraded: F0611 11:01:27.382663       1 base_controller.go:96] unable to sync caches for CertSyncController\nStaticPodsDegraded: \nNodeControllerDegraded: All master nodes are ready" to "NodeInstallerDegraded: 1 nodes are failing on revision 6:\nNodeInstallerDegraded: installer:    1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:57:59.941259       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:09.940822       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:19.940980       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:29.941484       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:39.940922       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:39.941659       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: F0611 10:58:39.941689       1 cmd.go:106] timed out waiting for the condition\nNodeInstallerDegraded: \nNodeControllerDegraded: All master nodes are ready"
2024-06-11T11:01:35.482221277Z I0611 11:01:35.482147       1 request.go:697] Waited for 1.194451075s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-6-retry-1-ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T11:01:36.681964257Z I0611 11:01:36.681817       1 request.go:697] Waited for 1.194946104s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-6-retry-1-ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T11:01:37.701665770Z I0611 11:01:37.701600       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-2" is in transition to 6, but has not made progress because static pod is pending
2024-06-11T11:01:37.932668971Z I0611 11:01:37.932536       1 core.go:218] Pod "openshift-kube-scheduler/openshift-kube-scheduler-guard-ci-op-9xx71rvq-1e28e-w667k-master-2" changes: {"apiVersion":"v1","kind":"Pod","metadata":{"annotations":{"k8s.ovn.org/pod-networks":null,"k8s.v1.cni.cncf.io/network-status":null,"target.workload.openshift.io/management":"{\"effect\": \"PreferredDuringScheduling\"}"},"creationTimestamp":null,"managedFields":null,"resourceVersion":null,"uid":null},"spec":{"containers":[{"args":["-c","# properly handle TERM and exit as soon as it is signaled\nset -euo pipefail\ntrap 'jobs -p | xargs -r kill; exit 0' TERM\nsleep infinity \u0026 wait\n"],"command":["/bin/bash"],"image":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:68ae5e595cb6b6ffa3f6861f7a41a92f5db8e9cd77fabb216dd7a96b9c1b4cf5","imagePullPolicy":"IfNotPresent","name":"guard","readinessProbe":{"failureThreshold":3,"httpGet":{"host":"10.0.0.7","path":"healthz","port":10259,"scheme":"HTTPS"},"periodSeconds":5,"successThreshold":1,"timeoutSeconds":5},"resources":{"requests":{"cpu":"10m","memory":"5Mi"}},"terminationMessagePolicy":"FallbackToLogsOnError"}],"dnsPolicy":null,"enableServiceLinks":null,"preemptionPolicy":null,"priority":null,"restartPolicy":null,"schedulerName":null,"securityContext":null,"serviceAccount":null,"serviceAccountName":null,"tolerations":[{"effect":"NoSchedule","key":"node-role.kubernetes.io/master","operator":"Exists"},{"effect":"NoExecute","key":"node.kubernetes.io/not-ready","operator":"Exists"},{"effect":"NoExecute","key":"node.kubernetes.io/unreachable","operator":"Exists"},{"effect":"NoSchedule","key":"node-role.kubernetes.io/etcd","operator":"Exists"}],"volumes":null},"status":{"conditions":null,"containerStatuses":null,"hostIP":null,"hostIPs":null,"phase":null,"podIP":null,"podIPs":null,"qosClass":null,"startTime":null}}
2024-06-11T11:01:38.945095086Z I0611 11:01:38.944968       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodUpdated' Updated Pod/openshift-kube-scheduler-guard-ci-op-9xx71rvq-1e28e-w667k-master-2 -n openshift-kube-scheduler because it changed
2024-06-11T11:01:40.082025367Z I0611 11:01:40.081939       1 request.go:697] Waited for 1.136184949s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ci-op-9xx71rvq-1e28e-w667k-master-0
2024-06-11T11:01:40.887321165Z I0611 11:01:40.887236       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-2" is in transition to 6, but has not made progress because static pod is pending
2024-06-11T11:01:43.287358255Z I0611 11:01:43.287277       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-2" is in transition to 6, but has not made progress because static pod is pending
2024-06-11T11:02:11.603671006Z I0611 11:02:11.603604       1 installer_controller.go:500] "ci-op-9xx71rvq-1e28e-w667k-master-2" moving to (v1.NodeStatus) {
2024-06-11T11:02:11.603671006Z  NodeName: (string) (len=35) "ci-op-9xx71rvq-1e28e-w667k-master-2",
2024-06-11T11:02:11.603671006Z  CurrentRevision: (int32) 6,
2024-06-11T11:02:11.603671006Z  TargetRevision: (int32) 0,
2024-06-11T11:02:11.603671006Z  LastFailedRevision: (int32) 6,
2024-06-11T11:02:11.603671006Z  LastFailedTime: (*v1.Time)(0xc0018a8018)(2024-06-11 11:00:30 +0000 UTC),
2024-06-11T11:02:11.603671006Z  LastFailedReason: (string) (len=15) "InstallerFailed",
2024-06-11T11:02:11.603671006Z  LastFailedCount: (int) 1,
2024-06-11T11:02:11.603671006Z  LastFallbackCount: (int) 0,
2024-06-11T11:02:11.603671006Z  LastFailedRevisionErrors: ([]string) (len=1 cap=1) {
2024-06-11T11:02:11.603671006Z   (string) (len=2059) "installer:    1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nW0611 10:57:59.941259       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nW0611 10:58:09.940822       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nW0611 10:58:19.940980       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nW0611 10:58:29.941484       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nW0611 10:58:39.940922       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nW0611 10:58:39.941659       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nF0611 10:58:39.941689       1 cmd.go:106] timed out waiting for the condition\n"
2024-06-11T11:02:11.603671006Z  }
2024-06-11T11:02:11.603671006Z }
2024-06-11T11:02:11.603671006Z  because static pod is ready
2024-06-11T11:02:11.624665356Z I0611 11:02:11.624597       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeCurrentRevisionChanged' Updated node "ci-op-9xx71rvq-1e28e-w667k-master-2" from revision 0 to 6 because static pod is ready
2024-06-11T11:02:11.625094080Z I0611 11:02:11.625043       1 prune_controller.go:269] Nothing to prune
2024-06-11T11:02:11.625978628Z I0611 11:02:11.625933       1 status_controller.go:218] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2024-06-11T11:01:30Z","message":"NodeControllerDegraded: All master nodes are ready","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2024-06-11T10:48:34Z","message":"NodeInstallerProgressing: 1 node is at revision 5; 2 nodes are at revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-06-11T10:52:16Z","message":"StaticPodsAvailable: 3 nodes are active; 1 node is at revision 5; 2 nodes are at revision 6","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2024-06-11T10:48:31Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-06-11T10:48:20Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2024-06-11T11:02:11.641216163Z I0611 11:02:11.641164       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded message changed from "NodeInstallerDegraded: 1 nodes are failing on revision 6:\nNodeInstallerDegraded: installer:    1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:57:59.941259       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:09.940822       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:19.940980       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:29.941484       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:39.940922       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 10:58:39.941659       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-2: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: F0611 10:58:39.941689       1 cmd.go:106] timed out waiting for the condition\nNodeInstallerDegraded: \nNodeControllerDegraded: All master nodes are ready" to "NodeControllerDegraded: All master nodes are ready",Progressing message changed from "NodeInstallerProgressing: 1 node is at revision 0; 1 node is at revision 5; 1 node is at revision 6" to "NodeInstallerProgressing: 1 node is at revision 5; 2 nodes are at revision 6",Available message changed from "StaticPodsAvailable: 2 nodes are active; 1 node is at revision 0; 1 node is at revision 5; 1 node is at revision 6" to "StaticPodsAvailable: 3 nodes are active; 1 node is at revision 5; 2 nodes are at revision 6"
2024-06-11T11:02:12.770090808Z I0611 11:02:12.770022       1 request.go:697] Waited for 1.144015875s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/configmaps/config
2024-06-11T11:02:15.975602522Z I0611 11:02:15.975536       1 installer_controller.go:524] node ci-op-9xx71rvq-1e28e-w667k-master-0 with revision 5 is the oldest and needs new revision 6
2024-06-11T11:02:15.975602522Z I0611 11:02:15.975593       1 installer_controller.go:532] "ci-op-9xx71rvq-1e28e-w667k-master-0" moving to (v1.NodeStatus) {
2024-06-11T11:02:15.975602522Z  NodeName: (string) (len=35) "ci-op-9xx71rvq-1e28e-w667k-master-0",
2024-06-11T11:02:15.975602522Z  CurrentRevision: (int32) 5,
2024-06-11T11:02:15.975602522Z  TargetRevision: (int32) 6,
2024-06-11T11:02:15.975602522Z  LastFailedRevision: (int32) 0,
2024-06-11T11:02:15.975602522Z  LastFailedTime: (*v1.Time)(<nil>),
2024-06-11T11:02:15.975602522Z  LastFailedReason: (string) "",
2024-06-11T11:02:15.975602522Z  LastFailedCount: (int) 0,
2024-06-11T11:02:15.975602522Z  LastFallbackCount: (int) 0,
2024-06-11T11:02:15.975602522Z  LastFailedRevisionErrors: ([]string) <nil>
2024-06-11T11:02:15.975602522Z }
2024-06-11T11:02:15.994365650Z I0611 11:02:15.994214       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeTargetRevisionChanged' Updating node "ci-op-9xx71rvq-1e28e-w667k-master-0" from revision 5 to 6 because node ci-op-9xx71rvq-1e28e-w667k-master-0 with revision 5 is the oldest
2024-06-11T11:02:15.995248499Z I0611 11:02:15.995194       1 prune_controller.go:269] Nothing to prune
2024-06-11T11:02:17.788739455Z I0611 11:02:17.788669       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/installer-6-ci-op-9xx71rvq-1e28e-w667k-master-0 -n openshift-kube-scheduler because it was missing
2024-06-11T11:02:18.574222788Z I0611 11:02:18.574162       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-0" is in transition to 6, but has not made progress because installer is not finished, but in Pending phase
2024-06-11T11:02:18.970564396Z I0611 11:02:18.970499       1 request.go:697] Waited for 1.180551672s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ci-op-9xx71rvq-1e28e-w667k-master-0
2024-06-11T11:02:20.170104681Z I0611 11:02:20.170015       1 request.go:697] Waited for 1.148950306s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ci-op-9xx71rvq-1e28e-w667k-master-0
2024-06-11T11:02:20.774431120Z I0611 11:02:20.774360       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-0" is in transition to 6, but has not made progress because installer is not finished, but in Running phase
2024-06-11T11:02:22.574765299Z I0611 11:02:22.574705       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-0" is in transition to 6, but has not made progress because installer is not finished, but in Running phase
2024-06-11T11:03:13.415367729Z E0611 11:03:13.415248       1 leaderelection.go:332] error retrieving resource lock openshift-kube-scheduler-operator/openshift-cluster-kube-scheduler-operator-lock: Get "https://172.30.0.1:443/apis/coordination.k8s.io/v1/namespaces/openshift-kube-scheduler-operator/leases/openshift-cluster-kube-scheduler-operator-lock?timeout=1m47s": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:03:39.417504316Z E0611 11:03:39.417424       1 leaderelection.go:332] error retrieving resource lock openshift-kube-scheduler-operator/openshift-cluster-kube-scheduler-operator-lock: Get "https://172.30.0.1:443/apis/coordination.k8s.io/v1/namespaces/openshift-kube-scheduler-operator/leases/openshift-cluster-kube-scheduler-operator-lock?timeout=1m47s": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:03:47.203400302Z E0611 11:03:47.203342       1 base_controller.go:268] InstallerStateController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:03:47.205101391Z E0611 11:03:47.205063       1 base_controller.go:268] BackingResourceController reconciliation failed: ["manifests/installer-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa": dial tcp 172.30.0.1:443: connect: connection refused, "manifests/installer-cluster-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-scheduler-installer": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T11:03:47.205459710Z E0611 11:03:47.205434       1 base_controller.go:268] StaticPodStateController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:03:47.210231460Z E0611 11:03:47.210193       1 base_controller.go:268] InstallerStateController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:03:47.212521980Z E0611 11:03:47.212470       1 base_controller.go:268] BackingResourceController reconciliation failed: ["manifests/installer-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa": dial tcp 172.30.0.1:443: connect: connection refused, "manifests/installer-cluster-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-scheduler-installer": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T11:03:47.214200367Z E0611 11:03:47.214157       1 base_controller.go:268] StaticPodStateController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:03:47.404181819Z E0611 11:03:47.404115       1 base_controller.go:268] InstallerStateController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:03:47.606125797Z E0611 11:03:47.606064       1 base_controller.go:268] BackingResourceController reconciliation failed: ["manifests/installer-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa": dial tcp 172.30.0.1:443: connect: connection refused, "manifests/installer-cluster-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-scheduler-installer": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T11:03:48.004213350Z E0611 11:03:48.004163       1 base_controller.go:268] InstallerStateController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:03:48.205698704Z E0611 11:03:48.205643       1 base_controller.go:268] BackingResourceController reconciliation failed: ["manifests/installer-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa": dial tcp 172.30.0.1:443: connect: connection refused, "manifests/installer-cluster-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-scheduler-installer": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T11:03:48.804186755Z E0611 11:03:48.804127       1 base_controller.go:268] InstallerStateController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:03:49.206397923Z E0611 11:03:49.206331       1 base_controller.go:268] BackingResourceController reconciliation failed: ["manifests/installer-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa": dial tcp 172.30.0.1:443: connect: connection refused, "manifests/installer-cluster-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-scheduler-installer": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T11:03:49.604969001Z E0611 11:03:49.604896       1 base_controller.go:268] StaticPodStateController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:03:49.804492353Z E0611 11:03:49.804431       1 base_controller.go:268] InstallerStateController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:03:50.205400153Z E0611 11:03:50.205348       1 base_controller.go:268] BackingResourceController reconciliation failed: ["manifests/installer-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa": dial tcp 172.30.0.1:443: connect: connection refused, "manifests/installer-cluster-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-scheduler-installer": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T11:03:50.803578787Z E0611 11:03:50.803512       1 base_controller.go:268] InstallerStateController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:03:51.205719752Z E0611 11:03:51.205664       1 base_controller.go:268] BackingResourceController reconciliation failed: ["manifests/installer-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa": dial tcp 172.30.0.1:443: connect: connection refused, "manifests/installer-cluster-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-scheduler-installer": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T11:03:51.406524271Z E0611 11:03:51.406463       1 base_controller.go:268] KubeControllerManagerStaticResources reconciliation failed: ["assets/kube-scheduler/ns.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/leader-election-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:leader-locking-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/scheduler-clusterrolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler:public-2": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-role.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/roles/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/rolebindings/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/svc.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/openshift-kube-scheduler-sa": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-client-crb.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler-recovery": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/localhost-recovery-client": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T11:03:52.003667951Z E0611 11:03:52.003593       1 base_controller.go:268] InstallerStateController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:03:52.406093331Z E0611 11:03:52.406013       1 base_controller.go:268] BackingResourceController reconciliation failed: ["manifests/installer-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa": dial tcp 172.30.0.1:443: connect: connection refused, "manifests/installer-cluster-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-scheduler-installer": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T11:03:52.606140009Z E0611 11:03:52.606074       1 base_controller.go:268] StaticPodStateController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:03:52.805033928Z E0611 11:03:52.804978       1 base_controller.go:268] TargetConfigController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:03:53.204443050Z E0611 11:03:53.204372       1 base_controller.go:268] InstallerStateController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:03:54.006260851Z E0611 11:03:54.006197       1 base_controller.go:268] BackingResourceController reconciliation failed: ["manifests/installer-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa": dial tcp 172.30.0.1:443: connect: connection refused, "manifests/installer-cluster-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-scheduler-installer": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T11:03:54.606287982Z E0611 11:03:54.606207       1 base_controller.go:268] KubeControllerManagerStaticResources reconciliation failed: ["assets/kube-scheduler/ns.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/leader-election-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:leader-locking-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/scheduler-clusterrolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler:public-2": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-role.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/roles/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/rolebindings/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/svc.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/openshift-kube-scheduler-sa": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-client-crb.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler-recovery": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/localhost-recovery-client": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T11:03:54.805632424Z E0611 11:03:54.805543       1 base_controller.go:268] StaticPodStateController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:03:55.205272508Z E0611 11:03:55.205218       1 base_controller.go:268] InstallerStateController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:03:56.007041492Z E0611 11:03:56.006967       1 base_controller.go:268] BackingResourceController reconciliation failed: ["manifests/installer-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa": dial tcp 172.30.0.1:443: connect: connection refused, "manifests/installer-cluster-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-scheduler-installer": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T11:03:56.606588945Z E0611 11:03:56.606510       1 base_controller.go:268] TargetConfigController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:03:57.005607247Z E0611 11:03:57.005545       1 base_controller.go:268] StaticPodStateController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:03:57.406604245Z E0611 11:03:57.406513       1 base_controller.go:268] KubeControllerManagerStaticResources reconciliation failed: ["assets/kube-scheduler/ns.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/leader-election-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:leader-locking-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/scheduler-clusterrolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler:public-2": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-role.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/roles/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/rolebindings/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/svc.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/openshift-kube-scheduler-sa": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-client-crb.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler-recovery": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/localhost-recovery-client": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T11:03:58.404164851Z E0611 11:03:58.404107       1 base_controller.go:268] InstallerStateController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:03:59.006085218Z E0611 11:03:59.006031       1 base_controller.go:268] StaticPodStateController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:03:59.208514760Z E0611 11:03:59.208444       1 base_controller.go:268] BackingResourceController reconciliation failed: ["manifests/installer-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa": dial tcp 172.30.0.1:443: connect: connection refused, "manifests/installer-cluster-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-scheduler-installer": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T11:04:00.005321405Z E0611 11:04:00.005247       1 base_controller.go:268] TargetConfigController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:04:00.206385081Z E0611 11:04:00.206288       1 base_controller.go:268] KubeControllerManagerStaticResources reconciliation failed: ["assets/kube-scheduler/ns.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/leader-election-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:leader-locking-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/scheduler-clusterrolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler:public-2": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-role.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/roles/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/rolebindings/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/svc.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/openshift-kube-scheduler-sa": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-client-crb.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler-recovery": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/localhost-recovery-client": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T11:04:01.005249126Z E0611 11:04:01.005193       1 base_controller.go:268] StaticPodStateController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:04:02.405073964Z E0611 11:04:02.405000       1 base_controller.go:268] KubeControllerManagerStaticResources reconciliation failed: ["assets/kube-scheduler/ns.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/leader-election-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:leader-locking-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/scheduler-clusterrolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler:public-2": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-role.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/roles/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/rolebindings/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/svc.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/openshift-kube-scheduler-sa": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-client-crb.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler-recovery": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/localhost-recovery-client": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T11:04:02.604776880Z E0611 11:04:02.604608       1 base_controller.go:268] TargetConfigController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:04:03.405231914Z E0611 11:04:03.405179       1 base_controller.go:268] StaticPodStateController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:04:04.004020606Z E0611 11:04:04.003947       1 base_controller.go:268] InstallerStateController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:04:04.605478932Z E0611 11:04:04.605422       1 base_controller.go:268] KubeControllerManagerStaticResources reconciliation failed: ["assets/kube-scheduler/ns.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/leader-election-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:leader-locking-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/scheduler-clusterrolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler:public-2": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-role.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/roles/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/rolebindings/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/svc.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/openshift-kube-scheduler-sa": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-client-crb.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler-recovery": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/localhost-recovery-client": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T11:04:04.806455766Z E0611 11:04:04.806387       1 base_controller.go:268] BackingResourceController reconciliation failed: ["manifests/installer-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa": dial tcp 172.30.0.1:443: connect: connection refused, "manifests/installer-cluster-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-scheduler-installer": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T11:04:05.417075254Z E0611 11:04:05.417013       1 leaderelection.go:332] error retrieving resource lock openshift-kube-scheduler-operator/openshift-cluster-kube-scheduler-operator-lock: Get "https://172.30.0.1:443/apis/coordination.k8s.io/v1/namespaces/openshift-kube-scheduler-operator/leases/openshift-cluster-kube-scheduler-operator-lock?timeout=1m47s": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:04:05.605994679Z E0611 11:04:05.605930       1 base_controller.go:268] TargetConfigController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:04:06.405154574Z E0611 11:04:06.405100       1 base_controller.go:268] StaticPodStateController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:04:07.006134176Z E0611 11:04:07.006070       1 base_controller.go:268] KubeControllerManagerStaticResources reconciliation failed: ["assets/kube-scheduler/ns.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/leader-election-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:leader-locking-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/scheduler-clusterrolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler:public-2": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-role.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/roles/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/rolebindings/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/svc.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/openshift-kube-scheduler-sa": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-client-crb.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler-recovery": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/localhost-recovery-client": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T11:04:07.804872049Z E0611 11:04:07.804815       1 base_controller.go:268] TargetConfigController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:04:08.804880871Z E0611 11:04:08.804824       1 base_controller.go:268] KubeControllerManagerStaticResources reconciliation failed: ["assets/kube-scheduler/ns.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/leader-election-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:leader-locking-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/scheduler-clusterrolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler:public-2": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-role.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/roles/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/rolebindings/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/svc.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/openshift-kube-scheduler-sa": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-client-crb.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler-recovery": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/localhost-recovery-client": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T11:04:09.806753486Z E0611 11:04:09.806684       1 base_controller.go:268] TargetConfigController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:04:10.206904363Z E0611 11:04:10.206802       1 base_controller.go:268] StaticPodStateController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:04:11.205062636Z E0611 11:04:11.205003       1 base_controller.go:268] KubeControllerManagerStaticResources reconciliation failed: ["assets/kube-scheduler/ns.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/leader-election-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:leader-locking-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/scheduler-clusterrolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler:public-2": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-role.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/roles/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/rolebindings/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/svc.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/openshift-kube-scheduler-sa": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-client-crb.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler-recovery": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/localhost-recovery-client": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T11:04:11.804973946Z E0611 11:04:11.804915       1 base_controller.go:268] TargetConfigController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:04:13.005690907Z E0611 11:04:13.005600       1 base_controller.go:268] TargetConfigController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:04:13.605068792Z E0611 11:04:13.605001       1 base_controller.go:268] KubeControllerManagerStaticResources reconciliation failed: ["assets/kube-scheduler/ns.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/leader-election-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:leader-locking-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/scheduler-clusterrolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler:public-2": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-role.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/roles/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/rolebindings/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/svc.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/openshift-kube-scheduler-sa": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-client-crb.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler-recovery": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/localhost-recovery-client": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T11:04:14.246359414Z E0611 11:04:14.246254       1 base_controller.go:268] InstallerStateController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:04:14.805709350Z E0611 11:04:14.805653       1 base_controller.go:268] TargetConfigController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:04:15.050178142Z E0611 11:04:15.050119       1 base_controller.go:268] BackingResourceController reconciliation failed: ["manifests/installer-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa": dial tcp 172.30.0.1:443: connect: connection refused, "manifests/installer-cluster-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-kube-scheduler-installer": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T11:04:15.605746504Z E0611 11:04:15.605665       1 base_controller.go:268] StaticPodStateController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:04:16.406711791Z E0611 11:04:16.406658       1 base_controller.go:268] KubeControllerManagerStaticResources reconciliation failed: ["assets/kube-scheduler/ns.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/leader-election-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:leader-locking-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/scheduler-clusterrolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler:public-2": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-role.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/roles/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/rolebindings/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/svc.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/openshift-kube-scheduler-sa": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-client-crb.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler-recovery": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/localhost-recovery-client": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T11:04:17.405169398Z E0611 11:04:17.405115       1 base_controller.go:268] TargetConfigController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:04:21.537118400Z E0611 11:04:21.537069       1 base_controller.go:268] KubeControllerManagerStaticResources reconciliation failed: ["assets/kube-scheduler/ns.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/leader-election-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:leader-locking-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/scheduler-clusterrolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler:public-2": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-role.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/roles/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/rolebindings/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/svc.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/openshift-kube-scheduler-sa": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-client-crb.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler-recovery": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/localhost-recovery-client": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2024-06-11T11:04:22.533194536Z E0611 11:04:22.533142       1 base_controller.go:268] TargetConfigController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:05:00.015660438Z I0611 11:05:00.015585       1 reflector.go:351] Caches populated for *v1.ClusterRoleBinding from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:05:05.175708829Z I0611 11:05:05.175646       1 reflector.go:351] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:05:10.122850997Z I0611 11:05:10.122770       1 reflector.go:351] Caches populated for *v1.ClusterOperator from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:05:11.431461770Z I0611 11:05:11.431392       1 reflector.go:351] Caches populated for *v1.ClusterVersion from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:05:11.844379130Z I0611 11:05:11.844277       1 reflector.go:351] Caches populated for *v1.PodDisruptionBudget from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:05:15.163243052Z I0611 11:05:15.163177       1 reflector.go:351] Caches populated for operator.openshift.io/v1, Resource=kubeschedulers from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:05:15.163709769Z I0611 11:05:15.163658       1 prune_controller.go:269] Nothing to prune
2024-06-11T11:05:15.174098452Z I0611 11:05:15.174048       1 installer_controller.go:491] Will retry "ci-op-9xx71rvq-1e28e-w667k-master-0" for revision 6 for the 1st time because installer pod failed: installer:    1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:05:15.174098452Z W0611 11:02:40.056541       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:05:15.174098452Z W0611 11:02:50.052606       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:05:15.174098452Z W0611 11:03:00.052110       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:05:15.174098452Z W0611 11:03:10.052879       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:05:15.174098452Z W0611 11:03:20.051639       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:05:15.174098452Z W0611 11:03:20.052502       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:05:15.174098452Z F0611 11:03:20.052533       1 cmd.go:106] timed out waiting for the condition
2024-06-11T11:05:15.174175455Z I0611 11:05:15.174138       1 installer_controller.go:500] "ci-op-9xx71rvq-1e28e-w667k-master-0" moving to (v1.NodeStatus) {
2024-06-11T11:05:15.174175455Z  NodeName: (string) (len=35) "ci-op-9xx71rvq-1e28e-w667k-master-0",
2024-06-11T11:05:15.174175455Z  CurrentRevision: (int32) 5,
2024-06-11T11:05:15.174175455Z  TargetRevision: (int32) 6,
2024-06-11T11:05:15.174175455Z  LastFailedRevision: (int32) 6,
2024-06-11T11:05:15.174175455Z  LastFailedTime: (*v1.Time)(0xc00195d098)(2024-06-11 11:05:15.174033949 +0000 UTC m=+928.984860038),
2024-06-11T11:05:15.174175455Z  LastFailedReason: (string) (len=15) "InstallerFailed",
2024-06-11T11:05:15.174175455Z  LastFailedCount: (int) 1,
2024-06-11T11:05:15.174175455Z  LastFallbackCount: (int) 0,
2024-06-11T11:05:15.174175455Z  LastFailedRevisionErrors: ([]string) (len=1 cap=1) {
2024-06-11T11:05:15.174175455Z   (string) (len=2059) "installer:    1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nW0611 11:02:40.056541       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nW0611 11:02:50.052606       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nW0611 11:03:00.052110       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nW0611 11:03:10.052879       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nW0611 11:03:20.051639       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nW0611 11:03:20.052502       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nF0611 11:03:20.052533       1 cmd.go:106] timed out waiting for the condition\n"
2024-06-11T11:05:15.174175455Z  }
2024-06-11T11:05:15.174175455Z }
2024-06-11T11:05:15.174175455Z  because installer pod failed: installer:    1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:05:15.174175455Z W0611 11:02:40.056541       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:05:15.174175455Z W0611 11:02:50.052606       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:05:15.174175455Z W0611 11:03:00.052110       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:05:15.174175455Z W0611 11:03:10.052879       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:05:15.174175455Z W0611 11:03:20.051639       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:05:15.174175455Z W0611 11:03:20.052502       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:05:15.174175455Z F0611 11:03:20.052533       1 cmd.go:106] timed out waiting for the condition
2024-06-11T11:05:15.175095489Z I0611 11:05:15.174950       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'InstallerPodFailed' installer errors: installer:    1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:05:15.175095489Z W0611 11:02:40.056541       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:05:15.175095489Z W0611 11:02:50.052606       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:05:15.175095489Z W0611 11:03:00.052110       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:05:15.175095489Z W0611 11:03:10.052879       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:05:15.175095489Z W0611 11:03:20.051639       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:05:15.175095489Z W0611 11:03:20.052502       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2024-06-11T11:05:15.175095489Z F0611 11:03:20.052533       1 cmd.go:106] timed out waiting for the condition
2024-06-11T11:05:15.203535136Z I0611 11:05:15.203479       1 prune_controller.go:269] Nothing to prune
2024-06-11T11:05:15.203917850Z I0611 11:05:15.203868       1 status_controller.go:218] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2024-06-11T11:01:30Z","message":"NodeInstallerDegraded: 1 nodes are failing on revision 6:\nNodeInstallerDegraded: installer:    1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:02:40.056541       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:02:50.052606       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:03:00.052110       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:03:10.052879       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:03:20.051639       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:03:20.052502       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: F0611 11:03:20.052533       1 cmd.go:106] timed out waiting for the condition\nNodeInstallerDegraded: \nNodeControllerDegraded: All master nodes are ready","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2024-06-11T10:48:34Z","message":"NodeInstallerProgressing: 1 node is at revision 5; 2 nodes are at revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-06-11T10:52:16Z","message":"StaticPodsAvailable: 3 nodes are active; 1 node is at revision 5; 2 nodes are at revision 6","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2024-06-11T10:48:31Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-06-11T10:48:20Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2024-06-11T11:05:15.229384387Z I0611 11:05:15.228213       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready" to "NodeInstallerDegraded: 1 nodes are failing on revision 6:\nNodeInstallerDegraded: installer:    1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:02:40.056541       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:02:50.052606       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:03:00.052110       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:03:10.052879       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:03:20.051639       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:03:20.052502       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: F0611 11:03:20.052533       1 cmd.go:106] timed out waiting for the condition\nNodeInstallerDegraded: \nNodeControllerDegraded: All master nodes are ready"
2024-06-11T11:05:15.236258040Z I0611 11:05:15.236203       1 prune_controller.go:269] Nothing to prune
2024-06-11T11:05:15.236685956Z I0611 11:05:15.236637       1 status_controller.go:218] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2024-06-11T11:01:30Z","message":"NodeInstallerDegraded: 1 nodes are failing on revision 6:\nNodeInstallerDegraded: installer:    1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:02:40.056541       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:02:50.052606       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:03:00.052110       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:03:10.052879       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:03:20.051639       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:03:20.052502       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: F0611 11:03:20.052533       1 cmd.go:106] timed out waiting for the condition\nNodeInstallerDegraded: \nStaticPodsDegraded: pod/openshift-kube-scheduler-ci-op-9xx71rvq-1e28e-w667k-master-0 container \"kube-scheduler\" is terminated: Error: 29\nStaticPodsDegraded: I0611 11:04:46.868824       1 reflector.go:351] Caches populated for *v1.StatefulSet from k8s.io/client-go@v0.29.0/tools/cache/reflector.go:229\nStaticPodsDegraded: I0611 11:04:46.929608       1 reflector.go:351] Caches populated for *v1.PodDisruptionBudget from k8s.io/client-go@v0.29.0/tools/cache/reflector.go:229\nStaticPodsDegraded: I0611 11:04:46.981425       1 reflector.go:351] Caches populated for *v1.ReplicaSet from k8s.io/client-go@v0.29.0/tools/cache/reflector.go:229\nStaticPodsDegraded: I0611 11:04:47.047155       1 reflector.go:351] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.29.0/tools/cache/reflector.go:229\nStaticPodsDegraded: I0611 11:04:47.075669       1 reflector.go:351] Caches populated for *v1.PersistentVolumeClaim from k8s.io/client-go@v0.29.0/tools/cache/reflector.go:229\nStaticPodsDegraded: I0611 11:04:47.109733       1 reflector.go:351] Caches populated for *v1.PersistentVolume from k8s.io/client-go@v0.29.0/tools/cache/reflector.go:229\nStaticPodsDegraded: I0611 11:04:47.240812       1 reflector.go:351] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.29.0/tools/cache/reflector.go:229\nStaticPodsDegraded: I0611 11:04:47.297462       1 reflector.go:351] Caches populated for *v1.Service from k8s.io/client-go@v0.29.0/tools/cache/reflector.go:229\nStaticPodsDegraded: I0611 11:04:47.330244       1 reflector.go:351] Caches populated for *v1.StorageClass from k8s.io/client-go@v0.29.0/tools/cache/reflector.go:229\nStaticPodsDegraded: I0611 11:04:47.335251       1 reflector.go:351] Caches populated for *v1.Node from k8s.io/client-go@v0.29.0/tools/cache/reflector.go:229\nStaticPodsDegraded: I0611 11:04:47.346175       1 reflector.go:351] Caches populated for *v1.ReplicationController from k8s.io/client-go@v0.29.0/tools/cache/reflector.go:229\nStaticPodsDegraded: I0611 11:04:47.377346       1 reflector.go:351] Caches populated for *v1.CSINode from k8s.io/client-go@v0.29.0/tools/cache/reflector.go:229\nStaticPodsDegraded: I0611 11:05:06.582629       1 leaderelection.go:285] failed to renew lease openshift-kube-scheduler/kube-scheduler: timed out waiting for the condition\nStaticPodsDegraded: E0611 11:05:06.591407       1 server.go:252] \"Leaderelection lost\"\nStaticPodsDegraded: I0611 11:05:06.591496       1 scheduling_queue.go:870] \"Scheduling queue is closed\"\nStaticPodsDegraded: \nNodeControllerDegraded: All master nodes are ready","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2024-06-11T10:48:34Z","message":"NodeInstallerProgressing: 1 node is at revision 5; 2 nodes are at revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-06-11T10:52:16Z","message":"StaticPodsAvailable: 3 nodes are active; 1 node is at revision 5; 2 nodes are at revision 6","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2024-06-11T10:48:31Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-06-11T10:48:20Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2024-06-11T11:05:15.257105908Z I0611 11:05:15.256488       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded message changed from "NodeInstallerDegraded: 1 nodes are failing on revision 6:\nNodeInstallerDegraded: installer:    1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:02:40.056541       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:02:50.052606       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:03:00.052110       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:03:10.052879       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:03:20.051639       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:03:20.052502       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: F0611 11:03:20.052533       1 cmd.go:106] timed out waiting for the condition\nNodeInstallerDegraded: \nNodeControllerDegraded: All master nodes are ready" to "NodeInstallerDegraded: 1 nodes are failing on revision 6:\nNodeInstallerDegraded: installer:    1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:02:40.056541       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:02:50.052606       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:03:00.052110       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:03:10.052879       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:03:20.051639       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:03:20.052502       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: F0611 11:03:20.052533       1 cmd.go:106] timed out waiting for the condition\nNodeInstallerDegraded: \nStaticPodsDegraded: pod/openshift-kube-scheduler-ci-op-9xx71rvq-1e28e-w667k-master-0 container \"kube-scheduler\" is terminated: Error: 29\nStaticPodsDegraded: I0611 11:04:46.868824       1 reflector.go:351] Caches populated for *v1.StatefulSet from k8s.io/client-go@v0.29.0/tools/cache/reflector.go:229\nStaticPodsDegraded: I0611 11:04:46.929608       1 reflector.go:351] Caches populated for *v1.PodDisruptionBudget from k8s.io/client-go@v0.29.0/tools/cache/reflector.go:229\nStaticPodsDegraded: I0611 11:04:46.981425       1 reflector.go:351] Caches populated for *v1.ReplicaSet from k8s.io/client-go@v0.29.0/tools/cache/reflector.go:229\nStaticPodsDegraded: I0611 11:04:47.047155       1 reflector.go:351] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.29.0/tools/cache/reflector.go:229\nStaticPodsDegraded: I0611 11:04:47.075669       1 reflector.go:351] Caches populated for *v1.PersistentVolumeClaim from k8s.io/client-go@v0.29.0/tools/cache/reflector.go:229\nStaticPodsDegraded: I0611 11:04:47.109733       1 reflector.go:351] Caches populated for *v1.PersistentVolume from k8s.io/client-go@v0.29.0/tools/cache/reflector.go:229\nStaticPodsDegraded: I0611 11:04:47.240812       1 reflector.go:351] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.29.0/tools/cache/reflector.go:229\nStaticPodsDegraded: I0611 11:04:47.297462       1 reflector.go:351] Caches populated for *v1.Service from k8s.io/client-go@v0.29.0/tools/cache/reflector.go:229\nStaticPodsDegraded: I0611 11:04:47.330244       1 reflector.go:351] Caches populated for *v1.StorageClass from k8s.io/client-go@v0.29.0/tools/cache/reflector.go:229\nStaticPodsDegraded: I0611 11:04:47.335251       1 reflector.go:351] Caches populated for *v1.Node from k8s.io/client-go@v0.29.0/tools/cache/reflector.go:229\nStaticPodsDegraded: I0611 11:04:47.346175       1 reflector.go:351] Caches populated for *v1.ReplicationController from k8s.io/client-go@v0.29.0/tools/cache/reflector.go:229\nStaticPodsDegraded: I0611 11:04:47.377346       1 reflector.go:351] Caches populated for *v1.CSINode from k8s.io/client-go@v0.29.0/tools/cache/reflector.go:229\nStaticPodsDegraded: I0611 11:05:06.582629       1 leaderelection.go:285] failed to renew lease openshift-kube-scheduler/kube-scheduler: timed out waiting for the condition\nStaticPodsDegraded: E0611 11:05:06.591407       1 server.go:252] \"Leaderelection lost\"\nStaticPodsDegraded: I0611 11:05:06.591496       1 scheduling_queue.go:870] \"Scheduling queue is closed\"\nStaticPodsDegraded: \nNodeControllerDegraded: All master nodes are ready"
2024-06-11T11:05:15.322085500Z I0611 11:05:15.322019       1 reflector.go:351] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:05:17.166342297Z I0611 11:05:17.166237       1 reflector.go:351] Caches populated for *v1.Secret from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:05:18.567458679Z I0611 11:05:18.567369       1 reflector.go:351] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:05:19.368166838Z I0611 11:05:19.368098       1 reflector.go:351] Caches populated for *v1.Secret from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:05:19.785916763Z I0611 11:05:19.785849       1 prune_controller.go:269] Nothing to prune
2024-06-11T11:05:19.786825507Z I0611 11:05:19.786740       1 status_controller.go:218] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2024-06-11T11:01:30Z","message":"NodeInstallerDegraded: 1 nodes are failing on revision 6:\nNodeInstallerDegraded: installer:    1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:02:40.056541       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:02:50.052606       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:03:00.052110       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:03:10.052879       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:03:20.051639       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:03:20.052502       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: F0611 11:03:20.052533       1 cmd.go:106] timed out waiting for the condition\nNodeInstallerDegraded: \nStaticPodsDegraded: pod/openshift-kube-scheduler-ci-op-9xx71rvq-1e28e-w667k-master-0 container \"kube-scheduler\" is waiting: CrashLoopBackOff: back-off 10s restarting failed container=kube-scheduler pod=openshift-kube-scheduler-ci-op-9xx71rvq-1e28e-w667k-master-0_openshift-kube-scheduler(2b9a08053c55e258a76335101c72ecbc)\nNodeControllerDegraded: All master nodes are ready","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2024-06-11T10:48:34Z","message":"NodeInstallerProgressing: 1 node is at revision 5; 2 nodes are at revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-06-11T10:52:16Z","message":"StaticPodsAvailable: 3 nodes are active; 1 node is at revision 5; 2 nodes are at revision 6","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2024-06-11T10:48:31Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-06-11T10:48:20Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2024-06-11T11:05:19.801651625Z I0611 11:05:19.801533       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded message changed from "NodeInstallerDegraded: 1 nodes are failing on revision 6:\nNodeInstallerDegraded: installer:    1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:02:40.056541       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:02:50.052606       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:03:00.052110       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:03:10.052879       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:03:20.051639       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:03:20.052502       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: F0611 11:03:20.052533       1 cmd.go:106] timed out waiting for the condition\nNodeInstallerDegraded: \nStaticPodsDegraded: pod/openshift-kube-scheduler-ci-op-9xx71rvq-1e28e-w667k-master-0 container \"kube-scheduler\" is terminated: Error: 29\nStaticPodsDegraded: I0611 11:04:46.868824       1 reflector.go:351] Caches populated for *v1.StatefulSet from k8s.io/client-go@v0.29.0/tools/cache/reflector.go:229\nStaticPodsDegraded: I0611 11:04:46.929608       1 reflector.go:351] Caches populated for *v1.PodDisruptionBudget from k8s.io/client-go@v0.29.0/tools/cache/reflector.go:229\nStaticPodsDegraded: I0611 11:04:46.981425       1 reflector.go:351] Caches populated for *v1.ReplicaSet from k8s.io/client-go@v0.29.0/tools/cache/reflector.go:229\nStaticPodsDegraded: I0611 11:04:47.047155       1 reflector.go:351] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.29.0/tools/cache/reflector.go:229\nStaticPodsDegraded: I0611 11:04:47.075669       1 reflector.go:351] Caches populated for *v1.PersistentVolumeClaim from k8s.io/client-go@v0.29.0/tools/cache/reflector.go:229\nStaticPodsDegraded: I0611 11:04:47.109733       1 reflector.go:351] Caches populated for *v1.PersistentVolume from k8s.io/client-go@v0.29.0/tools/cache/reflector.go:229\nStaticPodsDegraded: I0611 11:04:47.240812       1 reflector.go:351] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.29.0/tools/cache/reflector.go:229\nStaticPodsDegraded: I0611 11:04:47.297462       1 reflector.go:351] Caches populated for *v1.Service from k8s.io/client-go@v0.29.0/tools/cache/reflector.go:229\nStaticPodsDegraded: I0611 11:04:47.330244       1 reflector.go:351] Caches populated for *v1.StorageClass from k8s.io/client-go@v0.29.0/tools/cache/reflector.go:229\nStaticPodsDegraded: I0611 11:04:47.335251       1 reflector.go:351] Caches populated for *v1.Node from k8s.io/client-go@v0.29.0/tools/cache/reflector.go:229\nStaticPodsDegraded: I0611 11:04:47.346175       1 reflector.go:351] Caches populated for *v1.ReplicationController from k8s.io/client-go@v0.29.0/tools/cache/reflector.go:229\nStaticPodsDegraded: I0611 11:04:47.377346       1 reflector.go:351] Caches populated for *v1.CSINode from k8s.io/client-go@v0.29.0/tools/cache/reflector.go:229\nStaticPodsDegraded: I0611 11:05:06.582629       1 leaderelection.go:285] failed to renew lease openshift-kube-scheduler/kube-scheduler: timed out waiting for the condition\nStaticPodsDegraded: E0611 11:05:06.591407       1 server.go:252] \"Leaderelection lost\"\nStaticPodsDegraded: I0611 11:05:06.591496       1 scheduling_queue.go:870] \"Scheduling queue is closed\"\nStaticPodsDegraded: \nNodeControllerDegraded: All master nodes are ready" to "NodeInstallerDegraded: 1 nodes are failing on revision 6:\nNodeInstallerDegraded: installer:    1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:02:40.056541       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:02:50.052606       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:03:00.052110       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:03:10.052879       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:03:20.051639       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:03:20.052502       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: F0611 11:03:20.052533       1 cmd.go:106] timed out waiting for the condition\nNodeInstallerDegraded: \nStaticPodsDegraded: pod/openshift-kube-scheduler-ci-op-9xx71rvq-1e28e-w667k-master-0 container \"kube-scheduler\" is waiting: CrashLoopBackOff: back-off 10s restarting failed container=kube-scheduler pod=openshift-kube-scheduler-ci-op-9xx71rvq-1e28e-w667k-master-0_openshift-kube-scheduler(2b9a08053c55e258a76335101c72ecbc)\nNodeControllerDegraded: All master nodes are ready"
2024-06-11T11:05:20.187172989Z I0611 11:05:20.187101       1 reflector.go:351] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:05:20.566823570Z I0611 11:05:20.566766       1 reflector.go:351] Caches populated for *v1.Secret from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:05:20.964533425Z I0611 11:05:20.964473       1 request.go:697] Waited for 1.178313547s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ci-op-9xx71rvq-1e28e-w667k-master-0
2024-06-11T11:05:23.166897056Z I0611 11:05:23.166834       1 reflector.go:351] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:05:24.964094752Z I0611 11:05:24.964028       1 reflector.go:351] Caches populated for *v1.RoleBinding from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:05:25.166343569Z I0611 11:05:25.166251       1 reflector.go:351] Caches populated for *v1.ServiceAccount from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:05:26.767868548Z I0611 11:05:26.767805       1 reflector.go:351] Caches populated for *v1.Namespace from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:05:29.966974827Z I0611 11:05:29.966914       1 reflector.go:351] Caches populated for *v1.Secret from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:05:30.407728711Z I0611 11:05:30.407655       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/installer-6-retry-1-ci-op-9xx71rvq-1e28e-w667k-master-0 -n openshift-kube-scheduler because it was missing
2024-06-11T11:05:30.569070863Z I0611 11:05:30.569009       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-0" is in transition to 6, but has not made progress because installer is not finished, but in Pending phase
2024-06-11T11:05:31.114241368Z I0611 11:05:31.114098       1 reflector.go:351] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:05:31.248640687Z I0611 11:05:31.248588       1 reflector.go:351] Caches populated for *v1.Infrastructure from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:05:31.972111719Z I0611 11:05:31.972049       1 reflector.go:351] Caches populated for *v1.Pod from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:05:33.166558791Z I0611 11:05:33.166497       1 reflector.go:351] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:05:33.251341946Z I0611 11:05:33.251246       1 reflector.go:351] Caches populated for *v1.APIServer from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:05:33.975906421Z I0611 11:05:33.975836       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-0" is in transition to 6, but has not made progress because installer is not finished, but in Running phase
2024-06-11T11:05:34.187280387Z I0611 11:05:34.187221       1 prune_controller.go:269] Nothing to prune
2024-06-11T11:05:34.188424432Z I0611 11:05:34.188355       1 status_controller.go:218] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2024-06-11T11:01:30Z","message":"NodeInstallerDegraded: 1 nodes are failing on revision 6:\nNodeInstallerDegraded: installer:    1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:02:40.056541       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:02:50.052606       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:03:00.052110       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:03:10.052879       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:03:20.051639       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:03:20.052502       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: F0611 11:03:20.052533       1 cmd.go:106] timed out waiting for the condition\nNodeInstallerDegraded: \nNodeControllerDegraded: All master nodes are ready","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2024-06-11T10:48:34Z","message":"NodeInstallerProgressing: 1 node is at revision 5; 2 nodes are at revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2024-06-11T10:52:16Z","message":"StaticPodsAvailable: 3 nodes are active; 1 node is at revision 5; 2 nodes are at revision 6","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2024-06-11T10:48:31Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-06-11T10:48:20Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2024-06-11T11:05:34.206756158Z I0611 11:05:34.206676       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded message changed from "NodeInstallerDegraded: 1 nodes are failing on revision 6:\nNodeInstallerDegraded: installer:    1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:02:40.056541       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:02:50.052606       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:03:00.052110       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:03:10.052879       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:03:20.051639       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:03:20.052502       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: F0611 11:03:20.052533       1 cmd.go:106] timed out waiting for the condition\nNodeInstallerDegraded: \nStaticPodsDegraded: pod/openshift-kube-scheduler-ci-op-9xx71rvq-1e28e-w667k-master-0 container \"kube-scheduler\" is waiting: CrashLoopBackOff: back-off 10s restarting failed container=kube-scheduler pod=openshift-kube-scheduler-ci-op-9xx71rvq-1e28e-w667k-master-0_openshift-kube-scheduler(2b9a08053c55e258a76335101c72ecbc)\nNodeControllerDegraded: All master nodes are ready" to "NodeInstallerDegraded: 1 nodes are failing on revision 6:\nNodeInstallerDegraded: installer:    1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:02:40.056541       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:02:50.052606       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:03:00.052110       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:03:10.052879       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:03:20.051639       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:03:20.052502       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: F0611 11:03:20.052533       1 cmd.go:106] timed out waiting for the condition\nNodeInstallerDegraded: \nNodeControllerDegraded: All master nodes are ready"
2024-06-11T11:05:35.364806397Z I0611 11:05:35.364730       1 request.go:697] Waited for 1.176696877s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler
2024-06-11T11:05:35.770910972Z I0611 11:05:35.770853       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-0" is in transition to 6, but has not made progress because installer is not finished, but in Running phase
2024-06-11T11:05:36.564828364Z I0611 11:05:36.564623       1 request.go:697] Waited for 1.179885521s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler
2024-06-11T11:05:37.960802734Z I0611 11:05:37.960728       1 reflector.go:351] Caches populated for *v1.RoleBinding from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:05:37.968271185Z I0611 11:05:37.968222       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-0" is in transition to 6, but has not made progress because installer is not finished, but in Running phase
2024-06-11T11:05:38.564008767Z I0611 11:05:38.563959       1 request.go:697] Waited for 1.178046634s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-config-managed/secrets?resourceVersion=23636
2024-06-11T11:05:38.566498884Z I0611 11:05:38.566441       1 reflector.go:351] Caches populated for *v1.Secret from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:05:40.103821592Z I0611 11:05:40.103752       1 reflector.go:351] Caches populated for *v1.Scheduler from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:05:42.965895603Z I0611 11:05:42.965838       1 reflector.go:351] Caches populated for *v1.Service from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:05:51.820717716Z I0611 11:05:51.820632       1 reflector.go:351] Caches populated for *v1.FeatureGate from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:05:52.657436964Z I0611 11:05:52.657363       1 reflector.go:351] Caches populated for *v1.ConfigMap from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:06:03.483550909Z I0611 11:06:03.483479       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-0" is in transition to 6, but has not made progress because installer is not finished, but in Running phase
2024-06-11T11:06:04.453479493Z I0611 11:06:04.453397       1 reflector.go:351] Caches populated for *v1.Node from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:06:06.873008608Z I0611 11:06:06.872955       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-0" is in transition to 6, but has not made progress because waiting for static pod of revision 6, found 5
2024-06-11T11:06:08.675354426Z I0611 11:06:08.673024       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-0" is in transition to 6, but has not made progress because waiting for static pod of revision 6, found 5
2024-06-11T11:06:14.253724128Z I0611 11:06:14.253663       1 reflector.go:351] Caches populated for *v1.Role from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:06:15.384901007Z I0611 11:06:15.376889       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-0" is in transition to 6, but has not made progress because waiting for static pod of revision 6, found 5
2024-06-11T11:06:18.307359764Z I0611 11:06:18.307275       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-0" is in transition to 6, but has not made progress because static pod is pending
2024-06-11T11:06:20.302741934Z I0611 11:06:20.302667       1 request.go:697] Waited for 1.163273269s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/configmaps/config
2024-06-11T11:06:21.108394096Z I0611 11:06:21.108333       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-0" is in transition to 6, but has not made progress because static pod is pending
2024-06-11T11:06:21.303043926Z I0611 11:06:21.302987       1 request.go:697] Waited for 1.180368354s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler
2024-06-11T11:06:22.503014980Z I0611 11:06:22.502931       1 request.go:697] Waited for 1.195020026s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/openshift-kube-scheduler-sa
2024-06-11T11:06:26.136828695Z I0611 11:06:26.136765       1 installer_controller.go:512] "ci-op-9xx71rvq-1e28e-w667k-master-0" is in transition to 6, but has not made progress because static pod is pending
2024-06-11T11:07:09.297605579Z I0611 11:07:09.297547       1 request.go:697] Waited for 1.088895657s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ci-op-9xx71rvq-1e28e-w667k-master-0
2024-06-11T11:07:10.297725998Z I0611 11:07:10.297669       1 request.go:697] Waited for 1.39336405s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa
2024-06-11T11:07:11.706705976Z I0611 11:07:11.706632       1 installer_controller.go:500] "ci-op-9xx71rvq-1e28e-w667k-master-0" moving to (v1.NodeStatus) {
2024-06-11T11:07:11.706705976Z  NodeName: (string) (len=35) "ci-op-9xx71rvq-1e28e-w667k-master-0",
2024-06-11T11:07:11.706705976Z  CurrentRevision: (int32) 6,
2024-06-11T11:07:11.706705976Z  TargetRevision: (int32) 0,
2024-06-11T11:07:11.706705976Z  LastFailedRevision: (int32) 6,
2024-06-11T11:07:11.706705976Z  LastFailedTime: (*v1.Time)(0xc0018a9680)(2024-06-11 11:05:15 +0000 UTC),
2024-06-11T11:07:11.706705976Z  LastFailedReason: (string) (len=15) "InstallerFailed",
2024-06-11T11:07:11.706705976Z  LastFailedCount: (int) 1,
2024-06-11T11:07:11.706705976Z  LastFallbackCount: (int) 0,
2024-06-11T11:07:11.706705976Z  LastFailedRevisionErrors: ([]string) (len=1 cap=1) {
2024-06-11T11:07:11.706705976Z   (string) (len=2059) "installer:    1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nW0611 11:02:40.056541       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nW0611 11:02:50.052606       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nW0611 11:03:00.052110       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nW0611 11:03:10.052879       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nW0611 11:03:20.051639       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nW0611 11:03:20.052502       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nF0611 11:03:20.052533       1 cmd.go:106] timed out waiting for the condition\n"
2024-06-11T11:07:11.706705976Z  }
2024-06-11T11:07:11.706705976Z }
2024-06-11T11:07:11.706705976Z  because static pod is ready
2024-06-11T11:07:11.726910318Z I0611 11:07:11.726829       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeCurrentRevisionChanged' Updated node "ci-op-9xx71rvq-1e28e-w667k-master-0" from revision 5 to 6 because static pod is ready
2024-06-11T11:07:11.728130675Z I0611 11:07:11.728061       1 status_controller.go:218] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2024-06-11T11:01:30Z","message":"NodeControllerDegraded: All master nodes are ready","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2024-06-11T11:07:11Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2024-06-11T10:52:16Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2024-06-11T10:48:31Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2024-06-11T10:48:20Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2024-06-11T11:07:11.747516179Z I0611 11:07:11.747428       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded message changed from "NodeInstallerDegraded: 1 nodes are failing on revision 6:\nNodeInstallerDegraded: installer:    1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:02:40.056541       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:02:50.052606       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:03:00.052110       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:03:10.052879       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:03:20.051639       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: W0611 11:03:20.052502       1 cmd.go:467] Error getting installer pods on current node ci-op-9xx71rvq-1e28e-w667k-master-0: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller\": dial tcp 172.30.0.1:443: connect: connection refused\nNodeInstallerDegraded: F0611 11:03:20.052533       1 cmd.go:106] timed out waiting for the condition\nNodeInstallerDegraded: \nNodeControllerDegraded: All master nodes are ready" to "NodeControllerDegraded: All master nodes are ready",Progressing changed from True to False ("NodeInstallerProgressing: 3 nodes are at revision 6"),Available message changed from "StaticPodsAvailable: 3 nodes are active; 1 node is at revision 5; 2 nodes are at revision 6" to "StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6"
2024-06-11T11:07:12.897341277Z I0611 11:07:12.897242       1 request.go:697] Waited for 1.169482114s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa
2024-06-11T11:07:13.897693007Z I0611 11:07:13.897633       1 request.go:697] Waited for 1.395771362s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ci-op-9xx71rvq-1e28e-w667k-master-1
2024-06-11T11:07:14.332394170Z I0611 11:07:14.332277       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/revision-pruner-6-ci-op-9xx71rvq-1e28e-w667k-master-0 -n openshift-kube-scheduler because it was missing
2024-06-11T11:07:15.100657582Z I0611 11:07:15.096793       1 request.go:697] Waited for 1.194084261s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T11:07:16.296688387Z I0611 11:07:16.296612       1 request.go:697] Waited for 1.292419399s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler
2024-06-11T11:07:17.111835712Z I0611 11:07:17.111770       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/revision-pruner-6-ci-op-9xx71rvq-1e28e-w667k-master-1 -n openshift-kube-scheduler because it was missing
2024-06-11T11:07:17.296818210Z I0611 11:07:17.296730       1 request.go:697] Waited for 1.384893675s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/secrets/localhost-recovery-client-token
2024-06-11T11:07:18.297149542Z I0611 11:07:18.297076       1 request.go:697] Waited for 1.387562593s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller
2024-06-11T11:07:19.297181461Z I0611 11:07:19.297125       1 request.go:697] Waited for 1.351684303s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T11:07:19.916386103Z I0611 11:07:19.915386       1 event.go:364] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ceacc5dd-2450-4cb7-adef-2d3844537cbb", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/revision-pruner-6-ci-op-9xx71rvq-1e28e-w667k-master-2 -n openshift-kube-scheduler because it was missing
2024-06-11T11:07:20.297632799Z I0611 11:07:20.297580       1 request.go:697] Waited for 1.393269147s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T11:07:21.496737340Z I0611 11:07:21.496679       1 request.go:697] Waited for 1.175166681s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ci-op-9xx71rvq-1e28e-w667k-master-2
2024-06-11T11:07:22.496770059Z I0611 11:07:22.496705       1 request.go:697] Waited for 1.187254816s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/configmaps/config
2024-06-11T11:07:23.497671464Z I0611 11:07:23.497592       1 request.go:697] Waited for 1.129886321s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler
2024-06-11T11:07:24.697854944Z I0611 11:07:24.697778       1 request.go:697] Waited for 1.191921204s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/openshift-kube-scheduler-sa
2024-06-11T11:07:25.896782367Z I0611 11:07:25.896712       1 request.go:697] Waited for 1.179004117s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/localhost-recovery-client
2024-06-11T11:12:16.587736091Z I0611 11:12:16.587669       1 reflector.go:351] Caches populated for *v1.ClusterVersion from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:12:16.661830450Z I0611 11:12:16.661757       1 reflector.go:351] Caches populated for *v1.ClusterOperator from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:12:17.192850703Z I0611 11:12:17.192788       1 reflector.go:351] Caches populated for *v1.APIServer from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:12:17.334912404Z I0611 11:12:17.334860       1 reflector.go:351] Caches populated for *v1.FeatureGate from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:12:18.409375663Z I0611 11:12:18.408995       1 reflector.go:351] Caches populated for *v1.Scheduler from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:12:18.839161586Z I0611 11:12:18.839099       1 reflector.go:351] Caches populated for operator.openshift.io/v1, Resource=kubeschedulers from k8s.io/client-go@v0.29.1/tools/cache/reflector.go:229
2024-06-11T11:12:19.936557658Z I0611 11:12:19.936496       1 request.go:697] Waited for 1.095703479s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/revision-pruner-6-ci-op-9xx71rvq-1e28e-w667k-master-0
