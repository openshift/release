---
apiVersion: monitoring.coreos.com/v1
kind: Prometheus
metadata:
  annotations:
    operator.prometheus.io/controller-id: openshift-monitoring/prometheus-operator
  creationTimestamp: "2024-06-11T11:11:37Z"
  generation: 1
  labels:
    app.kubernetes.io/component: prometheus
    app.kubernetes.io/instance: k8s
    app.kubernetes.io/managed-by: cluster-monitoring-operator
    app.kubernetes.io/name: prometheus
    app.kubernetes.io/part-of: openshift-monitoring
    app.kubernetes.io/version: 2.52.0
  managedFields:
  - apiVersion: monitoring.coreos.com/v1
    fieldsType: FieldsV1
    fieldsV1:
      f:status:
        f:availableReplicas: {}
        f:conditions:
          k:{"type":"Available"}:
            .: {}
            f:lastTransitionTime: {}
            f:message: {}
            f:observedGeneration: {}
            f:reason: {}
            f:status: {}
            f:type: {}
          k:{"type":"Reconciled"}:
            .: {}
            f:lastTransitionTime: {}
            f:message: {}
            f:observedGeneration: {}
            f:reason: {}
            f:status: {}
            f:type: {}
        f:paused: {}
        f:replicas: {}
        f:selector: {}
        f:shardStatuses:
          k:{"shardID":"0"}:
            .: {}
            f:availableReplicas: {}
            f:replicas: {}
            f:shardID: {}
            f:unavailableReplicas: {}
            f:updatedReplicas: {}
        f:shards: {}
        f:unavailableReplicas: {}
        f:updatedReplicas: {}
    manager: PrometheusOperator
    operation: Apply
    subresource: status
    time: "2024-06-11T11:29:19Z"
  - apiVersion: monitoring.coreos.com/v1
    fieldsType: FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .: {}
          f:operator.prometheus.io/controller-id: {}
        f:labels:
          .: {}
          f:app.kubernetes.io/component: {}
          f:app.kubernetes.io/instance: {}
          f:app.kubernetes.io/managed-by: {}
          f:app.kubernetes.io/name: {}
          f:app.kubernetes.io/part-of: {}
          f:app.kubernetes.io/version: {}
      f:spec:
        .: {}
        f:additionalAlertRelabelConfigs: {}
        f:additionalArgs: {}
        f:affinity:
          .: {}
          f:podAntiAffinity:
            .: {}
            f:requiredDuringSchedulingIgnoredDuringExecution: {}
        f:alerting:
          .: {}
          f:alertmanagers: {}
        f:arbitraryFSAccessThroughSMs: {}
        f:configMaps: {}
        f:containers: {}
        f:evaluationInterval: {}
        f:externalUrl: {}
        f:image: {}
        f:listenLocal: {}
        f:maximumStartupDurationSeconds: {}
        f:nodeSelector:
          .: {}
          f:kubernetes.io/os: {}
        f:podMetadata:
          .: {}
          f:annotations:
            .: {}
            f:openshift.io/required-scc: {}
            f:target.workload.openshift.io/management: {}
          f:labels:
            .: {}
            f:app.kubernetes.io/component: {}
            f:app.kubernetes.io/instance: {}
            f:app.kubernetes.io/name: {}
            f:app.kubernetes.io/part-of: {}
            f:app.kubernetes.io/version: {}
        f:podMonitorNamespaceSelector: {}
        f:podMonitorSelector: {}
        f:portName: {}
        f:priorityClassName: {}
        f:probeNamespaceSelector: {}
        f:probeSelector: {}
        f:replicas: {}
        f:resources:
          .: {}
          f:requests:
            .: {}
            f:cpu: {}
            f:memory: {}
        f:retention: {}
        f:ruleNamespaceSelector: {}
        f:ruleSelector: {}
        f:rules:
          .: {}
          f:alert: {}
        f:scrapeInterval: {}
        f:secrets: {}
        f:securityContext:
          .: {}
          f:fsGroup: {}
          f:runAsNonRoot: {}
          f:runAsUser: {}
        f:serviceAccountName: {}
        f:serviceMonitorNamespaceSelector: {}
        f:serviceMonitorSelector: {}
        f:storage:
          .: {}
          f:volumeClaimTemplate:
            .: {}
            f:metadata:
              .: {}
              f:annotations:
                .: {}
                f:openshift.io/cluster-monitoring-drop-pvc: {}
              f:name: {}
            f:spec:
              .: {}
              f:resources:
                .: {}
                f:requests:
                  .: {}
                  f:storage: {}
            f:status: {}
        f:thanos:
          .: {}
          f:blockSize: {}
          f:image: {}
          f:resources:
            .: {}
            f:requests:
              .: {}
              f:cpu: {}
              f:memory: {}
          f:version: {}
        f:tsdb: {}
        f:version: {}
        f:volumes: {}
        f:web:
          .: {}
          f:httpConfig:
            .: {}
            f:headers:
              .: {}
              f:contentSecurityPolicy: {}
    manager: operator
    operation: Update
    time: "2024-06-11T11:11:37Z"
  name: k8s
  namespace: openshift-monitoring
  resourceVersion: "38309"
  uid: 6e04e4af-b7c4-49f6-be98-51ce6da6a3f6
spec:
  additionalAlertRelabelConfigs:
    key: config.yaml
    name: alert-relabel-configs
    optional: true
  additionalArgs:
  - name: scrape.timestamp-tolerance
    value: 15ms
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchLabels:
            app.kubernetes.io/component: prometheus
            app.kubernetes.io/instance: k8s
            app.kubernetes.io/name: prometheus
            app.kubernetes.io/part-of: openshift-monitoring
        namespaces:
        - openshift-monitoring
        topologyKey: kubernetes.io/hostname
  alerting:
    alertmanagers:
    - apiVersion: v2
      bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
      name: alertmanager-main
      namespace: openshift-monitoring
      port: web
      scheme: https
      tlsConfig:
        ca: {}
        caFile: /etc/prometheus/configmaps/serving-certs-ca-bundle/service-ca.crt
        cert: {}
        serverName: alertmanager-main.openshift-monitoring.svc
  arbitraryFSAccessThroughSMs: {}
  configMaps:
  - serving-certs-ca-bundle
  - kubelet-serving-ca-bundle
  - metrics-client-ca
  containers:
  - args:
    - --secure-listen-address=0.0.0.0:9091
    - --upstream=http://127.0.0.1:9090
    - --config-file=/etc/kube-rbac-proxy/config.yaml
    - --tls-cert-file=/etc/tls/private/tls.crt
    - --tls-private-key-file=/etc/tls/private/tls.key
    - --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256
    - --ignore-paths=/-/healthy,/-/ready
    - --tls-min-version=VersionTLS12
    image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:592ec166fee1aabf6b7dfd82cdd541e5cb608f99c7cc41c9ad3841dd1b854776
    name: kube-rbac-proxy-web
    ports:
    - containerPort: 9091
      name: web
      protocol: TCP
    resources:
      requests:
        cpu: 1m
        memory: 15Mi
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
    terminationMessagePolicy: FallbackToLogsOnError
    volumeMounts:
    - mountPath: /etc/tls/private
      name: secret-prometheus-k8s-tls
    - mountPath: /etc/kube-rbac-proxy
      name: secret-prometheus-k8s-kube-rbac-proxy-web
  - args:
    - --secure-listen-address=0.0.0.0:9092
    - --upstream=http://127.0.0.1:9090
    - --allow-paths=/metrics,/federate
    - --config-file=/etc/kube-rbac-proxy/config.yaml
    - --tls-cert-file=/etc/tls/private/tls.crt
    - --tls-private-key-file=/etc/tls/private/tls.key
    - --client-ca-file=/etc/tls/client/client-ca.crt
    - --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256
    - --tls-min-version=VersionTLS12
    image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:592ec166fee1aabf6b7dfd82cdd541e5cb608f99c7cc41c9ad3841dd1b854776
    name: kube-rbac-proxy
    ports:
    - containerPort: 9092
      name: metrics
      protocol: TCP
    resources:
      requests:
        cpu: 1m
        memory: 15Mi
    terminationMessagePolicy: FallbackToLogsOnError
    volumeMounts:
    - mountPath: /etc/tls/private
      name: secret-prometheus-k8s-tls
    - mountPath: /etc/tls/client
      name: configmap-metrics-client-ca
      readOnly: true
    - mountPath: /etc/kube-rbac-proxy
      name: secret-kube-rbac-proxy
  - args:
    - --secure-listen-address=[$(POD_IP)]:10903
    - --upstream=http://127.0.0.1:10902
    - --tls-cert-file=/etc/tls/private/tls.crt
    - --tls-private-key-file=/etc/tls/private/tls.key
    - --client-ca-file=/etc/tls/client/client-ca.crt
    - --config-file=/etc/kube-rbac-proxy/config.yaml
    - --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256
    - --allow-paths=/metrics
    - --logtostderr=true
    - --tls-min-version=VersionTLS12
    env:
    - name: POD_IP
      valueFrom:
        fieldRef:
          fieldPath: status.podIP
    image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:592ec166fee1aabf6b7dfd82cdd541e5cb608f99c7cc41c9ad3841dd1b854776
    name: kube-rbac-proxy-thanos
    ports:
    - containerPort: 10903
      name: thanos-proxy
      protocol: TCP
    resources:
      requests:
        cpu: 1m
        memory: 10Mi
    terminationMessagePolicy: FallbackToLogsOnError
    volumeMounts:
    - mountPath: /etc/tls/private
      name: secret-prometheus-k8s-thanos-sidecar-tls
      readOnly: true
    - mountPath: /etc/kube-rbac-proxy
      name: secret-kube-rbac-proxy
      readOnly: true
    - mountPath: /etc/tls/client
      name: configmap-metrics-client-ca
      readOnly: true
  - args:
    - sidecar
    - --prometheus.url=http://localhost:9090/
    - --tsdb.path=/prometheus
    - --http-address=127.0.0.1:10902
    - --grpc-server-tls-cert=/etc/tls/grpc/server.crt
    - --grpc-server-tls-key=/etc/tls/grpc/server.key
    - --grpc-server-tls-client-ca=/etc/tls/grpc/ca.crt
    name: thanos-sidecar
    resources:
      requests:
        cpu: 1m
        memory: 25Mi
    terminationMessagePolicy: FallbackToLogsOnError
    volumeMounts:
    - mountPath: /etc/tls/grpc
      name: secret-grpc-tls
  - name: prometheus
    resources: {}
    terminationMessagePolicy: FallbackToLogsOnError
    volumeMounts:
    - mountPath: /etc/pki/ca-trust/extracted/pem/
      name: prometheus-trusted-ca-bundle
  evaluationInterval: 30s
  externalUrl: https://console-openshift-console.apps.ci-op-9xx71rvq-1e28e.qe.azure.devcluster.openshift.com/monitoring
  image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8e3ff0220509aee666082ff1316ade676d06a1f7167b2feba51d2ae64e7bb8e7
  listenLocal: true
  maximumStartupDurationSeconds: 3600
  nodeSelector:
    kubernetes.io/os: linux
  podMetadata:
    annotations:
      openshift.io/required-scc: nonroot
      target.workload.openshift.io/management: '{"effect": "PreferredDuringScheduling"}'
    labels:
      app.kubernetes.io/component: prometheus
      app.kubernetes.io/instance: k8s
      app.kubernetes.io/name: prometheus
      app.kubernetes.io/part-of: openshift-monitoring
      app.kubernetes.io/version: 2.52.0
  podMonitorNamespaceSelector:
    matchLabels:
      openshift.io/cluster-monitoring: "true"
  podMonitorSelector:
    matchExpressions:
    - key: monitoring.openshift.io/collection-profile
      operator: NotIn
      values:
      - minimal
  portName: web
  priorityClassName: system-cluster-critical
  probeNamespaceSelector:
    matchLabels:
      openshift.io/cluster-monitoring: "true"
  probeSelector:
    matchExpressions:
    - key: monitoring.openshift.io/collection-profile
      operator: NotIn
      values:
      - minimal
  replicas: 2
  resources:
    requests:
      cpu: 70m
      memory: 1Gi
  retention: 15d
  ruleNamespaceSelector:
    matchLabels:
      openshift.io/cluster-monitoring: "true"
  ruleSelector: {}
  rules:
    alert: {}
  scrapeInterval: 30s
  secrets:
  - prometheus-k8s-tls
  - prometheus-k8s-thanos-sidecar-tls
  - kube-rbac-proxy
  - prometheus-k8s-kube-rbac-proxy-web
  - metrics-client-certs
  securityContext:
    fsGroup: 65534
    runAsNonRoot: true
    runAsUser: 65534
  serviceAccountName: prometheus-k8s
  serviceMonitorNamespaceSelector:
    matchLabels:
      openshift.io/cluster-monitoring: "true"
  serviceMonitorSelector:
    matchExpressions:
    - key: monitoring.openshift.io/collection-profile
      operator: NotIn
      values:
      - minimal
  storage:
    volumeClaimTemplate:
      metadata:
        annotations:
          openshift.io/cluster-monitoring-drop-pvc: "yes"
        name: prometheus-data
      spec:
        resources:
          requests:
            storage: 20Gi
      status: {}
  thanos:
    blockSize: 2h
    image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f43437c43a1932bd4b62e7561a92cc5d85bf776a0349df7965451bc0482d4483
    resources:
      requests:
        cpu: 1m
        memory: 100Mi
    version: 0.35.0
  tsdb: {}
  version: 2.52.0
  volumes:
  - configMap:
      items:
      - key: ca-bundle.crt
        path: tls-ca-bundle.pem
      name: prometheus-trusted-ca-bundle
    name: prometheus-trusted-ca-bundle
  - name: secret-grpc-tls
    secret:
      secretName: prometheus-k8s-grpc-tls-9838dml7sm2ad
  web:
    httpConfig:
      headers:
        contentSecurityPolicy: frame-ancestors 'none'
status:
  availableReplicas: 2
  conditions:
  - lastTransitionTime: "2024-06-11T11:29:19Z"
    message: ""
    observedGeneration: 1
    reason: ""
    status: "True"
    type: Available
  - lastTransitionTime: "2024-06-11T11:29:19Z"
    message: ""
    observedGeneration: 1
    reason: ""
    status: "True"
    type: Reconciled
  paused: false
  replicas: 2
  selector: app.kubernetes.io/instance=k8s,app.kubernetes.io/managed-by=prometheus-operator,app.kubernetes.io/name=prometheus,operator.prometheus.io/name=k8s,prometheus=k8s
  shardStatuses:
  - availableReplicas: 2
    replicas: 2
    shardID: "0"
    unavailableReplicas: 0
    updatedReplicas: 2
  shards: 1
  unavailableReplicas: 0
  updatedReplicas: 2
