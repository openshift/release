---
apiVersion: v1
items:
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/default-container: cluster-cloud-controller-manager
    creationTimestamp: "2024-06-11T10:44:22Z"
    generateName: cluster-cloud-controller-manager-operator-6cf975b6c8-
    labels:
      k8s-app: cloud-manager-operator
      pod-template-hash: 6cf975b6c8
    managedFields:
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:kubectl.kubernetes.io/default-container: {}
            f:target.workload.openshift.io/management: {}
          f:generateName: {}
          f:labels:
            .: {}
            f:k8s-app: {}
            f:pod-template-hash: {}
          f:ownerReferences:
            .: {}
            k:{"uid":"4330651b-4c80-46f3-836a-0619666b51f4"}: {}
        f:spec:
          f:containers:
            k:{"name":"cluster-cloud-controller-manager"}:
              .: {}
              f:command: {}
              f:env:
                .: {}
                k:{"name":"RELEASE_VERSION"}:
                  .: {}
                  f:name: {}
                  f:value: {}
              f:image: {}
              f:imagePullPolicy: {}
              f:name: {}
              f:ports:
                .: {}
                k:{"containerPort":9257,"protocol":"TCP"}:
                  .: {}
                  f:containerPort: {}
                  f:hostPort: {}
                  f:name: {}
                  f:protocol: {}
                k:{"containerPort":9259,"protocol":"TCP"}:
                  .: {}
                  f:containerPort: {}
                  f:hostPort: {}
                  f:name: {}
                  f:protocol: {}
              f:resources:
                .: {}
                f:requests:
                  .: {}
                  f:cpu: {}
                  f:memory: {}
              f:terminationMessagePath: {}
              f:terminationMessagePolicy: {}
              f:volumeMounts:
                .: {}
                k:{"mountPath":"/etc/cloud-controller-manager-config/"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/etc/kubernetes"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
            k:{"name":"config-sync-controllers"}:
              .: {}
              f:command: {}
              f:env:
                .: {}
                k:{"name":"RELEASE_VERSION"}:
                  .: {}
                  f:name: {}
                  f:value: {}
              f:image: {}
              f:imagePullPolicy: {}
              f:name: {}
              f:ports:
                .: {}
                k:{"containerPort":9260,"protocol":"TCP"}:
                  .: {}
                  f:containerPort: {}
                  f:hostPort: {}
                  f:name: {}
                  f:protocol: {}
              f:resources:
                .: {}
                f:requests:
                  .: {}
                  f:cpu: {}
                  f:memory: {}
              f:terminationMessagePath: {}
              f:terminationMessagePolicy: {}
              f:volumeMounts:
                .: {}
                k:{"mountPath":"/etc/kubernetes"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                  f:readOnly: {}
            k:{"name":"kube-rbac-proxy"}:
              .: {}
              f:args: {}
              f:image: {}
              f:imagePullPolicy: {}
              f:name: {}
              f:ports:
                .: {}
                k:{"containerPort":9258,"protocol":"TCP"}:
                  .: {}
                  f:containerPort: {}
                  f:hostPort: {}
                  f:name: {}
                  f:protocol: {}
              f:resources:
                .: {}
                f:requests:
                  .: {}
                  f:cpu: {}
                  f:memory: {}
              f:terminationMessagePath: {}
              f:terminationMessagePolicy: {}
              f:volumeMounts:
                .: {}
                k:{"mountPath":"/etc/kube-rbac-proxy"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
                k:{"mountPath":"/etc/tls/private"}:
                  .: {}
                  f:mountPath: {}
                  f:name: {}
          f:dnsPolicy: {}
          f:enableServiceLinks: {}
          f:hostNetwork: {}
          f:nodeSelector: {}
          f:priorityClassName: {}
          f:restartPolicy: {}
          f:schedulerName: {}
          f:securityContext: {}
          f:serviceAccount: {}
          f:serviceAccountName: {}
          f:terminationGracePeriodSeconds: {}
          f:tolerations: {}
          f:volumes:
            .: {}
            k:{"name":"auth-proxy-config"}:
              .: {}
              f:configMap:
                .: {}
                f:defaultMode: {}
                f:name: {}
              f:name: {}
            k:{"name":"cloud-controller-manager-operator-tls"}:
              .: {}
              f:name: {}
              f:secret:
                .: {}
                f:defaultMode: {}
                f:optional: {}
                f:secretName: {}
            k:{"name":"host-etc-kube"}:
              .: {}
              f:hostPath:
                .: {}
                f:path: {}
                f:type: {}
              f:name: {}
            k:{"name":"images"}:
              .: {}
              f:configMap:
                .: {}
                f:defaultMode: {}
                f:name: {}
              f:name: {}
      manager: kube-controller-manager
      operation: Update
      time: "2024-06-11T10:44:22Z"
    - apiVersion: v1
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          f:conditions:
            k:{"type":"ContainersReady"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
            k:{"type":"Initialized"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
            k:{"type":"PodReadyToStartContainers"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
            k:{"type":"Ready"}:
              .: {}
              f:lastProbeTime: {}
              f:lastTransitionTime: {}
              f:status: {}
              f:type: {}
          f:containerStatuses: {}
          f:hostIP: {}
          f:hostIPs: {}
          f:phase: {}
          f:podIP: {}
          f:podIPs:
            .: {}
            k:{"ip":"10.0.0.6"}:
              .: {}
              f:ip: {}
          f:startTime: {}
      manager: kubelet
      operation: Update
      subresource: status
      time: "2024-06-11T10:56:19Z"
    name: cluster-cloud-controller-manager-operator-6cf975b6c8-zdsgh
    namespace: openshift-cloud-controller-manager-operator
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: cluster-cloud-controller-manager-operator-6cf975b6c8
      uid: 4330651b-4c80-46f3-836a-0619666b51f4
    resourceVersion: "18418"
    uid: f8efb5a9-7af4-4683-b2a4-a4caa7e8ae02
  spec:
    containers:
    - command:
      - /bin/bash
      - -c
      - |
        #!/bin/bash
        set -o allexport
        if [[ -f /etc/kubernetes/apiserver-url.env ]]; then
          source /etc/kubernetes/apiserver-url.env
        else
          URL_ONLY_KUBECONFIG=/etc/kubernetes/kubeconfig
        fi
        exec /cluster-controller-manager-operator \
        --leader-elect=true \
        --leader-elect-lease-duration=137s \
        --leader-elect-renew-deadline=107s \
        --leader-elect-retry-period=26s \
        --leader-elect-resource-namespace=openshift-cloud-controller-manager-operator \
        "--images-json=/etc/cloud-controller-manager-config/images.json" \
        --metrics-bind-address=127.0.0.1:9257 \
        --health-addr=127.0.0.1:9259
      env:
      - name: RELEASE_VERSION
        value: 4.16.0-0.nightly-2024-06-10-211334
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1bd9232bd59867a84e0c1ce986e4d77e8077d3d01eb3d0b9977ecdcad6a82d38
      imagePullPolicy: IfNotPresent
      name: cluster-cloud-controller-manager
      ports:
      - containerPort: 9257
        hostPort: 9257
        name: metrics
        protocol: TCP
      - containerPort: 9259
        hostPort: 9259
        name: healthz
        protocol: TCP
      resources:
        requests:
          cpu: 10m
          memory: 50Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/cloud-controller-manager-config/
        name: images
      - mountPath: /etc/kubernetes
        name: host-etc-kube
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-7tbxp
        readOnly: true
    - command:
      - /bin/bash
      - -c
      - |
        #!/bin/bash
        set -o allexport
        if [[ -f /etc/kubernetes/apiserver-url.env ]]; then
          source /etc/kubernetes/apiserver-url.env
        else
          URL_ONLY_KUBECONFIG=/etc/kubernetes/kubeconfig
        fi
        exec /config-sync-controllers \
        --leader-elect=true \
        --leader-elect-lease-duration=137s \
        --leader-elect-renew-deadline=107s \
        --leader-elect-retry-period=26s \
        --leader-elect-resource-namespace=openshift-cloud-controller-manager-operator \
        --health-addr=127.0.0.1:9260
      env:
      - name: RELEASE_VERSION
        value: 4.16.0-0.nightly-2024-06-10-211334
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1bd9232bd59867a84e0c1ce986e4d77e8077d3d01eb3d0b9977ecdcad6a82d38
      imagePullPolicy: IfNotPresent
      name: config-sync-controllers
      ports:
      - containerPort: 9260
        hostPort: 9260
        name: healthz
        protocol: TCP
      resources:
        requests:
          cpu: 10m
          memory: 25Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/kubernetes
        name: host-etc-kube
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-7tbxp
        readOnly: true
    - args:
      - --secure-listen-address=0.0.0.0:9258
      - --upstream=http://127.0.0.1:9257/
      - --tls-cert-file=/etc/tls/private/tls.crt
      - --tls-private-key-file=/etc/tls/private/tls.key
      - --config-file=/etc/kube-rbac-proxy/config-file.yaml
      - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305
      - --logtostderr=true
      - --v=3
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:592ec166fee1aabf6b7dfd82cdd541e5cb608f99c7cc41c9ad3841dd1b854776
      imagePullPolicy: IfNotPresent
      name: kube-rbac-proxy
      ports:
      - containerPort: 9258
        hostPort: 9258
        name: https
        protocol: TCP
      resources:
        requests:
          cpu: 10m
          memory: 20Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/kube-rbac-proxy
        name: auth-proxy-config
      - mountPath: /etc/tls/private
        name: cloud-controller-manager-operator-tls
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-7tbxp
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: ci-op-9xx71rvq-1e28e-w667k-master-1
    nodeSelector:
      node-role.kubernetes.io/master: ""
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: cluster-cloud-controller-manager
    serviceAccountName: cluster-cloud-controller-manager
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 120
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 120
    - effect: NoSchedule
      key: node.cloudprovider.kubernetes.io/uninitialized
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    volumes:
    - configMap:
        defaultMode: 420
        name: cloud-controller-manager-images
      name: images
    - hostPath:
        path: /etc/kubernetes
        type: Directory
      name: host-etc-kube
    - configMap:
        defaultMode: 420
        name: kube-rbac-proxy
      name: auth-proxy-config
    - name: cloud-controller-manager-operator-tls
      secret:
        defaultMode: 420
        optional: true
        secretName: cloud-controller-manager-operator-tls
    - name: kube-api-access-7tbxp
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-06-11T10:44:32Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-06-11T10:44:22Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-06-11T10:56:19Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-06-11T10:56:19Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-06-11T10:44:22Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://0489515d07ef112b5100e84e350afe260e2d59799589f980df6b598bb488d390
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1bd9232bd59867a84e0c1ce986e4d77e8077d3d01eb3d0b9977ecdcad6a82d38
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1bd9232bd59867a84e0c1ce986e4d77e8077d3d01eb3d0b9977ecdcad6a82d38
      lastState:
        terminated:
          containerID: cri-o://dc937d55e33d99618b4c899e662cac77be126260bc5bf8321b14cc0d2131f162
          exitCode: 1
          finishedAt: "2024-06-11T10:56:17Z"
          message: |
            p="config.openshift.io" controllerKind="ClusterOperator" worker count=1
            E0611 10:44:33.324891       1 clusteroperator_controller.go:132] Unable to sync cluster operator status: Operation cannot be fulfilled on clusteroperators.config.openshift.io "cloud-controller-manager": the object has been modified; please apply your changes to the latest version and try again
            E0611 10:44:33.324941       1 controller.go:329] "Reconciler error" err="Operation cannot be fulfilled on clusteroperators.config.openshift.io \"cloud-controller-manager\": the object has been modified; please apply your changes to the latest version and try again" logger="CCMOperator" controller="clusteroperator" controllerGroup="config.openshift.io" controllerKind="ClusterOperator" ClusterOperator="cloud-controller-manager" namespace="" name="cloud-controller-manager" reconcileID="1a5046e5-cd6a-47af-bd56-2f887ae49189"
            E0611 10:55:00.436998       1 leaderelection.go:332] error retrieving resource lock openshift-cloud-controller-manager-operator/cluster-cloud-controller-manager-leader: Get "https://api-int.ci-op-9xx71rvq-1e28e.qe.azure.devcluster.openshift.com:6443/apis/coordination.k8s.io/v1/namespaces/openshift-cloud-controller-manager-operator/leases/cluster-cloud-controller-manager-leader": dial tcp 10.0.0.4:6443: i/o timeout
            E0611 10:55:56.439170       1 leaderelection.go:332] error retrieving resource lock openshift-cloud-controller-manager-operator/cluster-cloud-controller-manager-leader: Get "https://api-int.ci-op-9xx71rvq-1e28e.qe.azure.devcluster.openshift.com:6443/apis/coordination.k8s.io/v1/namespaces/openshift-cloud-controller-manager-operator/leases/cluster-cloud-controller-manager-leader": dial tcp 10.0.0.4:6443: i/o timeout
            I0611 10:56:17.436873       1 leaderelection.go:285] failed to renew lease openshift-cloud-controller-manager-operator/cluster-cloud-controller-manager-leader: timed out waiting for the condition
            E0611 10:56:17.436952       1 main.go:229] "problem running manager" err="leader election lost" logger="CCMOperator.setup"
          reason: Error
          startedAt: "2024-06-11T10:44:32Z"
      name: cluster-cloud-controller-manager
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2024-06-11T10:56:18Z"
    - containerID: cri-o://608eed299a6c45e5b3e5dd190aa31193be0fb89b19012f18aac287802bdaebb3
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1bd9232bd59867a84e0c1ce986e4d77e8077d3d01eb3d0b9977ecdcad6a82d38
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1bd9232bd59867a84e0c1ce986e4d77e8077d3d01eb3d0b9977ecdcad6a82d38
      lastState:
        terminated:
          containerID: cri-o://80f4518bd0777aa91b59de5be94209ba430205d22e4d900c486fe7ff5cfe1209
          exitCode: 1
          finishedAt: "2024-06-11T10:56:17Z"
          message: |
            00] managed cloud-config is not found, falling back to infrastructure config
            W0611 10:44:32.698274       1 cloud_config_sync_controller.go:100] managed cloud-config is not found, falling back to infrastructure config
            W0611 10:44:32.703894       1 cloud_config_sync_controller.go:100] managed cloud-config is not found, falling back to infrastructure config
            W0611 10:44:32.708501       1 cloud_config_sync_controller.go:100] managed cloud-config is not found, falling back to infrastructure config
            W0611 10:47:12.324859       1 cloud_config_sync_controller.go:100] managed cloud-config is not found, falling back to infrastructure config
            E0611 10:55:00.567989       1 leaderelection.go:332] error retrieving resource lock openshift-cloud-controller-manager-operator/cluster-cloud-config-sync-leader: Get "https://api-int.ci-op-9xx71rvq-1e28e.qe.azure.devcluster.openshift.com:6443/apis/coordination.k8s.io/v1/namespaces/openshift-cloud-controller-manager-operator/leases/cluster-cloud-config-sync-leader": dial tcp 10.0.0.4:6443: i/o timeout
            E0611 10:55:56.569667       1 leaderelection.go:332] error retrieving resource lock openshift-cloud-controller-manager-operator/cluster-cloud-config-sync-leader: Get "https://api-int.ci-op-9xx71rvq-1e28e.qe.azure.devcluster.openshift.com:6443/apis/coordination.k8s.io/v1/namespaces/openshift-cloud-controller-manager-operator/leases/cluster-cloud-config-sync-leader": dial tcp 10.0.0.4:6443: i/o timeout
            I0611 10:56:17.567980       1 leaderelection.go:285] failed to renew lease openshift-cloud-controller-manager-operator/cluster-cloud-config-sync-leader: timed out waiting for the condition
            E0611 10:56:17.568064       1 main.go:172] "problem running manager" err="leader election lost" logger="CCCMOConfigSyncControllers.setup"
            I0611 10:56:17.568082       1 internal.go:516] "Stopping and waiting for non leader election runnables" logger="CCCMOConfigSyncControllers"
            I0611 10:56:17.568112       1 internal.go:520] "Stopping and waiting for leader election runnables" logger="CCCMOConfigSyncControllers"
          reason: Error
          startedAt: "2024-06-11T10:44:32Z"
      name: config-sync-controllers
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2024-06-11T10:56:18Z"
    - containerID: cri-o://4bf36e0e6a87dc3a175bd8bcc19e85b4bf7e8cbd49c9d55a12df56dd25c670df
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:592ec166fee1aabf6b7dfd82cdd541e5cb608f99c7cc41c9ad3841dd1b854776
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:592ec166fee1aabf6b7dfd82cdd541e5cb608f99c7cc41c9ad3841dd1b854776
      lastState:
        terminated:
          containerID: cri-o://db4d7d3a000d6c97b94429877e858d92b40886c16278c5125c29c9bfecba204c
          exitCode: 1
          finishedAt: "2024-06-11T10:47:27Z"
          message: "AG: --skip-log-headers=\"false\"\nI0611 10:47:27.563947       1
            flags.go:64] FLAG: --stderrthreshold=\"\"\nI0611 10:47:27.563952       1
            flags.go:64] FLAG: --tls-cert-file=\"/etc/tls/private/tls.crt\"\nI0611
            10:47:27.563959       1 flags.go:64] FLAG: --tls-cipher-suites=\"[TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305]\"\nI0611
            10:47:27.563998       1 flags.go:64] FLAG: --tls-min-version=\"VersionTLS12\"\nI0611
            10:47:27.564007       1 flags.go:64] FLAG: --tls-private-key-file=\"/etc/tls/private/tls.key\"\nI0611
            10:47:27.564014       1 flags.go:64] FLAG: --tls-reload-interval=\"1m0s\"\nI0611
            10:47:27.564038       1 flags.go:64] FLAG: --upstream=\"http://127.0.0.1:9257/\"\nI0611
            10:47:27.564045       1 flags.go:64] FLAG: --upstream-ca-file=\"\"\nI0611
            10:47:27.564051       1 flags.go:64] FLAG: --upstream-client-cert-file=\"\"\nI0611
            10:47:27.564057       1 flags.go:64] FLAG: --upstream-client-key-file=\"\"\nI0611
            10:47:27.564064       1 flags.go:64] FLAG: --upstream-force-h2c=\"false\"\nI0611
            10:47:27.564070       1 flags.go:64] FLAG: --v=\"3\"\nI0611 10:47:27.564087
            \      1 flags.go:64] FLAG: --version=\"false\"\nI0611 10:47:27.564098
            \      1 flags.go:64] FLAG: --vmodule=\"\"\nW0611 10:47:27.564112       1
            deprecated.go:66] \n==== Removed Flag Warning ======================\n\nlogtostderr
            is removed in the k8s upstream and has no effect any more.\n\n===============================================\n\t\t\nI0611
            10:47:27.564126       1 kube-rbac-proxy.go:530] Reading config file: /etc/kube-rbac-proxy/config-file.yaml\nI0611
            10:47:27.564839       1 kube-rbac-proxy.go:233] Valid token audiences:
            \nI0611 10:47:27.564920       1 kube-rbac-proxy.go:347] Reading certificate
            files\nE0611 10:47:27.564976       1 run.go:74] \"command failed\" err=\"failed
            to initialize certificate reloader: error loading certificates: error
            loading certificate: open /etc/tls/private/tls.crt: no such file or directory\"\n"
          reason: Error
          startedAt: "2024-06-11T10:47:27Z"
      name: kube-rbac-proxy
      ready: true
      restartCount: 6
      started: true
      state:
        running:
          startedAt: "2024-06-11T10:50:20Z"
    hostIP: 10.0.0.6
    hostIPs:
    - ip: 10.0.0.6
    phase: Running
    podIP: 10.0.0.6
    podIPs:
    - ip: 10.0.0.6
    qosClass: Burstable
    startTime: "2024-06-11T10:44:22Z"
kind: PodList
metadata:
  resourceVersion: "39850"
