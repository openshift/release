kind: Template
apiVersion: template.openshift.io/v1

parameters:
- name: JOB_NAME
  required: true
- name: JOB_NAME_SAFE
  required: true
- name: JOB_NAME_HASH
  required: true
- name: NAMESPACE
  required: true
- name: IMAGE_TESTS
  required: true
- name: IMAGE_OPENSTACK_INSTALLER
  required: true
- name: CLUSTER_TYPE
  value: "openstack"
- name: TEST_COMMAND
  required: true
- name: RELEASE_IMAGE_LATEST
  required: true
- name: OS_CLOUD
  value: openstack-cloud
- name: OPENSTACK_EXTERNAL_NETWORK
  value: external
- name: RHCOS_RELEASE
  value: master
- name: OPENSTACK_FLAVOR
  value: m1.s2.xlarge
- name: BASE_DOMAIN
  value: shiftstack.devcluster.openshift.com
  required: true
- name: KURYR_ENABLED
  value: "false"
- name: BUILD_ID
  required: false
- name: NETWORK_TYPE
  value: OpenShiftSDN
- name: USE_LEASE_CLIENT
- name: TMP_SHARED
  value: /tmp/shared
- name: ARTIFACT_DIR
  value: /tmp/artifacts
- name: TMP_CLUSTER_PROFILE
  value: /tmp/cluster-profile
- name: DO_NOT_CREATE_DNS_RECORD
- name: NUMBER_OF_MASTERS
  value: "3"
- name: NUMBER_OF_WORKERS
  value: "3"


objects:

# We want the cluster to be able to access the images
- kind: RoleBinding
  apiVersion: authorization.openshift.io/v1
  metadata:
    name: ${JOB_NAME_SAFE}-image-puller
    namespace: ${NAMESPACE}
  roleRef:
    name: system:image-puller
  subjects:
  - kind: SystemGroup
    name: system:unauthenticated
  - kind: SystemGroup
    name: system:authenticated

# Give admin access to a known bot
- kind: RoleBinding
  apiVersion: authorization.openshift.io/v1
  metadata:
    name: ${JOB_NAME_SAFE}-namespace-admins
    namespace: ${NAMESPACE}
  roleRef:
    name: admin
  subjects:
  - kind: ServiceAccount
    namespace: ci
    name: ci-chat-bot

# Define the pod consisting of three containers: setup, test, teardown
- kind: Pod
  apiVersion: v1
  metadata:
    name: ${JOB_NAME_SAFE}
    namespace: ${NAMESPACE}
    annotations:
      # we want to gather the teardown logs no matter what
      ci-operator.openshift.io/wait-for-container-artifacts: teardown
      ci-operator.openshift.io/save-container-logs: "true"
      ci-operator.openshift.io/container-sub-tests: "setup,test,teardown"
  spec:
    restartPolicy: Never
    activeDeadlineSeconds: 21600
    terminationGracePeriodSeconds: 900
    volumes:
    - name: artifacts
      emptyDir: {}
    - name: shared-tmp
      emptyDir: {}
    - name: cluster-profile
      secret:
        secretName: ${JOB_NAME_SAFE}-cluster-profile
    initContainers:
    - name: cp-shared
      image: ${IMAGE_TESTS}
      volumeMounts:
      - name: shared-tmp
        mountPath: /tmp/shared
      command:
      - cp
      - /usr/bin/openshift-tests
      - /usr/bin/oc
      - /usr/bin/kubectl
      - /tmp/shared/

    containers:

    # setup container runs a upi install
    - name: setup
      image: ${IMAGE_OPENSTACK_INSTALLER}
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - name: artifacts
        mountPath: ${ARTIFACT_DIR}
      - name: shared-tmp
        mountPath: ${TMP_SHARED}
      - name: cluster-profile
        mountPath: ${TMP_CLUSTER_PROFILE}

      env:
      - name: CLUSTER_TYPE
        value: ${CLUSTER_TYPE}
      - name: CLUSTER_NAME
        value: ${NAMESPACE}-${JOB_NAME_HASH}
      - name: AWS_SHARED_CREDENTIALS_FILE
        value: ${TMP_CLUSTER_PROFILE}/.awscred
      - name: AWS_DEFAULT_REGION
        value: us-east-1
      - name: AWS_DEFAULT_OUTPUT
        value: json
      - name: AWS_PROFILE
        value: openshift-ci-infra
      - name: BASE_DOMAIN
        value: ${BASE_DOMAIN}
      - name: SSH_PUB_KEY_PATH
        value: ${TMP_CLUSTER_PROFILE}/ssh-publickey
      - name: SSH_PRIVATE_KEY_PATH
        value: ${TMP_CLUSTER_PROFILE}/ssh-privatekey
      - name: PULL_SECRET_PATH
        value: ${TMP_CLUSTER_PROFILE}/pull-secret
      - name: OPENSHIFT_INSTALL_RELEASE_IMAGE_OVERRIDE
        value: ${RELEASE_IMAGE_LATEST}
      - name: OPENSHIFT_INSTALL_INVOKER
        value: openshift-internal-ci/${JOB_NAME}/${BUILD_ID}
      - name: OPENSTACK_FLAVOR
        value: ${OPENSTACK_FLAVOR}
      - name: OPENSTACK_EXTERNAL_NETWORK
        value: ${OPENSTACK_EXTERNAL_NETWORK}
      - name: OS_CLOUD
        value: ${OS_CLOUD}
      - name: OS_CLIENT_CONFIG_FILE
        value: ${TMP_CLUSTER_PROFILE}/clouds.yaml
      - name: DO_NOT_CREATE_DNS_RECORD
        value: ${DO_NOT_CREATE_DNS_RECORD}
      - name: TMP_SHARED
        value: ${TMP_SHARED}
      - name: ARTIFACT_DIR
        value: ${ARTIFACT_DIR}
      - name: TMP_CLUSTER_PROFILE
        value: ${TMP_CLUSTER_PROFILE}
      - name: ASSETS_DIR
        value: ${ARTIFACT_DIR}/installer
      - name: OS_UPI_DIR
        value: /var/lib/openshift-install/upi/openstack
      - name: NETWORK_TYPE
        value: OpenShiftSDN
      - name: OS_SUBNET_RANGE
        value: 10.0.128.0/17
      - name: INSTALL_INITIAL_RELEASE
      - name: RELEASE_IMAGE_INITIAL
      - name: RELEASE_IMAGE_LATEST
        value: ${RELEASE_IMAGE_LATEST}
      - name: IMAGE_OPENSTACK_INSTALLER
        value: ${IMAGE_OPENSTACK_INSTALLER}
      - name: USER
        value: test
      - name: HOME
        value: /tmp/home
      - name: NUMBER_OF_MASTERS
        value: ${NUMBER_OF_MASTERS}
      - name: NUMBER_OF_WORKERS
        value: ${NUMBER_OF_WORKERS}
      - name: RHCOS_RELEASE
        value: ${RHCOS_RELEASE}

      command:
      - /bin/bash
      - -c
      - |
        #!/bin/bash
        echo "OpenStack: setup for cluster ${CLUSTER_NAME}"
        # configure pipefail to return value from last non-zero exiting process or children
        set -euo pipefail

        #Before we can do anything on the container we need to make sure our user has the approriate permissions.
        function allow_user_access() {
          if ! whoami &> /dev/null; then
            if [ -w /etc/passwd ]; then
              echo "${USER_NAME:-default}:x:$(id -u):0:${USER_NAME:-default} user:${HOME}:/sbin/nologin" >> /etc/passwd
            fi
          fi
          mkdir -p "${HOME}"
        }

        set -x
        oc version
        touch "${TMP_SHARED}/setup_early_failure"

    # Once the cluster is up, executes shared tests
    - name: test
      image: ${IMAGE_TESTS}
      terminationMessagePolicy: FallbackToLogsOnError
      resources:
        requests:
          cpu: 3
          memory: 600Mi
        limits:
          memory: 4Gi
      volumeMounts:
      - name: shared-tmp
        mountPath: ${TMP_SHARED}
      - name: cluster-profile
        mountPath: ${TMP_CLUSTER_PROFILE}
      - name: artifacts
        mountPath: ${ARTIFACT_DIR}
      env:
      - name: OS_CLOUD
        value: ${OS_CLOUD}
      - name: OS_CLIENT_CONFIG_FILE
        value: ${TMP_CLUSTER_PROFILE}/clouds.yaml
      - name: KUBECONFIG
        value: /tmp/artifacts/installer/auth/kubeconfig
      - name: TMP_SHARED
        value: ${TMP_SHARED}
      - name: ARTIFACT_DIR
        value: ${ARTIFACT_DIR}
      - name: TMP_CLUSTER_PROFILE
        value: ${TMP_CLUSTER_PROFILE}
      - name: USER
        value: test
      - name: HOME
        value: /tmp/home
      - name: NUMBER_OF_MASTERS
        value: ${NUMBER_OF_MASTERS}
      - name: NUMBER_OF_WORKERS
        value: ${NUMBER_OF_WORKERS}
      command:
      - /bin/bash
      - -c
      - |
        #!/bin/bash
        set -euo pipefail

        export PATH=${TMP_SHARED}:/usr/libexec/origin:$PATH

        trap 'touch ${TMP_SHARED}/exit' EXIT ERR
        trap 'jobs -p | xargs -r kill || true; exit 0' TERM

        function allow_user_access() {
          if ! whoami &> /dev/null; then
            if [ -w /etc/passwd ]; then
              echo "${USER_NAME:-default}:x:$(id -u):0:${USER_NAME:-default} user:${HOME}:/sbin/nologin" >> /etc/passwd
            fi
          fi
          mkdir -p "${HOME}"
        }

        function wait_for_other_containers() {
          # Wait for setup container to finish or an exit
          while true; do
            echo "waiting for setup container to finish"
            if [[ -f ${TMP_SHARED}/setup-failed ]]; then
              echo "Setup reported a failure." 2>&1
              exit 0
            fi
            if [[ -f ${TMP_SHARED}/exit ]]; then
              echo "Another process exited" 2>&1
              exit 1
            fi
            if [[ ! -f ${TMP_SHARED}/setup-complete ]]; then
              sleep 15 & wait
              continue
            fi
            # don't let clients impact the global kubeconfig
            cp "${KUBECONFIG}" /tmp/admin.kubeconfig
            export KUBECONFIG=/tmp/admin.kubeconfig
            break
          done
        }

        function install_secret_on_cluster() {
          # if the cluster profile included an insights secret, install it to the cluster to
          # report support data from the support-operator
          if [[ -f ${TMP_CLUSTER_PROFILE}/insights-live.yaml ]]; then
            oc create -f ${TMP_CLUSTER_PROFILE}/insights-live.yaml || true
          fi
        }

        function setup_env_vars() {
          # set up env vars
          export KUBE_SSH_BASTION="$( oc --insecure-skip-tls-verify get node -l node-role.kubernetes.io/master -o 'jsonpath={.items[0].status.addresses[?(@.type=="ExternalIP")].address}' ):22"
          export KUBE_SSH_KEY_PATH=${TMP_CLUSTER_PROFILE}/ssh-privatekey
          export KUBE_SSH_USER=core
          mkdir -p ~/.ssh
          cp ${TMP_CLUSTER_PROFILE}/ssh-privatekey ~/.ssh/kube_openstack_rsa || true
          export TEST_PROVIDER='{"type":"openstack"}'
        }

        function configure_xtrace() {
          XTRACE_ENABLED=0
          if set -o | grep xtrace.*on &>/dev/null; then
            XTRACE_ENABLED=1
            set -x
          fi
        }

        function setup_test_bed() {
          mkdir -p "${HOME}"
          mkdir -p /tmp/output
          cd /tmp/output
        }

        function run-upgrade-tests() {
          openshift-tests run-upgrade "${TEST_SUITE}" --to-image "${IMAGE:-${RELEASE_IMAGE_LATEST}}" \
            --options "${TEST_OPTIONS:-}" \
            --provider "${TEST_PROVIDER:-}" -o ${ARTIFACT_DIR}/e2e.log --junit-dir ${ARTIFACT_DIR}/junit
        }

        function run-tests() {
          openshift-tests run "${TEST_SUITE}" \
            --provider "${TEST_PROVIDER:-}" -o ${ARTIFACT_DIR}/e2e.log --junit-dir ${ARTIFACT_DIR}/junit
        }

        function run-minimal-tests() {
          # Only execute Smoke (<4.4) or Early (>= 4.4) tests while the test
          # infrastructure is getting prepared to run the actual suite
          # reliably.
          openshift-tests run openshift/conformance/parallel --dry-run |
            grep 'Smoke\|Early' |
          openshift-tests run -o ${ARTIFACT_DIR}/e2e.log \
            --junit-dir ${ARTIFACT_DIR}/junit -f -
          return 0
        }

        function run-no-tests() {
          # This can be used if we just want to check the installer exits 0
          echo "WARNING: No tests were run against the installed cluster"
          return 0
        }

        # Test execution
        allow_user_access
        wait_for_other_containers
        setup_test_bed
        ${TEST_COMMAND}

    # teardown container performs cleanup of all created resources
    - name: teardown
      image: ${IMAGE_OPENSTACK_INSTALLER}
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - name: shared-tmp
        mountPath: ${TMP_SHARED}
      - name: cluster-profile
        mountPath: ${TMP_CLUSTER_PROFILE}
      - name: artifacts
        mountPath: ${ARTIFACT_DIR}
      env:
      - name: CLUSTER_TYPE
        value: ${CLUSTER_TYPE}
      - name: CLUSTER_NAME
        value: ${NAMESPACE}-${JOB_NAME_HASH}
      - name: AWS_SHARED_CREDENTIALS_FILE
        value: ${TMP_CLUSTER_PROFILE}/.awscred
      - name: AWS_DEFAULT_REGION
        value: us-east-1
      - name: AWS_DEFAULT_OUTPUT
        value: json
      - name: AWS_PROFILE
        value: openshift-ci-infra
      - name: KUBECONFIG
        value: /tmp/artifacts/installer/auth/kubeconfig
      - name: SSH_PRIVATE_KEY_PATH
        value: ${TMP_CLUSTER_PROFILE}/ssh-privatekey
      - name: OS_CLOUD
        value: ${OS_CLOUD}
      - name: OS_CLIENT_CONFIG_FILE
        value: ${TMP_CLUSTER_PROFILE}/clouds.yaml
      - name: DO_NOT_CREATE_DNS_RECORD
        value: ${DO_NOT_CREATE_DNS_RECORD}
      - name: TMP_SHARED
        value: ${TMP_SHARED}
      - name: ARTIFACT_DIR
        value: ${ARTIFACT_DIR}
      - name: TMP_CLUSTER_PROFILE
        value: ${TMP_CLUSTER_PROFILE}
      - name: ASSETS_DIR
        value: ${ARTIFACT_DIR}/installer
      - name: OS_UPI_DIR
        value: /var/lib/openshift-install/upi/openstack
      - name: USER
        value: test
      - name: HOME
        value: /tmp/home
      - name: NUMBER_OF_MASTERS
        value: ${NUMBER_OF_MASTERS}
      - name: NUMBER_OF_WORKERS
        value: ${NUMBER_OF_WORKERS}

      command:
      - /bin/bash
      - -c
      - |
        #!/bin/bash
        set -eo pipefail

        mkdir -p "${HOME}"

        function queue() {
          local TARGET="${1}"
          echo target: $TARGET
          shift
          local LIVE="$(jobs | wc -l)"
          echo live $LIVE
          while [[ "${LIVE}" -ge 45 ]]; do
            sleep 1
            LIVE="$(jobs | wc -l)"
          done
          echo "${@}"
          if [[ -n "${FILTER:-}" ]]; then
            "${@}" | "${FILTER}" >"${TARGET}" &
          else
            "${@}" >"${TARGET}" &
          fi
        }

        function exit_on_setup_early_failure() {
            if [[ -f ${TMP_SHARED}/setup_early_failure ]]; then
              echo Setup container reported Early Failure. No teardown will be executed for ${CLUSTER_NAME}
              exit 0
            fi
        }
        function collect_logs_from_proxy() {
          # collect logs from the proxy
          if [ -f "${ASSETS_DIR}/proxyip" ]
          then
            proxy_ip="$(cat ${ASSETS_DIR}/proxyip)"
            mkdir -p ${ARTIFACT_DIR}/proxy
            eval $(ssh-agent)
            ssh-add /etc/openshift-installer/ssh-privatekey
            ssh -A -o PreferredAuthentications=publickey -o StrictHostKeyChecking=false -o UserKnownHostsFile=/dev/null core@${proxy_ip} 'journalctl -u squid' > ${ARTIFACT_DIR}/proxy/squid.service

          fi
        }

        function start_teardown() {
          #start of teardown
          # We have to truncate cluster name to 14 chars, because there is a limitation in the install-config
          # Now it looks like "ci-op-rl6z646h-65230".
          # We will remove "ci-op-" prefix from there to keep just last 14 characters.
          UNSAFE_CLUSTER_NAME=${CLUSTER_NAME:${#CLUSTER_NAME}<14?0:-14}
          export CLUSTER_NAME=${UNSAFE_CLUSTER_NAME#"-"}
          echo "Teardown for ${CLUSTER_NAME}"
          touch ${TMP_SHARED}/exit
          export PATH=$PATH:${TMP_SHARED}
          mkdir -p ${ARTIFACT_DIR}/pods ${ARTIFACT_DIR}/nodes ${ARTIFACT_DIR}/metrics ${ARTIFACT_DIR}/bootstrap ${ARTIFACT_DIR}/network
        }


        function gather_openstack_information() {
          echo "Gathering openstack server list"
          openstack server list | grep -e $CLUSTER_NAME >${ARTIFACT_DIR}/openstack_nodes.log

          echo "Gathering openstack server details"
          for server in $(openstack server list -c Name -f value | grep -e $CLUSTER_NAME | sort); do
              echo -e "\n$ openstack server show $server" >>${ARTIFACT_DIR}/openstack_nodes.log
              openstack server show $server >>${ARTIFACT_DIR}/openstack_nodes.log
              if [[ "$server" == *"bootstrap"* ]]; then
                openstack console log show $server &>${ARTIFACT_DIR}/bootstrap/nova.log
              else
                openstack console log show $server &>${ARTIFACT_DIR}/nodes/console_${server}.log
              fi
          done

          echo "Gathering openstack securitygroups"
          export INFRA_ID=$(sed -n 's|.*"infraID":"\([^"]*\)".*|\1|p' ${ASSETS_DIR}/metadata.json )

          os_sg_master="${INFRA_ID}-master"
          os_sg_worker="${INFRA_ID}-worker"

          openstack security group rule list $os_sg_master &>${ARTIFACT_DIR}/${INFRA_ID}-master-security-group
          openstack security group rule list $os_sg_worker &>${ARTIFACT_DIR}/${INFRA_ID}-worker-security-group

        }

        function gather_bootstrap_logs() {
           echo gathering bootstrap logs
            if [ -f "${TMP_SHARED}/BOOTSTRAP_FIP" ] ; then
                echo we do know the bootstrap fip
                B_FIP=$(cat ${TMP_SHARED}/BOOTSTRAP_FIP)
                ssh -i ${SSH_PRIVATE_KEY_PATH} -o "StrictHostKeyChecking no" core@${B_FIP} 'journalctl -b  -u release-image.service -u bootkube.service > bootstrap.log; tar -czf bootstrap.log.tgz bootstrap.log'
                scp -i ${SSH_PRIVATE_KEY_PATH} -o "StrictHostKeyChecking no" core@${B_FIP}:~/bootstrap.log.tgz ${ARTIFACT_DIR}/bootstrap/
            fi
           echo finished gathering bootstrap logs
        }

        function gather_master_logs() {
            echo "NO MASTER LOGS WERE COLLECTED"
            # this still needs to be implementd.
        }

        function gather_compute_logs() {
           echo "NO COMPUTE LOGS WERE COLLECTED"
           # this still needs to be implemented
        }

        function gather_openshift_information() {

          if [[ ! -f ${TMP_SHARED}/oc-bootstrap-started ]]; then
            echo openshift-install wait-for bootstrap-complete was not executed. No OC information collected.
            return 1
          fi
          echo "Gathering openshift nodes"
          oc --insecure-skip-tls-verify --request-timeout=5s get nodes -o jsonpath --template '{range .items[*]}{.metadata.name}{"\n"}{end}' > /tmp/nodes
          echo "Gathering openshift pods"
          oc --insecure-skip-tls-verify --request-timeout=5s get pods --all-namespaces --template '{{ range .items }}{{ $name := .metadata.name }}{{ $ns := .metadata.namespace }}{{ range .spec.containers }}-n {{ $ns }} {{ $name }} -c {{ .name }}{{ "\n" }}{{ end }}{{ range .spec.initContainers }}-n {{ $ns }} {{ $name }} -c {{ .name }}{{ "\n" }}{{ end }}{{ end }}' > /tmp/containers
          oc --insecure-skip-tls-verify --request-timeout=5s get pods -l openshift.io/component=api --all-namespaces --template '{{ range .items }}-n {{ .metadata.namespace }} {{ .metadata.name }}{{ "\n" }}{{ end }}' > /tmp/pods-api

          queue ${ARTIFACT_DIR}/config-resources.json oc --insecure-skip-tls-verify --request-timeout=5s get apiserver.config.openshift.io authentication.config.openshift.io build.config.openshift.io console.config.openshift.io dns.config.openshift.io featuregate.config.openshift.io image.config.openshift.io infrastructure.config.openshift.io ingress.config.openshift.io network.config.openshift.io oauth.config.openshift.io project.config.openshift.io scheduler.config.openshift.io -o json
          queue ${ARTIFACT_DIR}/apiservices.json oc --insecure-skip-tls-verify --request-timeout=5s get apiservices -o json
          queue ${ARTIFACT_DIR}/clusteroperators.json oc --insecure-skip-tls-verify --request-timeout=5s get clusteroperators -o json
          queue ${ARTIFACT_DIR}/clusterversion.json oc --insecure-skip-tls-verify --request-timeout=5s get clusterversion -o json
          queue ${ARTIFACT_DIR}/configmaps.json oc --insecure-skip-tls-verify --request-timeout=5s get configmaps --all-namespaces -o json
          queue ${ARTIFACT_DIR}/credentialsrequests.json oc --insecure-skip-tls-verify --request-timeout=5s get credentialsrequests --all-namespaces -o json
          queue ${ARTIFACT_DIR}/csr.json oc --insecure-skip-tls-verify --request-timeout=5s get csr -o json
          queue ${ARTIFACT_DIR}/endpoints.json oc --insecure-skip-tls-verify --request-timeout=5s get endpoints --all-namespaces -o json
          FILTER=gzip queue ${ARTIFACT_DIR}/deployments.json.gz oc --insecure-skip-tls-verify --request-timeout=5s get deployments --all-namespaces -o json
          FILTER=gzip queue ${ARTIFACT_DIR}/daemonsets.json.gz oc --insecure-skip-tls-verify --request-timeout=5s get daemonsets --all-namespaces -o json
          queue ${ARTIFACT_DIR}/events.json oc --insecure-skip-tls-verify --request-timeout=5s get events --all-namespaces -o json
          queue ${ARTIFACT_DIR}/kubeapiserver.json oc --insecure-skip-tls-verify --request-timeout=5s get kubeapiserver -o json
          queue ${ARTIFACT_DIR}/kubecontrollermanager.json oc --insecure-skip-tls-verify --request-timeout=5s get kubecontrollermanager -o json
          queue ${ARTIFACT_DIR}/machineconfigpools.json oc --insecure-skip-tls-verify --request-timeout=5s get machineconfigpools -o json
          queue ${ARTIFACT_DIR}/machineconfigs.json oc --insecure-skip-tls-verify --request-timeout=5s get machineconfigs -o json
          queue ${ARTIFACT_DIR}/machinesets.json oc --insecure-skip-tls-verify --request-timeout=5s get machinesets -A -o json
          queue ${ARTIFACT_DIR}/machines.json oc --insecure-skip-tls-verify --request-timeout=5s get machines -A -o json
          queue ${ARTIFACT_DIR}/namespaces.json oc --insecure-skip-tls-verify --request-timeout=5s get namespaces -o json
          queue ${ARTIFACT_DIR}/nodes.json oc --insecure-skip-tls-verify --request-timeout=5s get nodes -o json
          queue ${ARTIFACT_DIR}/openshiftapiserver.json oc --insecure-skip-tls-verify --request-timeout=5s get openshiftapiserver -o json
          queue ${ARTIFACT_DIR}/pods.json oc --insecure-skip-tls-verify --request-timeout=5s get pods --all-namespaces -o json
          queue ${ARTIFACT_DIR}/persistentvolumes.json oc --insecure-skip-tls-verify --request-timeout=5s get persistentvolumes --all-namespaces -o json
          queue ${ARTIFACT_DIR}/persistentvolumeclaims.json oc --insecure-skip-tls-verify --request-timeout=5s get persistentvolumeclaims --all-namespaces -o json
          FILTER=gzip queue ${ARTIFACT_DIR}/replicasets.json.gz oc --insecure-skip-tls-verify --request-timeout=5s get replicasets --all-namespaces -o json
          queue ${ARTIFACT_DIR}/rolebindings.json oc --insecure-skip-tls-verify --request-timeout=5s get rolebindings --all-namespaces -o json
          queue ${ARTIFACT_DIR}/roles.json oc --insecure-skip-tls-verify --request-timeout=5s get roles --all-namespaces -o json
          queue ${ARTIFACT_DIR}/services.json oc --insecure-skip-tls-verify --request-timeout=5s get services --all-namespaces -o json
          FILTER=gzip queue ${ARTIFACT_DIR}/statefulsets.json.gz oc --insecure-skip-tls-verify --request-timeout=5s get statefulsets --all-namespaces -o json

          FILTER=gzip queue ${ARTIFACT_DIR}/openapi.json.gz oc --insecure-skip-tls-verify --request-timeout=5s get --raw /openapi/v2

          # gather nodes first in parallel since they may contain the most relevant debugging info
          while IFS= read -r i; do
            mkdir -p ${ARTIFACT_DIR}/nodes/$i
            queue ${ARTIFACT_DIR}/nodes/$i/heap oc --insecure-skip-tls-verify get --request-timeout=20s --raw /api/v1/nodes/$i/proxy/debug/pprof/heap
          done < /tmp/nodes

          FILTER=gzip queue ${ARTIFACT_DIR}/nodes/masters-journal.gz oc --insecure-skip-tls-verify adm node-logs --role=master --unify=false
          FILTER=gzip queue ${ARTIFACT_DIR}/nodes/workers-journal.gz oc --insecure-skip-tls-verify adm node-logs --role=worker --unify=false

          # Snapshot iptables-save on each node for debugging possible kube-proxy issues
          oc --insecure-skip-tls-verify get --request-timeout=20s -n openshift-sdn -l app=sdn pods --template '{{ range .items }}{{ .metadata.name }}{{ "\n" }}{{ end }}' > /tmp/sdn-pods
          while IFS= read -r i; do
            queue ${ARTIFACT_DIR}/network/iptables-save-$i oc --insecure-skip-tls-verify rsh --timeout=20 -n openshift-sdn -c sdn $i iptables-save -c
          done < /tmp/sdn-pods

          while IFS= read -r i; do
            file="$( echo "$i" | cut -d ' ' -f 3 | tr -s ' ' '_' )"
            queue ${ARTIFACT_DIR}/metrics/${file}-heap oc --insecure-skip-tls-verify exec $i -- /bin/bash -c 'oc --insecure-skip-tls-verify get --raw /debug/pprof/heap --server "https://$( hostname ):8443" --config /etc/origin/master/admin.kubeconfig'
            queue ${ARTIFACT_DIR}/metrics/${file}-controllers-heap oc --insecure-skip-tls-verify exec $i -- /bin/bash -c 'oc --insecure-skip-tls-verify get --raw /debug/pprof/heap --server "https://$( hostname ):8444" --config /etc/origin/master/admin.kubeconfig'
          done < /tmp/pods-api

          while IFS= read -r i; do
            file="$( echo "$i" | cut -d ' ' -f 2,3,5 | tr -s ' ' '_' )"
            FILTER=gzip queue ${ARTIFACT_DIR}/pods/${file}.log.gz oc --insecure-skip-tls-verify logs --request-timeout=20s $i
            FILTER=gzip queue ${ARTIFACT_DIR}/pods/${file}_previous.log.gz oc --insecure-skip-tls-verify logs --request-timeout=20s -p $i
          done < /tmp/containers

          echo "Snapshotting prometheus (may take 15s) ..."
          queue ${ARTIFACT_DIR}/metrics/prometheus.tar.gz oc --insecure-skip-tls-verify exec -n openshift-monitoring prometheus-k8s-0 -- tar cvzf - -C /prometheus .

          # move private key to ~/.ssh/ so that installer can use it to gather logs
          mkdir -p ~/.ssh
          cp "${SSH_PRIVATE_KEY_PATH}" ~/.ssh/id_rsa
          chmod 0600 ~/.ssh/id_rsa

          echo "Running must-gather..."
          mkdir -p ${ARTIFACT_DIR}/must-gather
          queue ${ARTIFACT_DIR}/must-gather/must-gather.log oc --insecure-skip-tls-verify adm must-gather --dest-dir ${ARTIFACT_DIR}/must-gather

          echo "Waiting for logs ..."
          wait

          tar -czC ${TMP_ARTIFACTS}/must-gather -f ${TMP_ARTIFACTS}/must-gather.tar.gz . &&
          rm -rf ${TMP_ARTIFACTS}/must-gather
        }


        function remove_dns_entries() {
          if [ -z "$DO_NOT_CREATE_DNS_RECORD" ]; then
            echo "Removing entries from DNS ..."
            export HOSTED_ZONE_ID=$(aws route53 list-hosted-zones-by-name --dns-name "shiftstack.devcluster.openshift.com" | python -c 'import json,sys;print json.load(sys.stdin)["HostedZones"][0]["Id"].split("/")[-1]')

            sed '
              s/UPSERT/DELETE/;
              s/Create/Delete/;
              ' "${ASSETS_DIR}/api-record.json" > "${ASSETS_DIR}/delete-api-record.json"
            aws route53 change-resource-record-sets \
              --hosted-zone-id "$HOSTED_ZONE_ID" \
              --change-batch "file://${ASSETS_DIR}/delete-api-record.json"

            sed '
              s/UPSERT/DELETE/;
              s/Create/Delete/;
            ' "${ASSETS_DIR}/ingress-record.json" > "${ASSETS_DIR}/delete-ingress-record.json"
            aws route53 change-resource-record-sets \
              --hosted-zone-id "$HOSTED_ZONE_ID" \
              --change-batch "file://${ASSETS_DIR}/delete-ingress-record.json"

          else
            echo "There are no DNS entries to remove"
          fi
        }

        function delete_api_fip() {
          LB_FIP_UID=`cat ${TMP_SHARED}/LB_FIP_UID`
          echo "Deleting api FIP ... for ${CLUSTER_NAME} with UID: ${LB_FIP_UID}"
          openstack floating ip delete $LB_FIP_UID
        }

        function delete_ingress_fip() {
          INGRESS_FIP_UID=`cat ${TMP_SHARED}/INGRESS_FIP_UID`
          echo "Deleting ingress FIP using UID: ${INGRESS_FIP_UID}"
          openstack floating ip delete $INGRESS_FIP_UID
        }

        function delete_glance_shim_image() {
          GLANCE_SHIM_IMAGE_ID=`cat ${TMP_SHARED}/GLANCE_SHIM_IMAGE_ID`
          echo "Deleting glance image $GLANCE_SHIM_IMAGE_ID"
          openstack image delete $GLANCE_SHIM_IMAGE_ID
        }

        function delete_glance_rhcos_image() {
          RHCOS_GLANCE_IMAGE_ID=`cat ${TMP_SHARED}/RHCOS_GLANCE_IMAGE_ID`
          echo "Deleting rhcos glance image ${RHCOS_GLANCE_IMAGE_ID}"
          openstack image delete ${RHCOS_GLANCE_IMAGE_ID}
        }

        function verify_teardown_playbooks() {
          ansible-playbook --list-tasks -i "${ASSETS_DIR}/inventory.yaml" "${ASSETS_DIR}/down-load-balancers.yaml"
          ansible-playbook --list-tasks -i "${ASSETS_DIR}/inventory.yaml" "${ASSETS_DIR}/down-compute-nodes.yaml"
          ansible-playbook --list-tasks -i "${ASSETS_DIR}/inventory.yaml" "${ASSETS_DIR}/down-control-plane.yaml"
          ansible-playbook --list-tasks -i "${ASSETS_DIR}/inventory.yaml" "${ASSETS_DIR}/down-bootstrap.yaml"
          ansible-playbook --list-tasks -i "${ASSETS_DIR}/inventory.yaml" "${ASSETS_DIR}/down-network.yaml"
          ansible-playbook --list-tasks -i "${ASSETS_DIR}/inventory.yaml" "${ASSETS_DIR}/down-security-groups.yaml"
          ansible-playbook --list-tasks -i "${ASSETS_DIR}/inventory.yaml" "${ASSETS_DIR}/down-containers.yaml"
        }

        function run_teardown_playbooks() {
          ansible-playbook -i "${ASSETS_DIR}/inventory.yaml" "${ASSETS_DIR}/down-load-balancers.yaml"
          ansible-playbook -i "${ASSETS_DIR}/inventory.yaml" "${ASSETS_DIR}/down-compute-nodes.yaml"
          ansible-playbook -i "${ASSETS_DIR}/inventory.yaml" "${ASSETS_DIR}/down-control-plane.yaml"
          ansible-playbook -i "${ASSETS_DIR}/inventory.yaml" "${ASSETS_DIR}/down-bootstrap.yaml"
          ansible-playbook -i "${ASSETS_DIR}/inventory.yaml" "${ASSETS_DIR}/down-network.yaml"
          ansible-playbook -i "${ASSETS_DIR}/inventory.yaml" "${ASSETS_DIR}/down-security-groups.yaml"
          ansible-playbook -i "${ASSETS_DIR}/inventory.yaml" "${ASSETS_DIR}/down-containers.yaml"
        }

        function delete_cluster_credentials() {
          ignition_sponge="$(mktemp)"
          jq '.ignition.config.merge[].httpHeaders[].value="REDACTED"' "${ASSETS_DIR}/${INFRA_ID}-bootstrap-ignition.json" > "$ignition_sponge"
          mv "$ignition_sponge" "${ASSETS_DIR}/${INFRA_ID}-bootstrap-ignition.json"

          rm -r \
            ${ASSETS_DIR}/auth \
            ${ASSETS_DIR}/bootstrap.ign
        }

        function destroy_cluster() {
          # This is UPI setup so we don't destroy the cluster using "destroy cluster"
          #TF_LOG=debug openshift-install  --dir ${ASSETS_DIR} destroy cluster --log-level=debug 2>&1 | grep --line-buffered -v password
          echo openshift-install destroy cluster NOT used
        }

        function allow_user_access() {
          if ! whoami &> /dev/null; then
            if [ -w /etc/passwd ]; then
              echo "${USER_NAME:-default}:x:$(id -u):0:${USER_NAME:-default} user:${HOME}:/sbin/nologin" >> /etc/passwd
            fi
          fi
          mkdir -p "${HOME}"
        }

        function wait_for_other_containers_to_exit() {
        # we wait for one of the containers to flag a ${TMP_SHARED}/exit, then exit (triggering teardown)
          echo Waiting for ${TMP_SHARED}/exit
          for i in $(seq 1 2200); do
            if [[ -f ${TMP_SHARED}/exit ]]; then
              exit 0
            fi
            sleep 6 & wait
          done
        }

        function signal_teardown_container_is_running(){
          touch ${TMP_SHARED}/teardown_running
        }

        function teardown() {
          set +e
          start_teardown
          gather_bootstrap_logs
          exit_on_setup_early_failure
          collect_logs_from_proxy
          gather_openstack_information
          gather_openshift_information
          remove_dns_entries
          delete_api_fip
          delete_ingress_fip
          delete_glance_shim_image
          delete_glance_rhcos_image
          verify_teardown_playbooks
          destroy_cluster
          run_teardown_playbooks
          delete_cluster_credentials
          echo "Teardown completed succesfully"
        }

        #Teardown execution
        trap 'teardown' EXIT
        trap 'jobs -p | xargs -r kill || true; exit 0' TERM

        signal_teardown_container_is_running
        allow_user_access
        wait_for_other_containers_to_exit

