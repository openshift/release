kind: Template
apiVersion: template.openshift.io/v1

parameters:
- name: JOB_NAME_SAFE
  required: true
- name: JOB_NAME_HASH
  required: true
- name: LOCAL_IMAGE_BIN
  required: true
- name: IMAGE_SYNC
  required: true
- name: IMAGE_ETCDBACKUP
  required: true
- name: IMAGE_AZURE_CONTROLLERS
  required: true
- name: NAMESPACE
  required: true
- name: CLUSTER_TYPE
  value: "azure"
  required: true
- name: TEST_IN_PRODUCTION
- name: DEPLOY_VERSION
  value: "v3.11"
- name: TEST_IMAGE
  value: "registry.svc.ci.openshift.org/azure/test-base:latest"
- name: TEST_COMMAND
  value: "make e2e"
- name: RESOURCEGROUP_SUFFIX
  generate: expression
  from: "[a-z]{8}"

objects:

# We want the cluster to be able to access these images
- kind: RoleBinding
  apiVersion: authorization.openshift.io/v1
  metadata:
    name: ${JOB_NAME_SAFE}-image-puller
    namespace: ${NAMESPACE}
  roleRef:
    name: system:image-puller
  subjects:
  - kind: SystemGroup
    name: system:unauthenticated

- kind: Pod
  apiVersion: v1
  metadata:
    name: ${JOB_NAME_SAFE}
    namespace: ${NAMESPACE}
    annotations:
      # we want to gather the teardown logs no matter what
      ci-operator.openshift.io/wait-for-container-artifacts: teardown,test
      ci-operator.openshift.io/save-container-logs: "true"
  spec:
    restartPolicy: Never
    activeDeadlineSeconds: 10800
    terminationGracePeriodSeconds: 600
    volumes:
    - name: artifacts
      emptyDir: {}
    - name: shared-tmp
      emptyDir: {}
    - name: cluster-secrets-azure
      secret:
        secretName: azure

    containers:

    # Once admin.kubeconfig exists, executes shared tests
    - name: test
      image: ${TEST_IMAGE}
      imagePullPolicy: Always
      resources:
        requests:
          cpu: 1
          memory: 300Mi
        limits:
          cpu: 3
          memory: 4Gi
      volumeMounts:
      - name: artifacts
        mountPath: /tmp/artifacts
      - name: shared-tmp
        mountPath: /tmp/shared
      env:
      - name: ARTIFACT_DIR
        value: /tmp/artifacts
      - name: HOME
        value: /tmp/shared/home
      - name: TEST_IN_PRODUCTION
        value: ${TEST_IN_PRODUCTION}
      command:
      - /bin/bash
      - -c
      - |
        #!/bin/bash
        # error handling and sync code
        set -euo pipefail

        if [[ -n "${TEST_IN_PRODUCTION}" ]]; then
          # If we are testing in production then currently
          # we don't have any _data locally to run openshift e2es.
          exit 0
        fi

        trap 'touch /tmp/shared/exit' EXIT
        trap 'kill $(jobs -p); exit 0' TERM

        mkdir -p "${HOME}"

        # wait until the setup job creates admin.kubeconfig
        while true; do
          if [[ ! -f  /tmp/shared/openshift-azure/_data/_out/admin.kubeconfig ]]; then
            sleep 15 & wait
            continue
          fi
          # if we got admin kubeconfig file with failure marker, ignore. Teardown is in progress.
          if [[ -f  /tmp/shared/exit ]]; then
             exit 1
          fi
          break
        done
        echo "Found shared kubeconfig"

        # test specific code
        # don't let clients impact the global kubeconfig
        mkdir -p /go/src/github.com/openshift/
        cp -r /tmp/shared/openshift-azure /go/src/github.com/openshift/
        cd /go/src/github.com/openshift/openshift-azure/
        ${TEST_COMMAND}

    # Runs an install
    - name: setup
      image: ${LOCAL_IMAGE_BIN}
      imagePullPolicy: Always
      volumeMounts:
      - name: shared-tmp
        mountPath: /tmp/shared
      - name: cluster-secrets-azure
        mountPath: /etc/azure/credentials
      env:
      - name: RESOURCEGROUP
        value: e2e-${RESOURCEGROUP_SUFFIX}
      - name: TYPE
        value: ${CLUSTER_TYPE}
      - name: HOME
        value: /tmp/shared/home
      - name: SYNC_IMAGE
        value: ${IMAGE_SYNC}
      - name: ETCDBACKUP_IMAGE
        value: ${IMAGE_ETCDBACKUP}
      - name: AZURE_CONTROLLERS_IMAGE
        value: ${IMAGE_AZURE_CONTROLLERS}
      - name: RUNNING_UNDER_TEST
        value: "true"
      - name: RESOURCEGROUP_TTL
        value: 4h
      - name: TEST_IN_PRODUCTION
        value: ${TEST_IN_PRODUCTION}
      - name: DEPLOY_VERSION
        value: ${DEPLOY_VERSION}
      command:
      - /bin/bash
      - -c
      - |
       #!/bin/bash
        set -euo pipefail

        trap 'rc=$?; if [[ $rc -ne 0 || -n "$TEST_IN_PRODUCTION" ]]; then
          touch /tmp/shared/exit;
        fi;
        cp -r /go/src/github.com/openshift/openshift-azure /tmp/shared &>/dev/null
        exit $rc' EXIT
        trap 'kill $(jobs -p); exit 0' TERM

        # Cluster creation specific configuration.
        mkdir -p "${HOME}"
        source /etc/azure/credentials/secret
        az login --service-principal -u ${AZURE_CLIENT_ID} -p ${AZURE_CLIENT_SECRET} --tenant ${AZURE_TENANT_ID} &>/dev/null
        # aad integration configuration - we dont test aad so populate as dummy
        export AZURE_AAD_CLIENT_ID=$AZURE_CLIENT_ID
        export AZURE_AAD_CLIENT_SECRET=$AZURE_CLIENT_SECRET
        export AZURE_REGION=eastus
        echo "Using sync image ${SYNC_IMAGE}"
        echo "Using etcdbackup image ${ETCDBACKUP_IMAGE}"
        echo "Using azure-controllers image ${AZURE_CONTROLLERS_IMAGE}"
        export DNS_DOMAIN=osadev.cloud
        export DNS_RESOURCEGROUP=dns
        export DEPLOY_VERSION=${DEPLOY_VERSION}
        export IMAGE_RESOURCEGROUP=images
        export IMAGE_RESOURCENAME=$(az image list -g $IMAGE_RESOURCEGROUP -o json --query "[?starts_with(name, '${DEPLOY_OS:-rhel7}-${DEPLOY_VERSION//v}') && tags.valid=='true'].name | sort(@) | [-1]" | tr -d '"')
        # create cluster for test
        cd /go/src/github.com/openshift/openshift-azure/
        timeout 1h ./hack/create.sh ${RESOURCEGROUP}
        if [[ -n "$TEST_IN_PRODUCTION" ]]; then
          make e2e-prod
        fi

    # Performs cleanup of all created resources
    - name: teardown
      image: ${LOCAL_IMAGE_BIN}
      imagePullPolicy: Always
      volumeMounts:
      - name: shared-tmp
        mountPath: /tmp/shared
      - name: cluster-secrets-azure
        mountPath: /etc/azure/credentials
      - name: artifacts
        mountPath: /tmp/artifacts
      env:
      - name: RESOURCEGROUP
        value: e2e-${RESOURCEGROUP_SUFFIX}
      - name: TYPE
        value: ${CLUSTER_TYPE}
      - name: HOME
        value: /tmp/shared/home
      - name: TEST_IN_PRODUCTION
        value: ${TEST_IN_PRODUCTION}
      command:
      - /bin/bash
      - -c
      - |
        #!/bin/bash

        # teardown is collecting debug data and deleting all used resources
        function teardown() {
          set +e
          mkdir -p "${HOME}"
          export HOME=/tmp/shared
          export DNS_DOMAIN=osadev.cloud
          export DNS_RESOURCEGROUP=dns

          cd /go/src/github.com/openshift/openshift-azure/
          source /etc/azure/credentials/secret
          az login --service-principal -u ${AZURE_CLIENT_ID} -p ${AZURE_CLIENT_SECRET} --tenant ${AZURE_TENANT_ID} &>/dev/null

          # Gather artifacts
          if [[ -z "$TEST_IN_PRODUCTION" ]]; then
            export KUBECONFIG=/tmp/shared/openshift-azure/_data/_out/admin.kubeconfig
            cp -r /tmp/shared/openshift-azure/_data /go/src/github.com/openshift/openshift-azure/
            oc get po --all-namespaces -o wide > /tmp/artifacts/pods
            oc get no -o wide > /tmp/artifacts/nodes
            oc get events --all-namespaces > /tmp/artifacts/events
            oc logs sync-master-000000 -n kube-system > /tmp/artifacts/sync.log
            oc logs master-api-master-000000 -n kube-system > /tmp/artifacts/api-master-000000.log
            oc logs master-api-master-000001 -n kube-system > /tmp/artifacts/api-master-000001.log
            oc logs master-api-master-000002 -n kube-system > /tmp/artifacts/api-master-000002.log
            oc logs master-etcd-master-000000 -n kube-system > /tmp/artifacts/etcd-master-000000.log
            oc logs master-etcd-master-000001 -n kube-system > /tmp/artifacts/etcd-master-000001.log
            oc logs master-etcd-master-000002 -n kube-system > /tmp/artifacts/etcd-master-000002.log
            cm_leader=$(oc get cm -n kube-system kube-controller-manager -o yaml | grep -o 00000[0-3])
            oc logs controllers-master-$cm_leader -n kube-system > /tmp/artifacts/controller-manager.log
          fi

          ./hack/delete.sh ${RESOURCEGROUP}
        }

        trap 'teardown' EXIT
        trap 'kill $(jobs -p); exit 0' TERM

        # teardown is triggered on file marker
        for i in `seq 1 120`; do
          if [[ -f /tmp/shared/exit ]]; then
            exit 0
          fi
          sleep 60 & wait
        done
