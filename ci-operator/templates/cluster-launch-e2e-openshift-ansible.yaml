# TODO: This file has the wrong name and needs to be refactored
kind: Template
apiVersion: template.openshift.io/v1

parameters:
- name: JOB_NAME_SAFE
  required: true
- name: JOB_NAME_HASH
  required: true
- name: NAMESPACE
  required: true
- name: IMAGE_FORMAT
  required: true
- name: IMAGE_ANSIBLE
  required: true
- name: IMAGE_TESTS
  required: true
- name: RPM_REPO
  required: true
- name: CLUSTER_TYPE
  required: true
- name: TEST_COMMAND

objects:

# We want the cluster to be able to access these images
- kind: RoleBinding
  apiVersion: authorization.openshift.io/v1
  metadata:
    name: ${JOB_NAME_SAFE}-image-puller
    namespace: ${NAMESPACE}
  roleRef:
    name: system:image-puller
  subjects:
  - kind: SystemGroup
    name: system:unauthenticated

# The e2e pod spins up a cluster, runs e2e tests, and then cleans up the cluster.
- kind: Pod
  apiVersion: v1
  metadata:
    name: ${JOB_NAME_SAFE}
    namespace: ${NAMESPACE}
    annotations:
      # we want to gather the teardown logs no matter what
      ci-operator.openshift.io/wait-for-container-artifacts: teardown
  spec:
    restartPolicy: Never
    activeDeadlineSeconds: 10800
    terminationGracePeriodSeconds: 600
    volumes:
    - name: artifacts
      emptyDir: {}
    - name: shared-tmp
      emptyDir: {}
    - name: shared-inventory
      emptyDir: {}
    - name: cluster-profile
      secret:
        secretName: ${JOB_NAME_SAFE}-cluster-profile

    containers:

    # Once admin.kubeconfig exists, executes shared tests
    - name: test
      image: ${IMAGE_TESTS}
      resources:
        requests:
          cpu: 1
          memory: 300Mi
        limits:
          cpu: 3
          memory: 4Gi
      volumeMounts:
      - name: shared-tmp
        mountPath: /tmp/shared
      - name: cluster-profile
        mountPath: /tmp/cluster
      - name: artifacts
        mountPath: /tmp/artifacts
      env:
      - name: HOME
        value: /tmp/home
      - name: TEST_COMMAND
        value: "${TEST_COMMAND}"
      command:
      - /bin/bash
      - -c
      - |
        #!/bin/bash
        set -euxo pipefail

        trap 'touch /tmp/shared/exit' EXIT
        trap 'kill $(jobs -p); exit 0' TERM

        cp "$(which oc)" /tmp/shared/

        mkdir -p "${HOME}"

        # wait until the setup job creates admin.kubeconfig
        while true; do
          if [[ ! -f /tmp/shared/admin.kubeconfig ]]; then
            sleep 15 & wait
            continue
          fi
          break
        done
        echo "Found shared kubeconfig"

        # don't let clients impact the global kubeconfig
        cp /tmp/shared/admin.kubeconfig /tmp/admin.kubeconfig
        export KUBECONFIG=/tmp/admin.kubeconfig

        PATH=/usr/libexec/origin:$PATH

        # set up cloud provider specific env vars
        if [[ "${CLUSTER_TYPE}" == "gcp" ]]; then
          export GOOGLE_APPLICATION_CREDENTIALS="/tmp/cluster/gce.json"
          export KUBE_SSH_USER=cloud-user
          mkdir -p ~/.ssh
          cp /tmp/cluster/ssh-privatekey ~/.ssh/google_compute_engine || true
          export PROVIDER_ARGS='-provider=gce -gce-zone=us-east1-c -gce-project=openshift-gce-devel-ci'
        fi

        mkdir -p /tmp/output
        cd /tmp/output

        # install telemetry client (TEMPORARY)
        set +e
        curl https://raw.githubusercontent.com/openshift/telemeter/master/deploy/default-rules > /tmp/rules
        oc create ns openshift-telemetry
        oc adm policy add-cluster-role-to-user cluster-reader -z default -n openshift-telemetry
        oc create secret generic telemeter-client --from-literal=to=https://telemeter.svc.ci.openshift.org --from-literal=id=${NAMESPACE}-${JOB_NAME_HASH} --from-literal=token=token-for-ci-test-job --from-literal=salt=random-salt --from-file=rules=/tmp/rules --dry-run -o yaml | oc apply -n openshift-telemetry -f -
        oc create -f https://raw.githubusercontent.com/openshift/telemeter/master/deploy/telemeter-client-openshift.yaml -n openshift-telemetry
        set -e

        # TODO: the test binary should really be a more structured command - most of these flags should be
        #       autodetected from the running cluster.
        # TODO: bump nodes up to 40 again
        function run-tests() {
          if [[ -n "${TEST_FOCUS}" ]]; then
            ginkgo -v -noColor -nodes=30 $( which extended.test ) -- \
              -ginkgo.focus="${TEST_FOCUS}" -ginkgo.skip="${TEST_SKIP:-"\\[local\\]"}" \
              -e2e-output-dir /tmp/artifacts -report-dir /tmp/artifacts/junit \
              -test.timeout=2h ${PROVIDER_ARGS-} || rc=$?
          fi
          if [[ -n "${TEST_FOCUS_SERIAL}" ]]; then
            ginkgo -v -noColor -nodes=1 $( which extended.test ) -- \
              -ginkgo.focus="${TEST_FOCUS_SERIAL}" -ginkgo.skip="${TEST_SKIP_SERIAL:-"\\[local\\]"}" \
              -e2e-output-dir /tmp/artifacts -report-dir /tmp/artifacts/junit/serial \
              -test.timeout=2h ${PROVIDER_ARGS-} || rc=$?
          fi
          exit ${rc:-0}
        }

        ${TEST_COMMAND}

    # Runs an install
    - name: setup
      image: ${IMAGE_ANSIBLE}
      volumeMounts:
      - name: shared-tmp
        mountPath: /tmp/shared
      - name: shared-inventory
        mountPath: /usr/share/ansible/openshift-ansible/test/ci/inventory
      - name: artifacts
        mountPath: /tmp/artifacts
      - name: cluster-profile
        mountPath: /usr/local/e2e-aws-cluster-profile
      env:
      - name: INSTANCE_PREFIX
        value: ${NAMESPACE}-${JOB_NAME_HASH}
      - name: TYPE
        value: ${CLUSTER_TYPE}
      - name: ANSIBLE_STDOUT_CALLBACK
        value: yaml
      - name: HOME
        value: /tmp
      - name: RPM_REPO
        value: ${RPM_REPO}
      command:
      - /usr/local/bin/entrypoint-provider
      args:
      - /bin/bash
      - -c
      - |
        #!/bin/bash
        set -euo pipefail

        trap 'rc=$?; gzip /tmp/artifacts/*.log; if [[ $rc -ne 0 ]]; then touch /tmp/shared/exit; fi; exit $rc' EXIT
        trap 'kill $(jobs -p); exit 0' TERM

        cd /usr/share/ansible/openshift-ansible/
        mkdir -p test/ci/inventory/group_vars/OSEv3
        echo "localhost ansible_connection=local" > test/ci/inventory/hosts
        cp -f /usr/local/e2e-aws-cluster-profile/vars.yaml test/ci/
        cp -f /usr/local/e2e-aws-cluster-profile/vars-origin.yaml test/ci/inventory/group_vars/OSEv3

        mkdir ~/.ssh
        cp -f /usr/local/e2e-aws-cluster-profile/ssh-privatekey ~/.ssh/id_rsa
        chmod 0600 ~/.ssh/id_rsa
        cp -f /usr/local/e2e-aws-cluster-profile/.awscred /etc/boto.cfg

        cd /usr/share/ansible/openshift-ansible
        ansible-playbook -vvv \
          -i test/ci/inventory \
          test/ci/launch.yml | tee -a /tmp/artifacts/ansible.log

        ansible -vvv \
          -i test/ci/inventory masters \
          -m fetch -a 'src=/etc/origin/master/admin.kubeconfig dest=/tmp/shared/ flat=true'

    # Performs cleanup of all created resources
    - name: teardown
      image: ${IMAGE_ANSIBLE}
      volumeMounts:
      - name: shared-tmp
        mountPath: /tmp/shared
      - name: shared-inventory
        mountPath: /usr/share/ansible/openshift-ansible/test/ci/inventory
      - name: artifacts
        mountPath: /tmp/artifacts
      - name: cluster-profile
        mountPath: /usr/local/e2e-aws-cluster-profile
      env:
      - name: INSTANCE_PREFIX
        value: ${NAMESPACE}-${JOB_NAME_HASH}
      - name: TYPE
        value: ${CLUSTER_TYPE}
      command:
      - /usr/local/bin/entrypoint-provider
      args:
      - /bin/bash
      - -c
      - |
        #!/bin/bash
        function teardown() {
          set +e
          touch /tmp/shared/exit

          echo "Deprovisioning cluster ..."
          mkdir ~/.ssh
          cp -f /usr/local/e2e-aws-cluster-profile/ssh-privatekey ~/.ssh/id_rsa
          chmod 0600 ~/.ssh/id_rsa
          cp -f /usr/local/e2e-aws-cluster-profile/.awscred /etc/boto.cfg

          cd /usr/share/ansible/openshift-ansible/
          echo "localhost ansible_connection=local" > test/ci/inventory/hosts
          cp -f /usr/local/e2e-aws-cluster-profile/vars.yaml test/ci/vars.yml
          ansible-playbook -vvv -i test/ci/inventory test/ci/deprovision.yml
        }

        trap 'teardown' EXIT
        trap 'kill $(jobs -p); exit 0' TERM

        for i in `seq 1 120`; do
          if [[ -f /tmp/shared/exit ]]; then
            exit 0
          fi
          sleep 60 & wait
        done
