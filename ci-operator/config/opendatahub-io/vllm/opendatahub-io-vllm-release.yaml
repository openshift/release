build_root:
  image_stream_tag:
    name: release
    namespace: openshift
    tag: golang-1.13
images:
- build_args:
  - name: DOCKER_BUILDKIT
    value: "1"
  - name: BUILDKIT_PROGRESS
    value: plain
  - name: max_jobs
    value: "8"
  - name: nvcc_threads
    value: "2"
  - name: TORCH_CUDA_ARCH_LIST
    value: 7.0 7.5 8.0 8.6 8.9 9.0+PTX
  - name: PYTHON_VERSION
    value: "3.11"
  context_dir: .
  dockerfile_path: Dockerfile.ubi
  to: vllm-build-release-cuda
promotion:
  to:
  - namespace: opendatahub-io
    tag: stable
releases:
  latest:
    release:
      architecture: amd64
      channel: stable
      version: "4.12"
resources:
  '*':
    requests:
      cpu: 200m
      memory: 200Mi
  smoke-test:
    limits:
      cpu: "4"
      memory: 24Gi
      nvidia.com/gpu: "1"
    requests:
      cpu: "4"
      memory: 8Gi
      nvidia.com/gpu: "1"
  vllm-build-release-cuda:
    limits:
      cpu: "16"
      memory: 24Gi
    requests:
      cpu: "16"
      memory: 24Gi
tests:
- always_run: false
  as: smoke-test
  cluster: build05
  commands: |
    set -euxo pipefail

    # we will need to download test models off HF hub
    unset HF_HUB_OFFLINE
    # spin up the server and run it in the background, allowing for images download
    python -m vllm.entrypoints.openai.api_server &
    server_pid=$!

    # wait for the server to be up
    sleep 60

    # get grpcurl
    curl --no-progress-meter --location --output grpcurl.tar.gz \
      https://github.com/fullstorydev/grpcurl/releases/download/v1.9.1/grpcurl_1.9.1_linux_x86_64.tar.gz
    tar -xf grpcurl.tar.gz

    # get grpc proto
    curl --no-progress-meter --location --remote-name \
      https://github.com/opendatahub-io/text-generation-inference/raw/main/proto/generation.proto


    # OpenAI API tests
    curl -v --no-progress-meter --fail-with-body \
      localhost:8000/v1/models | python -m json.tool || \
      (kill -9 $server_pid && exit 1)

    curl -v --no-progress-meter --fail-with-body \
      --header "Content-Type: application/json" \
      --data '{
        "prompt": "A red fedora symbolizes ",
        "model": "facebook/opt-125m"
    }' \
      localhost:8000/v1/completions | python -m json.tool || \
        (kill -9 $server_pid && exit 1)

    echo "OpenAI API success"


    # GRPC API test
    ./grpcurl -v \
        -plaintext \
        -proto generation.proto \
        -d '{ "requests": [{"text": "A red fedora symbolizes "}]}' \
        localhost:8033 \
        fmaas.GenerationService/Generate || (kill -9 $server_pid && exit 1)

    echo "GRPC API success"

    kill -9 $server_pid
  container:
    clone: false
    from: vllm-build-release-cuda
- as: pr-image-mirror
  steps:
    dependencies:
      SOURCE_IMAGE_REF: vllm-build-release-cuda
    env:
      IMAGE_REPO: vllm
    workflow: opendatahub-io-ci-image-mirror
- as: release-image-mirror
  postsubmit: true
  steps:
    dependencies:
      SOURCE_IMAGE_REF: vllm-build-release-cuda
    env:
      IMAGE_REPO: vllm
      RELEASE_VERSION: stable
    workflow: opendatahub-io-ci-image-mirror
zz_generated_metadata:
  branch: release
  org: opendatahub-io
  repo: vllm
