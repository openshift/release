base_images:
  base:
    name: "4.10"
    namespace: ocp
    tag: base
  cli:
    name: "4.10"
    namespace: ocp
    tag: cli
  openshift_release_golang-1.17:
    name: release
    namespace: openshift
    tag: golang-1.17
binary_build_commands: make manager
build_root:
  image_stream_tag:
    name: release
    namespace: openshift
    tag: golang-1.17
images:
- build_args:
  - name: ARCH
    value: amd64
  - name: ldflags
  dockerfile_literal: |-
    FROM registry.ci.openshift.org/openshift/release:golang-1.17 as toolchain

    # Run this with docker build --build_arg $(go env GOPROXY) to override the goproxy
    ARG goproxy=https://proxy.golang.org
    ENV GOPROXY=$goproxy
    ENV GOFLAGS="-mod=readonly"

    FROM toolchain as builder
    WORKDIR /workspace

    # Copy the sources
    COPY ./ ./
    RUN go mod download

    # Build
    ARG package=.
    ARG ARCH
    ARG LDFLAGS
    RUN CGO_ENABLED=0 GOOS=linux GOARCH=${ARCH} go build -ldflags "${LDFLAGS} -extldflags '-static'"  -o manager ${package}
    ENTRYPOINT [ "/start.sh", "/workspace/manager" ]

    # Copy the controller-manager into a thin image
    FROM gcr.io/distroless/static:nonroot
    WORKDIR /
    COPY --from=builder /workspace/manager .
    # Use uid of nonroot user (65532) because kubernetes expects numeric user when applying pod security policies
    USER 65532
    ENTRYPOINT ["/manager"]
  from: base
  inputs:
    openshift_release_golang-1.17:
      as:
      - registry.ci.openshift.org/openshift/release:golang-1.17
  to: cluster-api-kubevirt-controller
- build_args:
  - name: ARCH
    value: amd64
  - name: ldflags
  dockerfile_literal: |-
    FROM src

    RUN yum install -y expect
    COPY oc /usr/bin/oc

    RUN curl -L https://github.com/kubernetes-sigs/cluster-api/releases/download/v1.0.2/clusterctl-linux-amd64 -o /usr/bin/clusterctl && \
        chmod +x /usr/bin/clusterctl
  from: src
  inputs:
    cli:
      paths:
      - destination_dir: .
        source_path: /usr/bin/oc
  to: capk-oc-bin-image
promotion:
  namespace: hypershift
  tag: main
releases:
  initial:
    integration:
      name: "4.10"
      namespace: ocp
  latest:
    integration:
      include_built_images: true
      name: "4.10"
      namespace: ocp
resources:
  '*':
    limits:
      memory: 4Gi
    requests:
      cpu: 100m
      memory: 200Mi
tests:
- as: unit-tests
  commands: GOFLAGS="-mod=readonly" make test
  container:
    from: src
- as: cluster-api-provider-kubevirt-e2e
  steps:
    cluster_profile: gcp
    test:
    - as: deploy-cnv
      commands: |-
        oc apply -f - <<EOF
        apiVersion: v1
        kind: Namespace
        metadata:
          name: openshift-cnv
        EOF

        oc apply -f - <<EOF
        apiVersion: operators.coreos.com/v1
        kind: OperatorGroup
        metadata:
          name: openshift-cnv-group
          namespace: openshift-cnv
        spec:
          targetNamespaces:
          - openshift-cnv
        EOF

        cat <<EOF | oc apply -f -
        apiVersion: operators.coreos.com/v1alpha1
        kind: Subscription
        metadata:
          labels:
            operators.coreos.com/kubevirt-hyperconverged.openshift-cnv: ''
          name: kubevirt-hyperconverged
          namespace: openshift-cnv
        spec:
          channel: stable
          installPlanApproval: Automatic
          name: kubevirt-hyperconverged
          source: redhat-operators
          sourceNamespace: openshift-marketplace
        EOF

        sleep 60

        RETRIES=30
        CSV=
        for i in $(seq ${RETRIES}); do
          if [[ -z ${CSV} ]]; then
            CSV=$(oc get subscription -n openshift-cnv kubevirt-hyperconverged -o jsonpath='{.status.installedCSV}')
          fi

          if [[ -z ${CSV} ]]; then
            echo "Try ${i}/${RETRIES}: can't get the CSV yet. Checking again in 30 seconds"
            sleep 30
          fi

          if [[ $(oc get csv -n openshift-cnv ${CSV} -o jsonpath={.status.phase}) == "Succeeded" ]]; then
            echo "CNV is deployed"
            break
          else
            echo "Try ${i}/${RETRIES}: CNV is not deployed yet. Checking again in 30 seconds"
            sleep 30
          fi
        done

        if [[ $(oc get csv -n openshift-cnv ${CSV} -o jsonpath={.status.phase}) != "Succeeded" ]]; then
          echo "Error: Failed to deploy CNV"
          echo "CSV ${CSV} YAML"
          oc get CSV ${CSV} -n openshift-cnv -o yaml
          echo
          echo "CSV ${CSV} Describe"
          oc describe CSV ${CSV} -n openshift-cnv
          exit 1
        fi

        oc create -f - <<EOF
        apiVersion: hco.kubevirt.io/v1beta1
        kind: HyperConverged
        metadata:
          name: kubevirt-hyperconverged
          namespace: openshift-cnv
        EOF

        oc wait hyperconverged -n openshift-cnv kubevirt-hyperconverged --for=condition=Available --timeout=15m
      from: capk-oc-bin-image
      resources:
        requests:
          cpu: 500m
          memory: 1Gi
    - as: deploy-capk
      commands: |-
        set -x
        # Deploy cluster API
        cat <<EOF > clusterctl_config.yaml
        cert-manager:
          url: "https://github.com/cert-manager/cert-manager/releases/latest/cert-manager.yaml"
        EOF

        clusterctl init -v 4 --config=clusterctl_config.yaml

        oc adm policy add-scc-to-user privileged system:serviceaccount:capk-system:capk-manager

        export MANIFEST_IMG=${CAPK_IMAGE%:*}
        export MANIFEST_TAG=${CAPK_IMAGE/*:}
        GOFLAGS="-mod=readonly" make generate-manifests

        sed -i -E "s|(^ +- image: ).*$|\1${CAPK_IMAGE}|" config/default/manager_image_patch.yaml

        oc kustomize config/default | oc apply -f -
        sleep 60

        # Validate capk manager was created successfully
        oc wait pod --for=condition=Ready -l cluster.x-k8s.io/provider=infrastructure-kubevirt -n capk-system --timeout=120s
      dependencies:
      - env: CAPK_IMAGE
        name: cluster-api-kubevirt-controller
      from: capk-oc-bin-image
      resources:
        requests:
          cpu: 500m
          memory: 1Gi
    - as: create-cluster
      commands: |-
        set -x
        sed -i -E "/controlPlaneServiceTemplate:/{ n; /spec:/ { n; /type:/ { s/ClusterIP/LoadBalancer/; p; d;}}}" templates/cluster-template.yaml

        export NODE_VM_IMAGE_TEMPLATE=quay.io/kubevirtci/fedora-kubeadm:35
        export IMAGE_REPO=k8s.gcr.io
        export CRI_PATH="/var/run/crio/crio.sock"
        export CLUSTER_NAME=tenant-capk
        clusterctl generate cluster ${CLUSTER_NAME} \
                   --kubernetes-version v1.21.0 \
                   --control-plane-machine-count=1 \
                   --worker-machine-count=1 \
                   --from templates/cluster-template.yaml \
                   -n capk-system | oc apply -f -
      from: capk-oc-bin-image
      resources:
        requests:
          cpu: 500m
          memory: 1Gi
    - as: validate-cluster
      commands: |-
        set -x
        export CLUSTER_NAME=tenant-capk
        export CLUSTER_NS=capk-system

        sleep 60

        oc wait kubevirtcluster -l cluster.x-k8s.io/cluster-name=${CLUSTER_NAME} --for=condition=Ready -n ${CLUSTER_NS} --timeout=120s

        RETRIES=20
        VMS_EXISTS=false
        for i in $(seq ${RETRIES}); do
          if [[ $(oc get vm -n ${CLUSTER_NS} --no-headers | wc -l) -lt 2 ]]; then
            sleep 30
            echo "Try ${i}/${RETRIES}: not all the VMs were created, yet; trying again in 30 seconds"
            continue
          else
            VMS_EXISTS=true
            break
          fi
        done

        if [[ ${VMS_EXISTS} == "false" ]]; then
          echo "should create at least 2 VMs, but it didn't"
          exit 1
        fi

        #check VMs
        for VM in $(oc get vm -n ${CLUSTER_NS} -o custom-columns=NAME:.metadata.name --no-headers); do
          oc wait vm -n ${CLUSTER_NS} ${VM} --for=condition=Ready
        done

        #check VMIs
        for VMI in $(oc get vmi -n ${CLUSTER_NS} -o custom-columns=NAME:.metadata.name --no-headers); do
          oc wait vmi -n ${CLUSTER_NS} ${VMI} --for=condition=Ready
          [[ $(oc get vmi -n ${CLUSTER_NS} ${VMI} -o custom-columns=PHASE:.status.phase --no-headers) == "Running" ]]
        done

        oc get vmi -n ${CLUSTER_NS}

        TENANT_CLUSTER_IP=$(oc get svc -n ${CLUSTER_NS} -l cluster.x-k8s.io/cluster-name=tenant-capk -o custom-columns=EXT_IP:.status.loadBalancer.ingress[0].ip --no-headers)
        clusterctl get kubeconfig ${CLUSTER_NAME} -n ${CLUSTER_NS} > ${CLUSTER_NAME}-kubeconfig
        sed -i -E "/certificate-authority-data/d;s|(server: https://)[^:]+|\1${TENANT_CLUSTER_IP}|;/^- cluster:/a\    insecure-skip-tls-verify: true" ${CLUSTER_NAME}-kubeconfig

        # check the tenant cluster. Check if the API-Server is responsive
        MNGMNT_KUBECONFI=${KUBECONFIG}
        export KUBECONFIG=${CLUSTER_NAME}-kubeconfig

        RETRIES=20
        API_SERVER_IS_UP=false
        for i in $(seq ${RETRIES}); do
          if [[ $(oc get --raw='/readyz') ]]; then
            API_SERVER_IS_UP=true
            break
          else
            echo "Try ${i}/${RETRIES}: tenant API Server is not up, yet; trying again in 30 seconds"
            sleep 30
            continue
          fi
        done

        if [[ ${API_SERVER_IS_UP} == "false" ]]; then
          echo "Should have a responsive tenenat API server by now; aborting..."
          exit 1
        fi

        RETRIES=20
        NODES_ARE_UP=false
        for i in $(seq ${RETRIES}); do
          if [[ $(oc get nodes --no-headers | wc -l) -lt 2 ]]; then
            echo "Try ${i}/${RETRIES}: not all the nodes are up, yet; trying again in 30 seconds"
            sleep 30
            continue
          else
            NODES_ARE_UP=true
            break
          fi
        done

        if [[ ${NODES_ARE_UP} == "false" ]]; then
          echo "Should have two working nodes by now; aborting..."
          exit 1
        fi

        oc get nodes -o wide

        oc config set-context $(oc config current-context) --namespace=default

        # create a workload on the tenant cluster
        oc run testpod --image=nginx
        oc wait pod testpod --for=condition=Ready --timeout=90s
        oc get pods
      from: capk-oc-bin-image
      resources:
        requests:
          cpu: 500m
          memory: 1Gi
    workflow: ipi-gcp
zz_generated_metadata:
  branch: main
  org: openshift
  repo: cluster-api-provider-kubevirt
