build_root:
  project_image:
    dockerfile_literal: |
      FROM registry.ci.openshift.org/ocp/ubi-python-311:9
      USER 0
images:
- dockerfile_path: Containerfile
  to: lightspeed-service-api
- dockerfile_path: Containerfile.rag
  to: lightspeed-service-api-rag
- dockerfile_path: Containerfile.redis
  to: lightspeed-service-redis
promotion:
  to:
  - namespace: ols
    tag: latest
releases:
  initial:
    integration:
      name: "4.15"
      namespace: ocp
  latest:
    integration:
      include_built_images: true
      name: "4.15"
      namespace: ocp
resources:
  '*':
    limits:
      memory: 4Gi
    requests:
      cpu: 100m
      memory: 200Mi
tests:
- as: verify
  commands: pip install black ruff mypy && make verify && make check-types
  container:
    from: src
- as: unit
  commands: |
    export CODECOV_TOKEN=$(cat /tmp/secret/CODECOV_TOKEN)
    make install-deps && make install-deps-test && make test-unit
  container:
    from: src
  secret:
    mount_path: /tmp/secret
    name: ols-codecov-token
- as: integration
  commands: |
    export CODECOV_TOKEN=$(cat /tmp/secret/CODECOV_TOKEN)
    make install-deps && make install-deps-test && make test-integration
  container:
    from: src
  secret:
    mount_path: /tmp/secret
    name: ols-codecov-token
- as: e2e-ols-bam-granite13b-chat-v2
  cron: '@daily'
  presubmit: true
  run_if_changed: ^(Containerfile.*|Makefile|manifests/.*|ols/.*|pyproject.toml|pdm.lock|scripts/.*|tests/.*)$
  steps:
    test:
    - as: e2e
      commands: |
        export PROVIDER_KEY_PATH=/var/run/bam/token
        export PROVIDER=bam
        export MODEL=ibm/granite-13b-chat-v2
        scripts/test-e2e-standalone.sh
      credentials:
      - mount_path: /var/run/bam
        name: bam-apitoken
        namespace: test-credentials
      from: src
      grace_period: 5ns
      resources:
        requests:
          cpu: 100m
- always_run: false
  as: e2e-ols-openai-gpt35
  cron: '@daily'
  presubmit: true
  steps:
    test:
    - as: e2e
      commands: |
        export PROVIDER_KEY_PATH=/var/run/openai/token
        export PROVIDER=openai
        export MODEL=gpt-3.5-turbo-1106
        scripts/test-e2e-standalone.sh
      credentials:
      - mount_path: /var/run/openai
        name: openai-apitoken
        namespace: test-credentials
      from: src
      grace_period: 5ns
      resources:
        requests:
          cpu: 100m
- always_run: false
  as: e2e-ols-watsonx-granite13b-chat-v2
  cron: '@daily'
  presubmit: true
  steps:
    test:
    - as: e2e
      commands: |
        export PROVIDER_KEY_PATH=/var/run/watsonx/token
        export PROVIDER=watsonx
        export PROVIDER_PROJECT_ID=ad629765-c373-4731-9d69-dc701724c081
        export MODEL=ibm/granite-13b-chat-v2
        scripts/test-e2e-standalone.sh
      credentials:
      - mount_path: /var/run/watsonx
        name: watsonx-apitoken
        namespace: test-credentials
      from: src
      grace_period: 5ns
      resources:
        requests:
          cpu: 100m
zz_generated_metadata:
  branch: main
  org: openshift
  repo: lightspeed-service
