base_images:
  base:
    name: release
    namespace: openshift
    tag: golang-1.13
releases:
  latest:
    integration:
      name: "4.10"
      namespace: ocp
resources:
  '*':
    limits:
      memory: 4Gi
    requests:
      cpu: 100m
      memory: 200Mi
tests:
- as: e2e-compute
  cron: 0 2 * * *
  steps:
    cluster_profile: azure4
    test:
    - as: enable-cpu-manager
      cli: latest
      commands: |
        curl -L "https://raw.githubusercontent.com/dhiller/kubevirt-testing/main/hack/kubevirt-testing.sh" | \
          bash -s enable_cpu_manager
      from: base
      resources:
        requests:
          cpu: 100m
          memory: 200Mi
    - as: deploy-kubevirt
      cli: latest
      commands: |
        curl -L "https://raw.githubusercontent.com/dhiller/kubevirt-testing/main/hack/kubevirt-testing.sh" | \
          bash -s deploy_nightly_test_setup
      from: base
      resources:
        requests:
          cpu: 100m
          memory: 200Mi
    - as: test
      cli: latest
      commands: |
        export DOCKER_PREFIX='quay.io/kubevirt'
        export KUBEVIRT_E2E_FOCUS='\[sig-compute\]'
        export KUBEVIRT_E2E_SKIP='QUARANTINE|test_id:(779|780|782|783|790|959|998|1466|1664|1665|1681|1783|1853|2221|2222|2224|2226|2303|2653|3095|3096|3123|3145|3203|3237|3239|3241|3242|3244|3245|3294|4023|4113|4114|4119|4135|4138|4140|4599|4607|4608|4746|5004|5360|6842|6843|6870|6965|6966|6967|6968|6969|6970|6971|6972|6973|6974|6975|6976|6977|6979|6980|6982|6981|6995|7299|7679)|VM Live Migration should replace containerdisk and kernel boot images with their reproducible digest during migration|VM Live Migration with CPU pinning and huge pages and NUMA passthrough should not make migrations fail|VM Live Migration with CPU pinning and huge pages should not make migrations fail|HostDevices with ephemeral disk Should successfully passthrough an emulated PCI device|MediatedDevices with mediated devices configuration Should successfully passthrough a mediated device with a disabled display|VirtualMachine A valid VirtualMachine given should report an error status when data volume error occurs|VMI with external kernel boot with external alpine-based kernel & initrd images ensure successful boot|Migration should generate empty isos of the right size on the target|MediatedDevices with mediated devices configuration Should successfully passthrough a mediated device|should not start with hook sidecar annotation when the version is not provided|VirtualMachinePool should remove VMs once they are marked for deletion|VirtualMachinePool should handle pool with dataVolumeTemplates|VirtualMachinePool should replace deleted VM and get replacement|VirtualMachinePool should roll out VM template changes without impacting VMI|VirtualMachinePool should roll out VMI template changes and proactively roll out new VMIs|VirtualMachinePool should remove owner references on the VirtualMachine if it is orphan deleted|VirtualMachinePool should not scale when paused and scale when resume|pool should scale to three, to two and then to zero replicas|pool should scale to five, to six and then to zero replicas|VM Live Migration Starting a VirtualMachineInstance with an pending target pod should automatically cancel unschedulable migration after a timeout period|VM Live Migration Starting a VirtualMachineInstance with migration policies migration policy should override cluster-wide policy if defined|VM Live Migration Starting a VirtualMachineInstance with migration policies migration policy should not affect cluster-wide policy if not defined'
        export TEST_TIMEOUT="6h50m"
        curl -L "https://raw.githubusercontent.com/dhiller/kubevirt-testing/main/hack/kubevirt-testing.sh" | \
          bash -s test_nightly
      from: base
      resources:
        requests:
          cpu: 100m
          memory: 200Mi
      timeout: 7h0m0s
    workflow: ipi-azure
  timeout: 8h0m0s
- as: e2e-storage
  cron: 15 3 * * *
  steps:
    cluster_profile: azure4
    test:
    - as: enable-cpu-manager
      cli: latest
      commands: |
        curl -L "https://raw.githubusercontent.com/dhiller/kubevirt-testing/main/hack/kubevirt-testing.sh" | \
          bash -s enable_cpu_manager
      from: base
      resources:
        requests:
          cpu: 100m
          memory: 200Mi
    - as: deploy-kubevirt
      cli: latest
      commands: |
        curl -L "https://raw.githubusercontent.com/dhiller/kubevirt-testing/main/hack/kubevirt-testing.sh" | \
          bash -s deploy_nightly_test_setup
      from: base
      resources:
        requests:
          cpu: 100m
          memory: 200Mi
    - as: test
      cli: latest
      commands: |
        export DOCKER_PREFIX='quay.io/kubevirt'
        export KUBEVIRT_E2E_FOCUS='\[sig-storage\]'
        export KUBEVIRT_E2E_SKIP='QUARANTINE|test_id:(836|837|838|851|868|1015|2226|2306|3057|3107|3130|3131|3132|3133|3136|3137|3138|3189|3190|3191|4611|4618|4619|5252|5259|5260|5261|5262|5263|6053|6480|6686|6766|6767|6769|6836|6837|6838|6949|6952|7425|7472)|Starting a VirtualMachine with a DataVolume using Alpine http import a DataVolume with preallocation shouldn.t have discard=unmap|Starting a VirtualMachine with an invalid DataVolume using DataVolume with invalid URL shold be possible to stop VM if datavolume is crashing|Hotplug hostpath should attach a hostpath based volume to running VM|Hotplug iothreads should allow adding and removing hotplugged volumes|Storage Starting a VirtualMachineInstance Run a VMI with VirtIO-FS and a datavolume should be successfully started and virtiofs could be accessed|Storage Starting a VirtualMachineInstance VirtIO-FS with an empty PVC should be successfully started and virtiofs could be accessed|Storage Starting a VirtualMachineInstance With a volumeMode block backed ephemeral disk should generate the block backingstore disk within the domain|Storage Starting a VirtualMachineInstance With a volumeMode block backed ephemeral disk should generate the pod with the volumeDevice|Storage Starting a VirtualMachineInstance with error disk|ImageUpload Create upload volume with force-bind flag Should succeed DataVolume|ImageUpload Create upload volume with force-bind flag Should succeed PVC|With a more complicated VM should restore vm with hot plug disks|With more complicated VM VM should contain snapshot status for all volumes|With more complicated VM should error if VolumeSnapshot deleted|With more complicated VM should not error if VolumeSnapshot has error|With more complicated VM should successfully recreate status|With online vm snapshot should succeed online snapshot with hot plug disk|Hotplug hostpath-separate-device should attach a hostpath based volume to running VM|Starting a VirtualMachineInstance with a DataVolume as a volume source \[Serial\]without fsgroup support should succesfully start|VirtualMachineRestore Tests \[rook-ceph\] With a more complicated VM should reject vm start if restore in progress|VirtualMachineRestore Tests \[rook-ceph\] With a more complicated VM should restore vm spec at startup without new changes|with Alpine PVC should be successfully started \[Serial\]with NFS Disk PVC using ipv4 address of the NFS pod not owned by qemu|Hotplug virtctl should add volume|Guestfs Run libguestfs on PVCs Should successfully run libguestfs-test-tool|Storage Starting a VirtualMachineInstance with lun disk should run the VMI|Hotplug virtctl should remove volume according to options with default|Hotplug virtctl should remove volume according to options with dry-run|ImageUpload Create upload archive volume Should succeed DataVolume|ImageUpload Create upload archive volume Should succeed PVC'
        curl -L "https://raw.githubusercontent.com/dhiller/kubevirt-testing/main/hack/kubevirt-testing.sh" | \
          bash -s test_nightly
      from: base
      resources:
        requests:
          cpu: 100m
          memory: 200Mi
      timeout: 4h0m0s
    workflow: ipi-azure
  timeout: 8h0m0s
- as: e2e-network
  cron: 30 4 * * *
  steps:
    cluster_profile: azure4
    test:
    - as: enable-cpu-manager
      cli: latest
      commands: |
        curl -L "https://raw.githubusercontent.com/dhiller/kubevirt-testing/main/hack/kubevirt-testing.sh" | \
          bash -s enable_cpu_manager
      from: base
      resources:
        requests:
          cpu: 100m
          memory: 200Mi
    - as: deploy-kubevirt
      cli: latest
      commands: |
        curl -L "https://raw.githubusercontent.com/dhiller/kubevirt-testing/main/hack/kubevirt-testing.sh" | \
          bash -s deploy_nightly_test_setup
      from: base
      resources:
        requests:
          cpu: 100m
          memory: 200Mi
    - as: test
      cli: latest
      commands: |
        export DOCKER_PREFIX='quay.io/kubevirt'
        export KUBEVIRT_E2E_FOCUS='\[sig-network\]'
        export KUBEVIRT_E2E_SKIP='QUARANTINE|test_id:(676|1535|1536|1542|1545|1546|1751|1780|3108)|Macvtap VMI migration should be successful when the VMI MAC address is defined in its spec|Macvtap VMI migration with live traffic should keep connectivity after a migration|Macvtap a virtual machine with one macvtap interface, with a custom MAC address and another virtual machine connected to the same network can communicate with the virtual machine in the same network|Macvtap a virtual machine with one macvtap interface, with a custom MAC address should have the specified MAC address reported back via the API|Subdomain with a headless service given VMI should have the expected FQDN with Bridge binding and subdomain|Subdomain with a headless service given VMI should have the expected FQDN with Masquerade binding and subdomain|VirtualMachineInstance using different types of interfaces\. Security Should allow outbound communication from VM under test - only if original MAC address is unchanged|Creating a VirtualMachineInstance when virt-handler is responsive VMIs with Bridge Networking should work with Duplicate Address Detection \(DAD\)|Starting a VirtualMachine with a DataVolume using Alpine http import a DataVolume with preallocation shouldn.t have discard=unmap|Networking VirtualMachineInstance with masquerade binding mechanism when performing migration \[Conformance\] preserves connectivity IPv4|Networking VirtualMachineInstance with masquerade binding mechanism when performing migration \[Conformance\] preserves connectivity IPv4 with explicit ports used by live migration|Networking VirtualMachineInstance with masquerade binding mechanism \[Serial\]with a dedicated migration network Should migrate over that network|Primary Pod Network Status VMI connected to the pod network using bridge binding when Guest Agent exists should report PodIP/s IPv4 as its own on interface status'
        curl -L "https://raw.githubusercontent.com/dhiller/kubevirt-testing/main/hack/kubevirt-testing.sh" | \
          bash -s test_nightly
      from: base
      resources:
        requests:
          cpu: 100m
          memory: 200Mi
      timeout: 4h0m0s
    workflow: ipi-azure
  timeout: 8h0m0s
zz_generated_metadata:
  branch: main
  org: kubevirt
  repo: kubevirt
  variant: nightly_4.10
