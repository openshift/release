workflow:
  as: redhat-chaos-pod-scenarios-prometheus-disruption
  steps:
    test:
      - ref: redhat-chaos-pod-scenarios-prometheus-disruption
    env:
      TARGET_NAMESPACE: "openshift-monitoring"
      POD_LABEL: "statefulset.kubernetes.io/pod-name=prometheus-k8s-0"
      DISRUPTION_COUNT: "1"
      EXPECTED_POD_COUNT: "1"
      KILL_TIMEOUT: "180"
      EXPECTED_RECOVERY_TIME: "65"
      ENABLE_ALERTS: "True"
      ALERTS_PATH: "/home/krkn/kraken/config/alerts_openshift.yaml"
      CHECK_CRITICAL_ALERTS: "True"
      WAIT_DURATION: "300"
      TELEMETRY_ENABLED: "True"
      TELEMETRY_API_URL: "https://ulnmf9xv7j.execute-api.us-west-2.amazonaws.com/production"
      TELEMETRY_USERNAME: "redhat-chaos"
      TELEMETRY_GROUP: "prow"
      TELEMETRY_RUN_TAG: "prometheus-pod-disruption"
      TELEMETRY_PROMETHEUS_BACKUP: "True"
      TELEMETRY_FULL_PROMETHEUS_BACKUP: "True"
      TELEMTRY_BACKUP_THREADS: "5"
      TELEMETRY_ARCHIVE_PATH: "/tmp"
      TELEMETRY_MAX_RETRIES: "0"
      TELEMETRY_ARCHIVE_SIZE: "1000000"
      TELEMETRY_LOGS_BACKUP: "True"
      TELEMTRY_CLI_PATH: "/usr/local/bin/oc"
      TELEMETRY_EVENTS_BACKUP: "True"
  documentation: |-
    This step runs the krkn-hub config workload to disrupt prometheus pod(s) and checks its recovery/health.
