ref:
  as: prow-failure-analysis
  from_image:
    namespace: devprod
    name: prow-failure-analysis
    tag: latest
  commands: prow-failure-analysis-commands.sh
  resources:
    requests:
      cpu: 100m
      memory: 512Mi
  env:
    - name: LLM_PROVIDER
      default: ""
      documentation: |-
        The LLM provider to use for analysis (e.g., openai, google, anthropic, ollama).
        Required. See prow-failure-analysis documentation for supported providers.
    - name: LLM_MODEL
      default: ""
      documentation: |-
        The LLM model to use for analysis (e.g., gpt-4, gemini-pro, claude-3-opus).
        Required. Must be compatible with the specified LLM_PROVIDER.
    - name: LLM_API_KEY_PATH
      default: ""
      documentation: |-
        Path to a file containing the LLM API key. Users must create a secret in the
        test-credentials namespace and mount it in their CI config. Example:
        credentials:
          - namespace: test-credentials
            name: my-llm-api-key
            mount_path: /tmp/secrets/llm
        Then set LLM_API_KEY_PATH=/tmp/secrets/llm/api_key
    - name: LLM_BASE_URL
      default: ""
      documentation: |-
        Optional base URL for the LLM API. Use this for custom endpoints or
        self-hosted models (e.g., Ollama).
    - name: GITHUB_TOKEN_PATH
      default: ""
      documentation: |-
        Optional path to a file containing a GitHub token for posting PR comments.
        If not provided, results will only be printed to the console.
        Users must create a secret and mount it similarly to LLM_API_KEY_PATH.
    - name: POST_COMMENT
      default: "false"
      documentation: |-
        Set to "true" to post the analysis results as a comment on the GitHub PR.
        Requires GITHUB_TOKEN_PATH to be set and valid.
    - name: GCS_BUCKET
      default: "test-platform-results"
      documentation: |-
        The GCS bucket containing Prow job artifacts. Defaults to test-platform-results
        which is the standard OpenShift CI bucket.
    - name: GCS_CREDS_PATH
      default: ""
      documentation: |-
        Optional path to GCS credentials file. If not specified, uses default
        application credentials or anonymous access for public buckets.
    - name: IGNORED_STEPS
      default: ""
      documentation: |-
        Comma-separated list of step name patterns to ignore during analysis.
        Useful for filtering out known noisy steps.
    - name: INCLUDED_ARTIFACTS
      default: ""
      documentation: |-
        Glob patterns for additional artifacts to include in analysis.
        Example: "*.yaml,cluster-state/*.json"
    - name: CORDON_DEVICE
      default: "cpu"
      documentation: |-
        Device to use for semantic log preprocessing (cpu, cuda, mps).
        Defaults to cpu. Use cuda or mps if GPU is available for faster processing.
    - name: CORDON_BACKEND
      default: "sentence-transformers"
      documentation: |-
        Backend to use for embeddings: "sentence-transformers", "llama-cpp", or "remote".
        Use "remote" with CORDON_API_KEY_PATH for hosted embedding APIs (OpenAI, Cohere, etc.).
    - name: CORDON_MODEL_NAME
      default: "all-MiniLM-L6-v2"
      documentation: |-
        Model name for embeddings. For sentence-transformers, use HuggingFace model names.
        For remote backends, use provider/model format (e.g., openai/text-embedding-3-small).
    - name: CORDON_API_KEY_PATH
      default: ""
      documentation: |-
        Path to a file containing the embedding API key for remote backends.
        Required when CORDON_BACKEND is set to "remote". Mount a secret similar to LLM_API_KEY_PATH.
    - name: CORDON_ENDPOINT
      default: ""
      documentation: |-
        Optional custom endpoint URL for remote embedding API.
        Only needed for custom/self-hosted OpenAI-compatible endpoints.
        Standard providers (OpenAI, Cohere, Gemini) use their default endpoints automatically.
    - name: CORDON_BATCH_SIZE
      default: "32"
      documentation: |-
        Batch size for embedding operations. Higher values = faster processing but more memory.
        Default is 32. Increase for faster processing if memory allows.
    - name: VERBOSE
      default: "false"
      documentation: |-
        Set to "true" to enable verbose logging output.
  documentation: |-
    Analyzes Prow CI job failures using AI-powered root cause analysis.

    This step uses the prow-failure-analysis tool to:
    1. Fetch build artifacts and logs from GCS
    2. Parse test results and failed step logs
    3. Use semantic anomaly detection to extract relevant failure information
    4. Analyze failures using LLMs to identify root causes
    5. Generate a concise root cause analysis report
    6. Optionally post results as a GitHub PR comment

    PREREQUISITES:
    Users must provide their own LLM API credentials by creating a secret
    in the test-credentials namespace and mounting it in their CI config.

    Example CI config usage (local embeddings):
    ```yaml
    tests:
    - as: my-test
      steps:
        credentials:
          - namespace: test-credentials
            name: my-llm-credentials
            mount_path: /tmp/secrets/llm
        env:
          LLM_PROVIDER: openai
          LLM_MODEL: gpt-4
          LLM_API_KEY_PATH: /tmp/secrets/llm/api_key
        post:
          - ref: prow-failure-analysis
    ```

    Example with remote embeddings (faster, no local model download):
    ```yaml
    tests:
    - as: my-test
      steps:
        credentials:
          - namespace: test-credentials
            name: my-llm-credentials
            mount_path: /tmp/secrets/llm
          - namespace: test-credentials
            name: my-embedding-credentials
            mount_path: /tmp/secrets/embedding
        env:
          LLM_PROVIDER: openai
          LLM_MODEL: gpt-4
          LLM_API_KEY_PATH: /tmp/secrets/llm/api_key
          CORDON_BACKEND: remote
          CORDON_MODEL_NAME: openai/text-embedding-3-small
          CORDON_API_KEY_PATH: /tmp/secrets/embedding/api_key
        post:
          - ref: prow-failure-analysis
    ```

    For more information, see: https://github.com/redhat-community-ai-tools/prow-failure-analysis
