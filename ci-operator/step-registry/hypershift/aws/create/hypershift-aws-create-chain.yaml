chain:
  as: hypershift-aws-create
  steps:
  - as: create-hostedcluster
    cli: latest
    env:
    - name: HYPERSHIFT_NODE_COUNT
      default: "3"
      documentation: "The number nodes to automatically create and join to the cluster."
    - name: HYPERSHIFT_BASE_DOMAIN
      default: ""
      documentation: "The cluster's FQDN will be a subdomain of the base domain."
    - name: HYPERSHIFT_AWS_REGION
      default: "us-east-1"
      documentation: |
        Specifies the AWS region for the cluster. If intentionally left as an empty string, 
        the region defaults to that of the management cluster.
    - name: HYPERSHIFT_HC_ZONES
      default: "us-east-1a"
      documentation: |
        Specifies the AWS AZs to create the NodePools. If intentionally left as an empty string,  
        it defaults to an AZ utilized by the management cluster.
    - name: ENDPOINT_ACCESS
      default: "Public"
      documentation: "Access for control plane endpoints (Public, PublicAndPrivate, Private)"
    - name: EXTRA_ARGS
      default: ""
      documentation: "Extra args to pass to the create cluster aws command"
    - name: HYPERSHIFT_GUEST_INFRA_OCP_ACCOUNT
      default: "false"
      documentation: "Whether to use the generic CI account or the HyperShift OSD account for the guest clusters infra. For the infra created for the clusters. E.g. For cluster-bot we use the generic CI account"
    - name: ENABLE_ICSP
      default: "false"
      documentation: "if true, add image content sources config(path=${SHARED_DIR}/mgmt_icsp.yaml)"
    - name: HYPERSHIFT_HC_RELEASE_IMAGE
      default: ""
      documentation: "Release image used for the HostedCluster. Empty by default it will use release:latest"
    - name: HYPERSHIFT_CP_AVAILABILITY_POLICY
      default: "SingleReplica"
      documentation: "Availability policy for hosted cluster components. Supported options: SingleReplica, HighlyAvailable, default SingleReplica"
    - name: HYPERSHIFT_INFRA_AVAILABILITY_POLICY
      default: "SingleReplica"
      documentation: "Availability policy for infrastructure services in guest cluster. Supported options: SingleReplica, HighlyAvailable, default SingleReplica"
    - name: HYPERSHIFT_INSTANCE_TYPE
      default: "m5.xlarge"
      documentation: "Instance type for the cluster nodes"
    - name: HYPERSHIFT_EXTERNAL_DNS_DOMAIN
      default: ""
      documentation: "Sets hostname to opinionated values in the specified domain for services with publishing type LoadBalancer or Route"
    - name: HYPERSHIFT_CREATE_CLUSTER_RENDER
      default: "false"
      documentation: "If true, render artifacts to ${SHARED_DIR}/hypershift_create_cluster_render.yaml"
    commands: |-
      set -exuo pipefail
      AWS_GUEST_INFRA_CREDENTIALS_FILE="/etc/hypershift-ci-jobs-awscreds/credentials"
      DEFAULT_BASE_DOMAIN=ci.hypershift.devcluster.openshift.com
      
      HC_REGION=${HYPERSHIFT_AWS_REGION:-$LEASED_RESOURCE}
      HC_ZONES="${HYPERSHIFT_HC_ZONES}"
      if [[ -z "$HC_ZONES" ]]; then
        HC_ZONES="$(oc get node -o jsonpath='{.items[0].metadata.labels.topology\.kubernetes\.io/zone}')"
      fi
      
      if [[ $HYPERSHIFT_GUEST_INFRA_OCP_ACCOUNT == "true" ]]; then
        AWS_GUEST_INFRA_CREDENTIALS_FILE="${CLUSTER_PROFILE_DIR}/.awscred"
        DEFAULT_BASE_DOMAIN=origin-ci-int-aws.dev.rhcloud.com
      fi
      DOMAIN=${HYPERSHIFT_BASE_DOMAIN:-$DEFAULT_BASE_DOMAIN}
      RELEASE_IMAGE=${HYPERSHIFT_HC_RELEASE_IMAGE:-$RELEASE_IMAGE_LATEST}

      CLUSTER_NAME="$(echo -n $PROW_JOB_ID|sha256sum|cut -c-20)"
      echo "$(date) Creating HyperShift cluster ${CLUSTER_NAME}"
      COMMAND=(
        /usr/bin/hypershift create cluster aws
        --name ${CLUSTER_NAME}
        --external-dns-domain=${HYPERSHIFT_EXTERNAL_DNS_DOMAIN}
        --node-pool-replicas ${HYPERSHIFT_NODE_COUNT}
        --instance-type ${HYPERSHIFT_INSTANCE_TYPE}
        --base-domain ${DOMAIN}
        --endpoint-access ${ENDPOINT_ACCESS}
        --region ${HC_REGION}
        --zones ${HC_ZONES}
        --control-plane-availability-policy ${HYPERSHIFT_CP_AVAILABILITY_POLICY}
        --infra-availability-policy ${HYPERSHIFT_INFRA_AVAILABILITY_POLICY}
        --pull-secret=/etc/ci-pull-credentials/.dockerconfigjson
        --aws-creds=${AWS_GUEST_INFRA_CREDENTIALS_FILE}
        --release-image ${RELEASE_IMAGE}
        --annotations=hypershift.openshift.io/skip-release-image-validation=true
        --additional-tags="expirationDate=$(date -d '4 hours' --iso=minutes --utc)" 
      )
      
      if [[ $ENABLE_ICSP == "true" ]]; then
        COMMAND+=(--image-content-sources "${SHARED_DIR}/mgmt_icsp.yaml")
      fi
      
      if [[ -n $EXTRA_ARGS ]]; then
        COMMAND+=(${EXTRA_ARGS})
      fi
     
      if [[ $HYPERSHIFT_CREATE_CLUSTER_RENDER == "true" ]]; then
        "${COMMAND[@]}" --render > "${SHARED_DIR}/hypershift_create_cluster_render.yaml"
        exit 0
      fi
      
      # add cluster-info tags if exists
      hosted_cluster_info_file=${SHARED_DIR}/hosted_cluster_info_file
      if [[ -f "${hosted_cluster_info_file}" ]] ; then
        while read -r TAG VALUE
        do
          printf 'Setting user tag to help cost usage analysis - %s: %s\n' "${TAG}" "${VALUE}"
          COMMAND+=(--additional-tags="${TAG}=${VALUE}")
        done < "${hosted_cluster_info_file}"
        cp ${hosted_cluster_info_file} ${ARTIFACT_DIR}/aws-hosted-cluster-info-user-tag
      fi

      "${COMMAND[@]}"

      set +e
      export CLUSTER_NAME
      timeout 25m bash -c '
        until [[ "$(oc get -n clusters hostedcluster/${CLUSTER_NAME} -o jsonpath='"'"'{.status.version.history[?(@.state!="")].state}'"'"')" = "Completed" ]]; do
            sleep 15
        done
      '
      if [[ $? -ne 0 ]]; then
        cat << EOF > ${ARTIFACT_DIR}/junit_hosted_cluster.xml
      <?xml version="1.0" encoding="UTF-8"?>
      <testsuite name="hypershift install" tests="1" failures="1">
        <testcase name="hosted cluster version rollout succeeds">
          <failure message="hosted cluster version rollout never completed">
            <![CDATA[
      error: hosted cluster version rollout never completed, dumping relevant hosted cluster condition messages
      Degraded: $(oc get -n clusters hostedcluster/${CLUSTER_NAME} -o jsonpath='{.status.conditions[?(@.type=="Degraded")].message}')
      ClusterVersionSucceeding: $(oc get -n clusters hostedcluster/${CLUSTER_NAME} -o jsonpath='{.status.conditions[?(@.type=="ClusterVersionSucceeding")].message}')
            ]]>
          </failure>
        </testcase>
      </testsuite>
      EOF
        exit 1
      else
        cat << EOF > ${ARTIFACT_DIR}/junit_hosted_cluster.xml
      <?xml version="1.0" encoding="UTF-8"?>
      <testsuite name="hypershift install" tests="1" failures="0">
        <testcase name="hosted cluster version rollout succeeds">
          <system-out>
            <![CDATA[
      info: hosted cluster version rollout completed successfully
            ]]>
          </system-out>
        </testcase>
      </testsuite>
      EOF
      fi
      set -e

      echo "Cluster became available, creating kubeconfig"
      bin/hypershift create kubeconfig --namespace=clusters --name=${CLUSTER_NAME} >${SHARED_DIR}/nested_kubeconfig
    from: hypershift-operator
    grace_period: 5m0s
    resources:
      requests:
        cpu: 100m
        memory: 100Mi
    timeout: 35m0s
    credentials:
    - mount_path: /etc/hypershift-ci-jobs-awscreds
      name: hypershift-ci-jobs-awscreds
      namespace: test-credentials
    - mount_path: /etc/ci-pull-credentials
      name: ci-pull-credentials
      namespace: test-credentials
    dependencies:
    - name: "release:latest"
      env: RELEASE_IMAGE_LATEST
    - name: hypershift-operator
      env: HYPERSHIFT_RELEASE_LATEST
