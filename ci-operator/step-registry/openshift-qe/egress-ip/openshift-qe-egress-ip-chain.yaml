chain:
  as: openshift-qe-egress-ip
  steps:
  - ref: openshift-qe-egress-ip-setup
  - ref: redhat-chaos-pod-scenarios-custom
  - ref: redhat-chaos-node-disruptions-worker-outage
  - ref: openshift-qe-egress-ip-tests
  env:
  # Pod disruption chaos step environment variables
  - name: TARGET_NAMESPACE
    default: "openshift-ovn-kubernetes"
  - name: POD_LABEL
    default: "app=ovnkube-node"
  - name: DISRUPTION_COUNT
    default: "3"
  - name: EXPECTED_POD_COUNT
    default: "3"
  - name: KILL_TIMEOUT
    default: "120"
  - name: EXPECTED_RECOVERY_TIME
    default: "90"
  
  # Node disruption chaos step environment variables
  - name: ACTION
    default: "node_reboot_scenario"
  - name: LABEL_SELECTOR
    default: "node-role.kubernetes.io/worker="
  - name: RUNS
    default: "1"
  - name: NODE_OUTAGE_TIMEOUT
    default: "120"

  # Disable telemetry to prevent IndexError bugs
  - name: ENABLE_ALERTS
    default: "False"
  - name: CHECK_CRITICAL_ALERTS
    default: "False"
  - name: WAIT_DURATION
    default: "60"
  - name: TELEMETRY_ENABLED
    default: "False"
  - name: TELEMETRY_PROMETHEUS_BACKUP
    default: "False"
  - name: TELEMETRY_FULL_PROMETHEUS_BACKUP
    default: "False"
  - name: TELEMETRY_LOGS_BACKUP
    default: "False"
  - name: TELEMETRY_EVENTS_BACKUP
    default: "False"
  - name: CAPTURE_METRICS
    default: "False"
  - name: ENABLE_ES
    default: "False"
  - name: JUNIT_TESTCASE
    default: '[Egress-IP] OVN pod disruption resilience test'
  documentation: >-
    Executes the enhanced OpenShift QE egress IP test suite using e2e methodology.
    
    This chain orchestrates:
    1. Infrastructure setup using improved egress IP setup methodology
    2. Chaos engineering pod disruption using krkn framework (targets ovnkube-node pods)
    3. Chaos engineering node reboot using krkn framework (targets worker nodes)  
    4. Post-chaos validation with internal routing verification
    
    The test suite validates egress IP functionality using internal routing
    methodology instead of external services, providing more reliable and consistent
    results under controlled chaos scenarios including pod disruption and node outages.