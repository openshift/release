apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: prow
    role: alert-rules
  name: prometheus-prow-rules
  namespace: prow-monitoring
spec:
  groups:
  - name: build-cop-target-low
    rules:
    - alert: branch-.*-images-low
      annotations:
        message: '@build-cop `branch-.*-images` jobs are passing at a rate of {{ $value
          | humanize }}%, which is below the target (100%). Check the <https://grafana-prow-monitoring.svc.ci.openshift.org/d/6829209d59479d48073d09725ce807fa/build-cop-dashboard?orgId=1&fullscreen&panelId=2|dashboard>
          and <https://prow.svc.ci.openshift.org/?job=branch-*-images|deck-portal>.'
      expr: |
        sum(rate(prowjob_state_transitions{job="plank",job_name=~"branch-.*-images",job_name!~"rehearse.*",state="success"}[30m]))/sum(rate(prowjob_state_transitions{job="plank",job_name=~"branch-.*-images",job_name!~"rehearse.*",state=~"success|failure"}[30m])) * 100 < 100
      for: 30m
      labels:
        severity: slack
        team: build-cop
    - alert: release-.*-4.1-low
      annotations:
        message: '@build-cop `release-.*-4.1` jobs are passing at a rate of {{ $value
          | humanize }}%, which is below the target (100%). Check the <https://grafana-prow-monitoring.svc.ci.openshift.org/d/6829209d59479d48073d09725ce807fa/build-cop-dashboard?orgId=1&fullscreen&panelId=2|dashboard>
          and <https://prow.svc.ci.openshift.org/?job=release-*-4.1|deck-portal>.'
      expr: |
        sum(rate(prowjob_state_transitions{job="plank",job_name=~"release-.*-4.1",job_name!~"rehearse.*",state="success"}[30m]))/sum(rate(prowjob_state_transitions{job="plank",job_name=~"release-.*-4.1",job_name!~"rehearse.*",state=~"success|failure"}[30m])) * 100 < 100
      for: 30m
      labels:
        severity: slack
        team: build-cop
    - alert: release-.*-4.2-low
      annotations:
        message: '@build-cop `release-.*-4.2` jobs are passing at a rate of {{ $value
          | humanize }}%, which is below the target (100%). Check the <https://grafana-prow-monitoring.svc.ci.openshift.org/d/6829209d59479d48073d09725ce807fa/build-cop-dashboard?orgId=1&fullscreen&panelId=2|dashboard>
          and <https://prow.svc.ci.openshift.org/?job=release-*-4.2|deck-portal>.'
      expr: |
        sum(rate(prowjob_state_transitions{job="plank",job_name=~"release-.*-4.2",job_name!~"rehearse.*",state="success"}[30m]))/sum(rate(prowjob_state_transitions{job="plank",job_name=~"release-.*-4.2",job_name!~"rehearse.*",state=~"success|failure"}[30m])) * 100 < 100
      for: 30m
      labels:
        severity: slack
        team: build-cop
    - alert: release-.*-upgrade.*-low
      annotations:
        message: '@build-cop `release-.*-upgrade.*` jobs are passing at a rate of
          {{ $value | humanize }}%, which is below the target (100%). Check the <https://grafana-prow-monitoring.svc.ci.openshift.org/d/6829209d59479d48073d09725ce807fa/build-cop-dashboard?orgId=1&fullscreen&panelId=2|dashboard>
          and <https://prow.svc.ci.openshift.org/?job=release-*-upgrade*|deck-portal>.'
      expr: |
        sum(rate(prowjob_state_transitions{job="plank",job_name=~"release-.*-upgrade.*",job_name!~"rehearse.*",state="success"}[30m]))/sum(rate(prowjob_state_transitions{job="plank",job_name=~"release-.*-upgrade.*",job_name!~"rehearse.*",state=~"success|failure"}[30m])) * 100 < 100
      for: 30m
      labels:
        severity: slack
        team: build-cop
  - name: ci-absent
    rules:
    - alert: deckDown
      annotations:
        message: The service deck has been down for 5 minutes.
      expr: |
        absent(up{job="deck"} == 1)
      for: 5m
      labels:
        severity: slack
    - alert: ghproxyDown
      annotations:
        message: The service ghproxy has been down for 5 minutes.
      expr: |
        absent(up{job="ghproxy"} == 1)
      for: 5m
      labels:
        severity: slack
    - alert: hookDown
      annotations:
        message: The service hook has been down for 5 minutes.
      expr: |
        absent(up{job="hook"} == 1)
      for: 5m
      labels:
        severity: slack
    - alert: jenkins-dev-operatorDown
      annotations:
        message: The service jenkins-dev-operator has been down for 5 minutes.
      expr: |
        absent(up{job="jenkins-dev-operator"} == 1)
      for: 5m
      labels:
        severity: slack
    - alert: jenkins-operatorDown
      annotations:
        message: The service jenkins-operator has been down for 5 minutes.
      expr: |
        absent(up{job="jenkins-operator"} == 1)
      for: 5m
      labels:
        severity: slack
    - alert: kata-jenkins-operatorDown
      annotations:
        message: The service kata-jenkins-operator has been down for 5 minutes.
      expr: |
        absent(up{job="kata-jenkins-operator"} == 1)
      for: 5m
      labels:
        severity: slack
    - alert: plankDown
      annotations:
        message: The service plank has been down for 5 minutes.
      expr: |
        absent(up{job="plank"} == 1)
      for: 5m
      labels:
        severity: slack
    - alert: sinkerDown
      annotations:
        message: The service sinker has been down for 5 minutes.
      expr: |
        absent(up{job="sinker"} == 1)
      for: 5m
      labels:
        severity: slack
    - alert: tideDown
      annotations:
        message: The service tide has been down for 5 minutes.
      expr: |
        absent(up{job="tide"} == 1)
      for: 5m
      labels:
        severity: slack
  - name: prow-monitoring-absent
    rules:
    - alert: ServiceLostHA
      annotations:
        message: The service {{ $labels.job }} has at most 1 instance for 5 minutes.
      expr: |
        sum(up{job=~"grafana|prometheus|alertmanager"}) by (job) <= 1
      for: 5m
      labels:
        severity: slack
    - alert: grafanaDown
      annotations:
        message: The service grafana has been down for 5 minutes.
      expr: |
        absent(up{job="grafana"} == 1)
      for: 5m
      labels:
        severity: slack
    - alert: alertmanagerDown
      annotations:
        message: The service alertmanager has been down for 5 minutes.
      expr: |
        absent(up{job="alertmanager"} == 1)
      for: 5m
      labels:
        severity: slack
    - alert: prometheusDown
      annotations:
        message: The service prometheus has been down for 5 minutes.
      expr: |
        absent(up{job="prometheus"} == 1)
      for: 5m
      labels:
        severity: slack
  - name: abnormal webhook behaviors
    rules:
    - alert: no-webhook-calls
      annotations:
        message: There have been no webhook calls on working hours for 5 minutes
      expr: |
        sum(increase(prow_webhook_counter[1m])) == 0
        and ( (day_of_week() > 0) and (day_of_week() < 6) and (hour() >= 7) )
      for: 5m
      labels:
        severity: slack
