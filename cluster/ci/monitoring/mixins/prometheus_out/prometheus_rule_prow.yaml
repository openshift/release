apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: prow
    role: alert-rules
  name: prometheus-prow-rules
  namespace: prow-monitoring
spec:
  groups:
  - name: mirroring-failures
    rules:
    - alert: mirroring-failures
      annotations:
        message: '@build-cop image mirroring jobs have failed. View failed jobs at
          the <https://prow.svc.ci.openshift.org/?job=periodic-image-mirroring-openshift|overview>.  If
          there is a consistent issue, open a bugzilla bug against Test Infrastructure
          and assign it to Clayton Coleman.'
      expr: |
        increase(prowjob_state_transitions{job_name="periodic-image-mirroring-openshift",state="failure"}[5m]) > 0
      for: 1m
      labels:
        severity: critical
        team: build-cop
  - name: ci-absent
    rules:
    - alert: deckDown
      annotations:
        message: The service deck has been down for 5 minutes.
      expr: |
        absent(up{job="deck"} == 1)
      for: 5m
      labels:
        severity: critical
    - alert: deck-internalDown
      annotations:
        message: The service deck-internal has been down for 5 minutes.
      expr: |
        absent(up{job="deck-internal"} == 1)
      for: 5m
      labels:
        severity: critical
    - alert: ghproxyDown
      annotations:
        message: The service ghproxy has been down for 5 minutes.
      expr: |
        absent(up{job="ghproxy"} == 1)
      for: 5m
      labels:
        severity: critical
    - alert: hookDown
      annotations:
        message: The service hook has been down for 5 minutes.
      expr: |
        absent(up{job="hook"} == 1)
      for: 5m
      labels:
        severity: critical
    - alert: jenkins-dev-operatorDown
      annotations:
        message: The service jenkins-dev-operator has been down for 5 minutes.
      expr: |
        absent(up{job="jenkins-dev-operator"} == 1)
      for: 5m
      labels:
        severity: critical
    - alert: jenkins-operatorDown
      annotations:
        message: The service jenkins-operator has been down for 5 minutes.
      expr: |
        absent(up{job="jenkins-operator"} == 1)
      for: 5m
      labels:
        severity: critical
    - alert: kata-jenkins-operatorDown
      annotations:
        message: The service kata-jenkins-operator has been down for 5 minutes.
      expr: |
        absent(up{job="kata-jenkins-operator"} == 1)
      for: 5m
      labels:
        severity: critical
    - alert: plankDown
      annotations:
        message: The service plank has been down for 5 minutes.
      expr: |
        absent(up{job="plank"} == 1)
      for: 5m
      labels:
        severity: critical
    - alert: sinkerDown
      annotations:
        message: The service sinker has been down for 5 minutes.
      expr: |
        absent(up{job="sinker"} == 1)
      for: 5m
      labels:
        severity: critical
    - alert: tideDown
      annotations:
        message: The service tide has been down for 5 minutes.
      expr: |
        absent(up{job="tide"} == 1)
      for: 5m
      labels:
        severity: critical
  - name: prow-monitoring-absent
    rules:
    - alert: ServiceLostHA
      annotations:
        message: The service {{ $labels.job }} has at most 1 instance for 5 minutes.
      expr: |
        sum(up{job=~"grafana|prometheus|alertmanager"}) by (job) <= 1
      for: 5m
      labels:
        severity: critical
    - alert: AlertManagerPodDown
      annotations:
        message: Not all of 3 alert manager pods have been running in the last 5 minutes.
      expr: |
        sum(up{job="alertmanager"}) < 3
      for: 5m
      labels:
        severity: critical
    - alert: grafanaDown
      annotations:
        message: The service grafana has been down for 5 minutes.
      expr: |
        absent(up{job="grafana"} == 1)
      for: 5m
      labels:
        severity: critical
    - alert: alertmanagerDown
      annotations:
        message: The service alertmanager has been down for 5 minutes.
      expr: |
        absent(up{job="alertmanager"} == 1)
      for: 5m
      labels:
        severity: critical
    - alert: prometheusDown
      annotations:
        message: The service prometheus has been down for 5 minutes.
      expr: |
        absent(up{job="prometheus"} == 1)
      for: 5m
      labels:
        severity: critical
  - name: abnormal webhook behaviors
    rules:
    - alert: no-webhook-calls
      annotations:
        message: There have been no webhook calls on working hours for 5 minutes
      expr: |
        (sum(increase(prow_webhook_counter[1m])) == 0 or absent(prow_webhook_counter))
        and ( (day_of_week() > 0) and (day_of_week() < 6) and (hour() >= 7) )
      for: 5m
      labels:
        severity: critical
  - name: configmap-full
    rules:
    - alert: ConfigMapFullInOneWeek
      annotations:
        message: Based on recent sampling, the ConfigMap {{ $labels.name }} in Namespace
          {{ $labels.namespace }} is expected to fill up within a week. Currently
          {{ printf "%0.2f" $value }}% is available.
      expr: |
        100 * ((1048576 - prow_configmap_size_bytes) / 1048576) < 15
        and
        predict_linear(prow_configmap_size_bytes[12h], 7 * 24 * 3600) > 1048576
      for: 5m
      labels:
        severity: critical
  - name: plank-infra
    rules:
    - alert: plank-job-with-infra-role-failures
      annotations:
        message: plank jobs {{ $labels.job_name }} with infra role has failures. Check
          on <https://grafana-prow-monitoring.svc.ci.openshift.org/d/8ce131e226b7fd2901c2fce45d4e21c1/dptp-dashboard?orgId=1&fullscreen&panelId=3|grafana>
          and <https://prow.svc.ci.openshift.org/?job={{ $labels.job_name }}|deck>
      expr: |
        sum(rate(prowjob_state_transitions{job="plank",job_name!~"rehearse.*",state="failure"}[5m])) by (job_name) * on (job_name) group_left prow_job_labels{job_agent="kubernetes",label_ci_openshift_io_role="infra"} > 0
      for: 1m
      labels:
        severity: critical
  - name: ghproxy
    rules:
    - alert: ghproxy-specific-status-code-abnormal
      annotations:
        message: '{{ $value | humanize }}% of all requests for {{ $labels.path }}
          through the GitHub proxy are errorring with code {{ $labels.status }}. Check
          <https://grafana-prow-monitoring.svc.ci.openshift.org/d/d72fe8d0400b2912e319b1e95d0ab1b3/github-cache?orgId=1&refresh=1m&fullscreen&panelId=9|grafana>'
      expr: |
        sum(rate(github_request_duration_count{status=~"[45]..",status!="404",status!="410"}[5m])) by (status,path) / ignoring(status) group_left sum(rate(github_request_duration_count[5m])) by (path) * 100 > 10
      labels:
        severity: warning
    - alert: ghproxy-global-status-code-abnormal
      annotations:
        message: '{{ $value | humanize }}% of all API requests through the GitHub
          proxy are errorring with code {{ $labels.status }}. Check <https://grafana-prow-monitoring.svc.ci.openshift.org/d/d72fe8d0400b2912e319b1e95d0ab1b3/github-cache?orgId=1&fullscreen&panelId=8|grafana>'
      expr: |
        sum(rate(github_request_duration_count{status=~"[45]..",status!="404",status!="410"}[5m])) by (status) / ignoring(status) group_left sum(rate(github_request_duration_count[5m])) * 100 > 3
      labels:
        severity: warning
    - alert: ghproxy-running-out-github-tokens-in-a-hour
      annotations:
        message: token {{ $labels.token_hash }} may run out of API quota before the
          next reset. Check the <https://grafana-prow-monitoring.svc.ci.openshift.org/d/d72fe8d0400b2912e319b1e95d0ab1b3/github-cache?orgId=1|dashboard>
      expr: |
        github_token_usage + deriv(github_token_usage[10m]) * github_token_reset / 1e9 < 100
      for: 5m
      labels:
        severity: critical
  - name: sinker-missing
    rules:
    - alert: SinkerNotRemovingPods
      annotations:
        message: Sinker has not removed any Pods in the last hour, likely indicating
          an outage in the service.
      expr: |
        absent(sum(rate(sinker_pods_removed[1h]))) == 1
      for: 5m
      labels:
        severity: critical
    - alert: SinkerNotRemovingProwJobs
      annotations:
        message: Sinker has not removed any Prow jobs in the last hour, likely indicating
          an outage in the service.
      expr: |
        absent(sum(rate(sinker_prow_jobs_cleaned[1h]))) == 1
      for: 5m
      labels:
        severity: critical
  - name: prow-job-failures
    rules:
    - alert: endurance-cluster-maintenance-aws-4.3-failures
      annotations:
        message: '@bparees Prow job endurance-cluster-maintenance-aws-4.3 has failures.
          Check on <https://prow.svc.ci.openshift.org/?type=periodic&job=endurance-cluster-maintenance-aws-4.3|deck>'
      expr: |
        rate(prowjob_state_transitions{job_name="endurance-cluster-maintenance-aws-4.3",state="failure"}[5m]) > 0
      for: 1m
      labels:
        severity: critical
        team: build-cop
    - alert: endurance-cluster-maintenance-aws-4.4-failures
      annotations:
        message: '@bparees Prow job endurance-cluster-maintenance-aws-4.4 has failures.
          Check on <https://prow.svc.ci.openshift.org/?type=periodic&job=endurance-cluster-maintenance-aws-4.4|deck>'
      expr: |
        rate(prowjob_state_transitions{job_name="endurance-cluster-maintenance-aws-4.4",state="failure"}[5m]) > 0
      for: 1m
      labels:
        severity: critical
        team: build-cop
    - alert: periodic-ci-openshift-kni-cnf-features-deploy-master-cnf-e2e-gcp-periodic-failures
      annotations:
        message: '@cnf-cop Prow job periodic-ci-openshift-kni-cnf-features-deploy-master-cnf-e2e-gcp-periodic
          has failures. Check on <https://prow.svc.ci.openshift.org/?type=periodic&job=periodic-ci-openshift-kni-cnf-features-deploy-master-cnf-e2e-gcp-periodic|deck>'
      expr: |
        rate(prowjob_state_transitions{job_name="periodic-ci-openshift-kni-cnf-features-deploy-master-cnf-e2e-gcp-periodic",state="failure"}[5m]) > 0
      for: 1m
      labels:
        severity: critical
        team: cnf-cop
    - alert: periodic-ci-openshift-kni-cnf-features-deploy-release-4.4-cnf-e2e-gcp-periodic-failures
      annotations:
        message: '@cnf-cop Prow job periodic-ci-openshift-kni-cnf-features-deploy-release-4.4-cnf-e2e-gcp-periodic
          has failures. Check on <https://prow.svc.ci.openshift.org/?type=periodic&job=periodic-ci-openshift-kni-cnf-features-deploy-release-4.4-cnf-e2e-gcp-periodic|deck>'
      expr: |
        rate(prowjob_state_transitions{job_name="periodic-ci-openshift-kni-cnf-features-deploy-release-4.4-cnf-e2e-gcp-periodic",state="failure"}[5m]) > 0
      for: 1m
      labels:
        severity: critical
        team: cnf-cop
    - alert: periodic-ci-operator-framework-operator-lifecycle-managment-master-rhoperator-metric-e2e-aws-olm-master-daily-failures
      annotations:
        message: '@olmcop Prow job periodic-ci-operator-framework-operator-lifecycle-managment-master-rhoperator-metric-e2e-aws-olm-master-daily
          has failures. Check on <https://prow.svc.ci.openshift.org/?type=periodic&job=periodic-ci-operator-framework-operator-lifecycle-managment-master-rhoperator-metric-e2e-aws-olm-master-daily|deck>'
      expr: |
        rate(prowjob_state_transitions{job_name="periodic-ci-operator-framework-operator-lifecycle-managment-master-rhoperator-metric-e2e-aws-olm-master-daily",state="failure"}[5m]) > 0
      for: 1m
      labels:
        severity: critical
        team: OLM
    - alert: periodic-ci-redhat-developer-service-binding-operator-master-dev-release-failures
      annotations:
        message: '@openshift-app-services Prow job periodic-ci-redhat-developer-service-binding-operator-master-dev-release
          has failures. Check on <https://prow.svc.ci.openshift.org/?type=periodic&job=periodic-ci-redhat-developer-service-binding-operator-master-dev-release|deck>'
      expr: |
        rate(prowjob_state_transitions{job_name="periodic-ci-redhat-developer-service-binding-operator-master-dev-release",state="failure"}[5m]) > 0
      for: 1m
      labels:
        severity: critical
        team: openshift-dev-services
    - alert: periodic-openshift-library-import-failures
      annotations:
        message: '@devex Prow job periodic-openshift-library-import has failures.
          Check on <https://prow.svc.ci.openshift.org/?type=periodic&job=periodic-openshift-library-import|deck>'
      expr: |
        rate(prowjob_state_transitions{job_name="periodic-openshift-library-import",state="failure"}[5m]) > 0
      for: 1m
      labels:
        severity: critical
        team: developer-experience
    - alert: release-openshift-ocp-installer-e2e-gcp-rt-4.4-failures
      annotations:
        message: '@cnf-cop Prow job release-openshift-ocp-installer-e2e-gcp-rt-4.4
          has failures. Check on <https://prow.svc.ci.openshift.org/?type=periodic&job=release-openshift-ocp-installer-e2e-gcp-rt-4.4|deck>'
      expr: |
        rate(prowjob_state_transitions{job_name="release-openshift-ocp-installer-e2e-gcp-rt-4.4",state="failure"}[5m]) > 0
      for: 1m
      labels:
        severity: critical
        team: cnf-cop
    - alert: release-openshift-ocp-installer-e2e-gcp-rt-4.5-failures
      annotations:
        message: '@cnf-cop Prow job release-openshift-ocp-installer-e2e-gcp-rt-4.5
          has failures. Check on <https://prow.svc.ci.openshift.org/?type=periodic&job=release-openshift-ocp-installer-e2e-gcp-rt-4.5|deck>'
      expr: |
        rate(prowjob_state_transitions{job_name="release-openshift-ocp-installer-e2e-gcp-rt-4.5",state="failure"}[5m]) > 0
      for: 1m
      labels:
        severity: critical
        team: cnf-cop
    - alert: periodic-ci-image-import-to-build01-low
      annotations:
        message: '`periodic-ci-image-import-to-build01` jobs are passing at a rate
          of {{ $value | humanize }}%, which is below the target (95%). Check <https://prow.svc.ci.openshift.org/?job=periodic-ci-image-import-to-build01|deck-portal>.'
      expr: |
        sum(rate(prowjob_state_transitions{job="plank",job_name="periodic-ci-image-import-to-build01",state="success"}[2d]))/sum(rate(prowjob_state_transitions{job="plank",job_name="periodic-ci-image-import-to-build01",state=~"success|failure"}[2d])) * 100 < 95
      for: 10m
      labels:
        severity: critical
