apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: prow
    role: alert-rules
  name: prometheus-prow-rules
  namespace: prow-monitoring
spec:
  groups:
  - name: build-cop-target-low
    rules:
    - alert: branch-.*-images-low
      annotations:
        message: '@build-cop `branch-.*-images` jobs are passing at a rate of {{ $value
          | humanize }}%, which is below the target (100%). Check the <https://grafana-prow-monitoring.svc.ci.openshift.org/d/6829209d59479d48073d09725ce807fa/build-cop-dashboard?orgId=1&fullscreen&panelId=2|dashboard>
          and <https://prow.svc.ci.openshift.org/?job=branch-*-images|deck-portal>.'
      expr: |
        sum(rate(prowjob_state_transitions{job="plank",job_name=~"branch-.*-images",job_name!~"rehearse.*",state="success"}[30m]))/sum(rate(prowjob_state_transitions{job="plank",job_name=~"branch-.*-images",job_name!~"rehearse.*",state=~"success|failure"}[30m])) * 100 < 100
      for: 10m
      labels:
        severity: slack
        team: build-cop
    - alert: release-.*-4.1-low
      annotations:
        message: '@build-cop `release-.*-4.1` jobs are passing at a rate of {{ $value
          | humanize }}%, which is below the target (100%). Check the <https://grafana-prow-monitoring.svc.ci.openshift.org/d/6829209d59479d48073d09725ce807fa/build-cop-dashboard?orgId=1&fullscreen&panelId=2|dashboard>
          and <https://prow.svc.ci.openshift.org/?job=release-*-4.1|deck-portal>.'
      expr: |
        sum(rate(prowjob_state_transitions{job="plank",job_name=~"release-.*-4.1",job_name!~"rehearse.*",state="success"}[30m]))/sum(rate(prowjob_state_transitions{job="plank",job_name=~"release-.*-4.1",job_name!~"rehearse.*",state=~"success|failure"}[30m])) * 100 < 80
      for: 10m
      labels:
        severity: slack
        team: build-cop
    - alert: release-.*-4.2-low
      annotations:
        message: '@build-cop `release-.*-4.2` jobs are passing at a rate of {{ $value
          | humanize }}%, which is below the target (100%). Check the <https://grafana-prow-monitoring.svc.ci.openshift.org/d/6829209d59479d48073d09725ce807fa/build-cop-dashboard?orgId=1&fullscreen&panelId=2|dashboard>
          and <https://prow.svc.ci.openshift.org/?job=release-*-4.2|deck-portal>.'
      expr: |
        sum(rate(prowjob_state_transitions{job="plank",job_name=~"release-.*-4.2",job_name!~"rehearse.*",state="success"}[30m]))/sum(rate(prowjob_state_transitions{job="plank",job_name=~"release-.*-4.2",job_name!~"rehearse.*",state=~"success|failure"}[30m])) * 100 < 80
      for: 10m
      labels:
        severity: slack
        team: build-cop
    - alert: release-.*-upgrade.*-low
      annotations:
        message: '@build-cop `release-.*-upgrade.*` jobs are passing at a rate of
          {{ $value | humanize }}%, which is below the target (100%). Check the <https://grafana-prow-monitoring.svc.ci.openshift.org/d/6829209d59479d48073d09725ce807fa/build-cop-dashboard?orgId=1&fullscreen&panelId=2|dashboard>
          and <https://prow.svc.ci.openshift.org/?job=release-*-upgrade*|deck-portal>.'
      expr: |
        sum(rate(prowjob_state_transitions{job="plank",job_name=~"release-.*-upgrade.*",job_name!~"rehearse.*",state="success"}[30m]))/sum(rate(prowjob_state_transitions{job="plank",job_name=~"release-.*-upgrade.*",job_name!~"rehearse.*",state=~"success|failure"}[30m])) * 100 < 80
      for: 10m
      labels:
        severity: slack
        team: build-cop
    - alert: release-.*4.1.*4.2.*-low
      annotations:
        message: '@build-cop `release-.*4.1.*4.2.*` jobs are passing at a rate of
          {{ $value | humanize }}%, which is below the target (100%). Check the <https://grafana-prow-monitoring.svc.ci.openshift.org/d/6829209d59479d48073d09725ce807fa/build-cop-dashboard?orgId=1&fullscreen&panelId=2|dashboard>
          and <https://prow.svc.ci.openshift.org/?job=release-*4.1*4.2*|deck-portal>.'
      expr: |
        sum(rate(prowjob_state_transitions{job="plank",job_name=~"release-.*4.1.*4.2.*",job_name!~"rehearse.*",state="success"}[30m]))/sum(rate(prowjob_state_transitions{job="plank",job_name=~"release-.*4.1.*4.2.*",job_name!~"rehearse.*",state=~"success|failure"}[30m])) * 100 < 80
      for: 10m
      labels:
        severity: slack
        team: build-cop
  - name: mirroring-failures
    rules:
    - alert: mirroring-failures
      annotations:
        message: '@build-cop image mirroring jobs have failed. View failed jobs at
          the <https://prow.svc.ci.openshift.org/?job=periodic-image-mirroring-openshift&state=failure|overview>.'
      expr: |
        increase(prowjob_state_transitions{job_name="periodic-image-mirroring-openshift",state="failure"}[5m]) > 0
      for: 1m
      labels:
        severity: slack
        team: build-cop
  - name: ci-absent
    rules:
    - alert: deckDown
      annotations:
        message: The service deck has been down for 5 minutes.
      expr: |
        absent(up{job="deck"} == 1)
      for: 5m
      labels:
        severity: slack
    - alert: deck-internalDown
      annotations:
        message: The service deck-internal has been down for 5 minutes.
      expr: |
        absent(up{job="deck-internal"} == 1)
      for: 5m
      labels:
        severity: slack
    - alert: ghproxyDown
      annotations:
        message: The service ghproxy has been down for 5 minutes.
      expr: |
        absent(up{job="ghproxy"} == 1)
      for: 5m
      labels:
        severity: slack
    - alert: hookDown
      annotations:
        message: The service hook has been down for 5 minutes.
      expr: |
        absent(up{job="hook"} == 1)
      for: 5m
      labels:
        severity: slack
    - alert: jenkins-dev-operatorDown
      annotations:
        message: The service jenkins-dev-operator has been down for 5 minutes.
      expr: |
        absent(up{job="jenkins-dev-operator"} == 1)
      for: 5m
      labels:
        severity: slack
    - alert: jenkins-operatorDown
      annotations:
        message: The service jenkins-operator has been down for 5 minutes.
      expr: |
        absent(up{job="jenkins-operator"} == 1)
      for: 5m
      labels:
        severity: slack
    - alert: kata-jenkins-operatorDown
      annotations:
        message: The service kata-jenkins-operator has been down for 5 minutes.
      expr: |
        absent(up{job="kata-jenkins-operator"} == 1)
      for: 5m
      labels:
        severity: slack
    - alert: plankDown
      annotations:
        message: The service plank has been down for 5 minutes.
      expr: |
        absent(up{job="plank"} == 1)
      for: 5m
      labels:
        severity: slack
    - alert: sinkerDown
      annotations:
        message: The service sinker has been down for 5 minutes.
      expr: |
        absent(up{job="sinker"} == 1)
      for: 5m
      labels:
        severity: slack
    - alert: tideDown
      annotations:
        message: The service tide has been down for 5 minutes.
      expr: |
        absent(up{job="tide"} == 1)
      for: 5m
      labels:
        severity: slack
  - name: prow-monitoring-absent
    rules:
    - alert: ServiceLostHA
      annotations:
        message: The service {{ $labels.job }} has at most 1 instance for 5 minutes.
      expr: |
        sum(up{job=~"grafana|prometheus|alertmanager"}) by (job) <= 1
      for: 5m
      labels:
        severity: slack
    - alert: grafanaDown
      annotations:
        message: The service grafana has been down for 5 minutes.
      expr: |
        absent(up{job="grafana"} == 1)
      for: 5m
      labels:
        severity: slack
    - alert: alertmanagerDown
      annotations:
        message: The service alertmanager has been down for 5 minutes.
      expr: |
        absent(up{job="alertmanager"} == 1)
      for: 5m
      labels:
        severity: slack
    - alert: prometheusDown
      annotations:
        message: The service prometheus has been down for 5 minutes.
      expr: |
        absent(up{job="prometheus"} == 1)
      for: 5m
      labels:
        severity: slack
  - name: abnormal webhook behaviors
    rules:
    - alert: no-webhook-calls
      annotations:
        message: There have been no webhook calls on working hours for 5 minutes
      expr: |
        (sum(increase(prow_webhook_counter[1m])) == 0 or absent(prow_webhook_counter))
        and ( (day_of_week() > 0) and (day_of_week() < 6) and (hour() >= 7) )
      for: 5m
      labels:
        severity: slack
  - name: configmap-full
    rules:
    - alert: ConfigMapFullInOneWeek
      annotations:
        message: Based on recent sampling, the ConfigMap {{ $labels.name }} in Namespace
          {{ $labels.namespace }} is expected to fill up within a week. Currently
          {{ printf "%0.2f" $value }}% is available.
      expr: |
        100 * ((1048576 - prow_configmap_size_bytes) / 1048576) < 15
        and
        predict_linear(prow_configmap_size_bytes[12h], 7 * 24 * 3600) > 1048576
      for: 5m
      labels:
        severity: slack
  - name: ipi-deprovision
    rules:
    - alert: ipi-deprovision-failures
      annotations:
        message: ipi-deprovision has failures. Check on <https://grafana-prow-monitoring.svc.ci.openshift.org/d/8ce131e226b7fd2901c2fce45d4e21c1/dptp-dashboard?orgId=1&fullscreen&panelId=2|grafana>
          and <https://prow.svc.ci.openshift.org/?type=periodic&job=periodic-ipi-deprovision|deck>
      expr: |
        rate(prowjob_state_transitions{job_name="periodic-ipi-deprovision",state="failure"}[30m]) > 0
      for: 1m
      labels:
        severity: slack
  - name: plank-infra
    rules:
    - alert: plank-job-with-infra-role-failures
      annotations:
        message: plank jobs {{ $labels.job_name }} with infra role has failures. Check
          on <https://grafana-prow-monitoring.svc.ci.openshift.org/d/8ce131e226b7fd2901c2fce45d4e21c1/dptp-dashboard?orgId=1&fullscreen&panelId=3|grafana>
          and <https://prow.svc.ci.openshift.org/|deck>
      expr: |
        sum(rate(prowjob_state_transitions{job="plank",job_name!~"rehearse.*",state="failure"}[5m])) by (job_name) * on (job_name) group_left prow_job_labels{job_agent="kubernetes",label_ci_openshift_io_role="infra"} > 0
      for: 1m
      labels:
        severity: slack
  - name: ghproxy
    rules:
    - alert: ghproxy-status-code-abnormal-4XX
      annotations:
        message: ghproxy has {{ $value | humanize }}% of status code 4XX for over
          1 minute.
      expr: |
        sum(github_request_duration_count{status=~"4..", status!="404"})/sum(github_request_duration_count{status!="404"}) * 100 > 5
      for: 1m
      labels:
        severity: slack
    - alert: ghproxy-status-code-abnormal-5XX
      annotations:
        message: ghproxy has {{ $value | humanize }}% of status code 5XX for over
          1 minute.
      expr: |
        sum(github_request_duration_count{status=~"5..", status!="404"})/sum(github_request_duration_count{status!="404"}) * 100 > 5
      for: 1m
      labels:
        severity: slack
    - alert: ghproxy-running-out-github-tokens-in-a-hour
      annotations:
        message: token {{ $labels.token_hash }} will run out of API quota before the
          next reset.
      expr: |
        github_token_usage{job="ghproxy"} <  1500
        and
        predict_linear(github_token_usage{job="ghproxy"}[1h], 1 * 3600) < 0
      for: 5m
      labels:
        severity: slack
  - name: sinker-missing
    rules:
    - alert: SinkerNotRemovingPods
      annotations:
        message: Sinker has not removed any Pods in the last hour, likely indicating
          an outage in the service.
      expr: |
        absent(sum(rate(sinker_pods_removed[1h]))) == 1
      for: 5m
      labels:
        severity: slack
    - alert: SinkerNotRemovingProwJobs
      annotations:
        message: Sinker has not removed any Prow jobs in the last hour, likely indicating
          an outage in the service.
      expr: |
        absent(sum(rate(sinker_prow_jobs_cleaned[1h]))) == 1
      for: 5m
      labels:
        severity: slack
  - name: prow-job-failures
    rules:
    - alert: periodic-openshift-library-import-failures
      annotations:
        message: Prow job periodic-openshift-library-import has failures. Check on
          <https://prow.svc.ci.openshift.org/?type=periodic&job=periodic-openshift-library-import|deck>
      expr: |
        rate(prowjob_state_transitions{job_name="periodic-openshift-library-import",state="failure"}[5m]) > 0
      for: 1m
      labels:
        severity: slack
        team: developer-experience
