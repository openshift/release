apiVersion: v1
kind: Namespace
metadata:
  name: loki
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: loki-promtail
rules:
- apiGroups:
  - ''
  resources:
  - nodes
  - nodes/proxy
  - services
  - endpoints
  - pods
  - configmaps
  verbs:
  - get
  - watch
  - list
- apiGroups:
  - 'config.openshift.io'
  resources:
  - 'clusterversions'
  verbs:
  - 'get'
- apiGroups:
  - security.openshift.io
  resourceNames:
  - privileged
  resources:
  - securitycontextconstraints
  verbs:
  - use
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: loki-promtail
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: loki-promtail
subjects:
- kind: ServiceAccount
  name: loki-promtail
  namespace: loki
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: loki-promtail
  namespace: loki
spec:
  selector:
    matchLabels:
      app.kubernetes.io/component: log-collector
      app.kubernetes.io/instance: loki-promtail
      app.kubernetes.io/name: promtail
      app.kubernetes.io/part-of: loki
  template:
    metadata:
      labels:
        app.kubernetes.io/component: log-collector
        app.kubernetes.io/instance: loki-promtail
        app.kubernetes.io/name: promtail
        app.kubernetes.io/part-of: loki
        app.kubernetes.io/version: "2.2.1"
      annotations:
        promtail.yaml: |-
          clients:
            - backoff_config:
                max_period: 5m
                max_retries: 20
                min_period: 1s
              batchsize: 102400
              batchwait: 10s
              bearer_token_file: /tmp/shared/prod_bearer_token
              timeout: 10s
              url: https://observatorium.api.openshift.com/api/logs/v1/dptp/loki/api/v1/push
          positions:
            filename: "/run/promtail/positions.yaml"
          scrape_configs:
          - job_name: kubernetes
            kubernetes_sd_configs:
            - role: pod
            pipeline_stages:
            - cri: {}
            - labeldrop:
              - filename
            - pack:
                labels:
                - namespace
                - pod_name
                - container_name
                - app
                - k8s_app
                - name
            - labelallow:
              - host
              - cluster_name
            relabel_configs:
            - action: keep
              regex: '^openshift-.+'
              source_labels:
              - __meta_kubernetes_namespace
            - source_labels:
              - __meta_kubernetes_pod_label_name
              target_label: __service__
            - source_labels:
              - __meta_kubernetes_pod_node_name
              target_label: __host__
            - action: replace
              replacement:
              separator: "/"
              source_labels:
              - __meta_kubernetes_namespace
              - __service__
              target_label: job
            - action: replace
              source_labels:
              - __meta_kubernetes_namespace
              target_label: namespace
            - action: replace
              source_labels:
              - __meta_kubernetes_pod_name
              target_label: pod_name
            - action: replace
              source_labels:
              - __meta_kubernetes_pod_container_name
              target_label: container_name
            - replacement: "/var/log/pods/*$1/*.log"
              separator: "/"
              source_labels:
              - __meta_kubernetes_pod_uid
              - __meta_kubernetes_pod_container_name
              target_label: __path__
            - action: labelmap
              regex: __meta_kubernetes_pod_label_(.+)
          - job_name: journal
            journal:
              path: /var/log/journal
              labels:
                job: systemd-journal
            pipeline_stages:
            - labeldrop:
              - filename
              - stream
            - labeldrop:
              - filename
            - pack:
                labels:
                - boot_id
                - systemd_unit
            - labelallow:
              - host
              - cluster_name
              - job
            relabel_configs:
            - action: labelmap
              regex: __journal__(.+)
          server:
            http_listen_port: 3101
          target_config:
            sync_period: 10s
    spec:
      containers:
      - args:
        - --oidc.client-id=$(CLIENT_ID)
        - --oidc.client-secret=$(CLIENT_SECRET)
        - --oidc.issuer-url=https://sso.redhat.com/auth/realms/redhat-external
        - --margin=10m
        - --file=/tmp/shared/prod_bearer_token
        name: prod-bearer-token
        env:
          - name: CLIENT_ID
            valueFrom:
              secretKeyRef:
                name: promtail-prod-creds
                key: client-id
          - name: CLIENT_SECRET
            valueFrom:
              secretKeyRef:
                name: promtail-prod-creds
                key: client-secret
        volumeMounts:
        - mountPath: "/tmp/shared"
          name: shared-data
        image: quay.io/observatorium/token-refresher
      - command:
        - sh
        - -c
        - |
          promtail \
            -client.external-labels=host=$(HOSTNAME),cluster_name=$(CLUSTER_NAME) \
            -config.file=/etc/promtail/promtail.yaml
        env:
        - name: HOSTNAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: CLUSTER_NAME
          valueFrom:
            configMapKeyRef:
              key: cluster-name
              name: cluster-name
        image: quay.io/openshift-logging/promtail:v2.2.1
        imagePullPolicy: IfNotPresent
        name: promtail
        ports:
        - containerPort: 3101
          name: http-metrics
        readinessProbe:
          failureThreshold: 5
          httpGet:
            path: "/ready"
            port: http-metrics
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        securityContext:
          privileged: true
          readOnlyRootFilesystem: true
          runAsGroup: 0
          runAsUser: 0
        volumeMounts:
        - mountPath: "/etc/promtail"
          name: config
        - mountPath: "/run/promtail"
          name: run
        - mountPath: "/var/lib/docker/containers"
          name: docker
          readOnly: true
        - mountPath: "/var/log/pods"
          name: pods
          readOnly: true
        - mountPath: "/tmp/shared"
          name: shared-data
        - mountPath: "/var/log/journal"
          name: journal
          readOnly: true
      serviceAccountName: loki-promtail
      tolerations:
      - effect: NoSchedule
        key: node-role.kubernetes.io/master
        operator: Exists
      volumes:
      - name: config
        downwardAPI:
          items:
          - path: promtail.yaml
            fieldRef:
              fieldPath: metadata.annotations['promtail.yaml']
      - hostPath:
          path: "/run/promtail"
        name: run
      - hostPath:
          path: "/var/lib/docker/containers"
        name: docker
      - hostPath:
          path: "/var/log/pods"
        name: pods
      - hostPath:
          path: "/var/log/journal"
        name: journal
      - emptyDir: {}
        name: shared-data
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 33%
---
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: loki-promtail
spec:
  allowPrivilegeEscalation: false
  fsGroup:
    rule: RunAsAny
  hostIPC: false
  hostNetwork: false
  hostPID: false
  privileged: false
  readOnlyRootFilesystem: true
  requiredDropCapabilities:
  - ALL
  runAsUser:
    rule: RunAsAny
  seLinux:
    rule: RunAsAny
  supplementalGroups:
    rule: RunAsAny
  volumes:
  - secret
  - configMap
  - hostPath
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: loki-promtail
  namespace: loki
rules:
- apiGroups:
  - extensions
  resourceNames:
  - loki-promtail
  resources:
  - podsecuritypolicies
  verbs:
  - use
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: loki-promtail
  namespace: loki
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: loki-promtail
subjects:
- kind: ServiceAccount
  name: loki-promtail
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: loki-promtail
  namespace: loki
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: promtail-prometheus
  namespace: loki
rules:
- apiGroups:
  - ""
  resources:
  - services
  - endpoints
  - pods
  verbs:
  - get
  - list
  - watch
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: prom-scrape-loki
  namespace: loki
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: promtail-prometheus
subjects:
  - kind: ServiceAccount
    name: prometheus-k8s
    namespace: openshift-monitoring
---
kind: Service
apiVersion: v1
metadata:
  name: promtail
  namespace: loki
  labels:
    app.kubernetes.io/name: promtail
spec:
  ports:
    - name: metrics
      protocol: TCP
      port: 3101
      targetPort: http-metrics
  selector:
    app.kubernetes.io/name: promtail
  type: ClusterIP
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: promtail-monitor
  namespace: openshift-monitoring
spec:
  endpoints:
  - interval: 30s
    port: metrics
    scheme: http
  namespaceSelector:
    matchNames:
      - loki
  selector:
    matchLabels:
      app.kubernetes.io/name: promtail
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: prometheus-loki-rules
  namespace: openshift-monitoring
spec:
  groups:
  - name: loki
    rules:
    - alert: promtail-dropping-data
      annotations:
        message: Promtail is unable to ship logs and dropping data.
      expr: rate(promtail_dropped_bytes_total[1h]) > 50
      for: 1m
      labels:
        severity: critical
