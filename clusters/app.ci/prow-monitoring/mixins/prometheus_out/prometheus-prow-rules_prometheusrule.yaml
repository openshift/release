apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: prow
    role: alert-rules
  name: prometheus-prow-rules
  namespace: prow-monitoring
spec:
  groups:
  - name: mirroring-failures
    rules:
    - alert: mirroring-failures
      annotations:
        message: '@build-cop image mirroring jobs have failed. View failed jobs at
          the <https://prow.ci.openshift.org/?job=periodic-image-mirroring-openshift|overview>.  If
          there is a consistent issue, open a bugzilla bug against Test Infrastructure
          and assign it to Clayton Coleman.'
      expr: |
        increase(prowjob_state_transitions{job_name="periodic-image-mirroring-openshift",state="failure"}[5m]) > 0
      for: 1m
      labels:
        severity: critical
        team: build-cop
  - name: ci-absent
    rules:
    - alert: deckDown
      annotations:
        message: The service deck has been down for 5 minutes.
      expr: |
        absent(up{job="deck"} == 1)
      for: 5m
      labels:
        severity: critical
    - alert: deck-internalDown
      annotations:
        message: The service deck-internal has been down for 5 minutes.
      expr: |
        absent(up{job="deck-internal"} == 1)
      for: 5m
      labels:
        severity: critical
    - alert: hook-appsDown
      annotations:
        message: The service hook-apps has been down for 5 minutes.
      expr: |
        absent(up{job="hook-apps"} == 1)
      for: 5m
      labels:
        severity: critical
    - alert: jenkins-operatorDown
      annotations:
        message: The service jenkins-operator has been down for 5 minutes.
      expr: |
        absent(up{job="jenkins-operator"} == 1)
      for: 5m
      labels:
        severity: critical
    - alert: kata-jenkins-operatorDown
      annotations:
        message: The service kata-jenkins-operator has been down for 5 minutes.
      expr: |
        absent(up{job="kata-jenkins-operator"} == 1)
      for: 5m
      labels:
        severity: critical
    - alert: prow-controller-managerDown
      annotations:
        message: The service prow-controller-manager has been down for 5 minutes.
      expr: |
        absent(up{job="prow-controller-manager"} == 1)
      for: 5m
      labels:
        severity: critical
    - alert: sinkerDown
      annotations:
        message: The service sinker has been down for 5 minutes.
      expr: |
        absent(up{job="sinker"} == 1)
      for: 5m
      labels:
        severity: critical
    - alert: tideDown
      annotations:
        message: The service tide has been down for 5 minutes.
      expr: |
        absent(up{job="tide"} == 1)
      for: 5m
      labels:
        severity: critical
    - alert: dptp-controller-managerDown
      annotations:
        message: The service dptp-controller-manager has been down for 5 minutes.
      expr: |
        absent(up{job="dptp-controller-manager"} == 1)
      for: 5m
      labels:
        severity: critical
    - alert: crierDown
      annotations:
        message: The service crier has been down for 5 minutes.
      expr: |
        absent(up{job="crier"} == 1)
      for: 5m
      labels:
        severity: critical
  - name: ci-absent-ghproxy
    rules:
    - alert: ghproxyDown
      annotations:
        message: The service ghproxy has been down for 10 minutes.
      expr: |
        absent(up{job="ghproxy"} == 1)
      for: 10m
      labels:
        severity: critical
  - name: prow-monitoring-absent
    rules:
    - alert: ServiceLostHA
      annotations:
        message: The service {{ $labels.job }} has at most 1 instance for 5 minutes.
      expr: |
        sum(up{job=~"grafana|prometheus|alertmanager"}) by (job) <= 1
      for: 5m
      labels:
        severity: critical
    - alert: AlertManagerPodDown
      annotations:
        message: Not all of 3 alert manager pods have been running in the last 5 minutes.
      expr: |
        sum(up{job="alertmanager"}) < 3
      for: 5m
      labels:
        severity: critical
    - alert: grafanaDown
      annotations:
        message: The service grafana has been down for 5 minutes.
      expr: |
        absent(up{job="grafana"} == 1)
      for: 5m
      labels:
        severity: critical
    - alert: alertmanagerDown
      annotations:
        message: The service alertmanager has been down for 5 minutes.
      expr: |
        absent(up{job="alertmanager"} == 1)
      for: 5m
      labels:
        severity: critical
    - alert: prometheusDown
      annotations:
        message: The service prometheus has been down for 5 minutes.
      expr: |
        absent(up{job="prometheus"} == 1)
      for: 5m
      labels:
        severity: critical
    - alert: blackboxDown
      annotations:
        message: The service blackbox has been down for 5 minutes.
      expr: |
        absent(up{job="blackbox"} == 1)
      for: 5m
      labels:
        severity: critical
  - name: abnormal webhook behaviors
    rules:
    - alert: no-webhook-calls
      annotations:
        message: There have been no webhook calls on working hours for 5 minutes
      expr: |
        (sum(increase(prow_webhook_counter[1m])) == 0 or absent(prow_webhook_counter))
        and ( (day_of_week() > 0) and (day_of_week() < 6) and (hour() >= 7) )
      for: 5m
      labels:
        severity: critical
  - name: configmap-full
    rules:
    - alert: ConfigMapFullInOneWeek
      annotations:
        message: Based on recent sampling, the ConfigMap {{ $labels.name }} in Namespace
          {{ $labels.namespace }} is expected to fill up within a week. Currently
          {{ printf "%0.2f" $value }}% is available.
      expr: |
        100 * ((1048576 - prow_configmap_size_bytes) / 1048576) < 15
        and
        predict_linear(prow_configmap_size_bytes[12h], 7 * 24 * 3600) > 1048576
      for: 5m
      labels:
        severity: critical
  - name: plank-infra
    rules:
    - alert: ci-tools-postsubmit-failures
      annotations:
        message: ci-tools postsubmit {{ $labels.job_name }} has failures. Check <https://prow.ci.openshift.org/?job={{
          $labels.job_name }}|deck>.
      expr: |
        sum by (job_name) (
          rate(
            prowjob_state_transitions{job="prow-controller-manager",job_name!~"rehearse.*",state="failure"}[5m]
          )
        )
        * on (job_name) group_left max by (job_name) (prow_job_labels{job_agent="kubernetes",label_ci_openshift_io_metadata_target="e2e-oo-post"}) > 0
      for: 1m
      labels:
        severity: critical
    - alert: plank-job-with-infra-role-failures
      annotations:
        message: plank jobs {{ $labels.job_name }} with infra role has failures. Check
          on <https://grafana-prow-monitoring.apps.ci.l2s4.p1.openshiftapps.com/d/8ce131e226b7fd2901c2fce45d4e21c1/dptp-dashboard?orgId=1&fullscreen&panelId=4|grafana>
          and <https://prow.ci.openshift.org/?job={{ $labels.job_name }}|deck>.
      expr: |
        sum by (job_name) (
          rate(
            prowjob_state_transitions{job="prow-controller-manager",job_name!~"rehearse.*",state="failure"}[5m]
          )
        )
        * on (job_name) group_left max by (job_name) (prow_job_labels{job_agent="kubernetes",label_ci_openshift_io_role="infra"}) > 0
      for: 1m
      labels:
        severity: critical
    - alert: plank-job-with-infra-internal-role-failures
      annotations:
        message: plank jobs {{ $labels.job_name }} with infra-internal role has failures.
          Check on <https://deck-internal-ci.apps.ci.l2s4.p1.openshiftapps.com/?job={{
          $labels.job_name }}|deck-internal>.
      expr: |
        sum by (job_name) (
          rate(
            prowjob_state_transitions{job="prow-controller-manager",job_name!~"rehearse.*",state="failure"}[5m]
          )
        )
        * on (job_name) group_left max by (job_name) (prow_job_labels{job_agent="kubernetes",label_ci_openshift_io_role="infra-internal"}) > 0
      for: 1m
      labels:
        severity: critical
  - name: cluster-client-creation-failure
    rules:
    - alert: component-failed-to-construct-a-client
      annotations:
        message: Component {{ $labels.service }} failed to construct a client. Search
          for "failed to construct {manager,client} for cluster <<clustername>>" in
          the beginning of the pod log. If the cluster unavailability was transient,
          a restart of the pod will fix the issue.
      expr: kubernetes_failed_client_creations > 0
      labels:
        severity: critical
  - name: ci-operator-infra-error
    rules:
    - alert: high-ci-operator-infra-error-rate
      annotations:
        message: An excessive amount of CI Operator executions are failing with {{
          $labels.reason }}, which is an infrastructure issue.
      expr: |
        sum(rate(ci_operator_error_rate{state="failed",reason!~".*cloning_source",reason!~".*executing_template",reason!~".*executing_multi_stage_test",reason!~".*building_image_from_source",reason!~".*building_.*_image",reason!="executing_graph:interrupted"}[30m])) by (reason) > 0.02
      for: 1m
      labels:
        severity: critical
  - name: ci-operator-error
    rules:
    - alert: high-ci-operator-error-rate
      annotations:
        message: An excessive amount of CI Operator executions are failing with {{
          $labels.reason }}, which does not necessarily point to an infrastructure
          issue but is happening at an excessive rate and should be investigated.
      expr: |
        sum(rate(ci_operator_error_rate{state="failed"}[30m])) by (reason) > 0.07
      for: 1m
      labels:
        severity: critical
  - name: jobs-failing-with-lease-acquire-timeout
    rules:
    - alert: jobs-failing-with-lease-acquire-timeout
      annotations:
        message: Jobs on provider {{ $labels.provider }} fail because they were unable
          to acquire a lease.
      expr: |
        label_replace((sum(increase(ci_operator_error_rate{state="failed",reason=~"executing_graph:step_failed:utilizing_lease:acquiring_lease:.*"}[15m])) by (reason)), "provider", "$1", "reason", "executing_graph:step_failed:utilizing_lease:acquiring_lease:(.*)-quota-slice") > 0
      for: 1m
      labels:
        severity: critical
  - name: http-probe
    rules:
    - alert: SSLCertExpiringSoon
      annotations:
        message: The SSL certificates for instance {{ $labels.instance }} are expiring
          in 28 days.
      expr: |
        probe_ssl_earliest_cert_expiry{job="blackbox"} - time() < 86400 * 28
      for: 1m
      labels:
        severity: critical
    - alert: ProbeFailing
      annotations:
        message: Probing the instance {{ $labels.instance }} has been failing for
          the past minute.
      expr: |
        up{job="blackbox"} == 0 or probe_success{job="blackbox"} == 0
      for: 1m
      labels:
        severity: critical
  - name: ghproxy
    rules:
    - alert: ghproxy-specific-status-code-abnormal
      annotations:
        message: '{{ $value | humanize }}% of all requests for {{ $labels.path }}
          through the GitHub proxy are errorring with code {{ $labels.status }}. Check
          <https://grafana-prow-monitoring.apps.ci.l2s4.p1.openshiftapps.com/d/d72fe8d0400b2912e319b1e95d0ab1b3/github-cache?orgId=1&refresh=1m&fullscreen&panelId=9|grafana>'
      expr: |
        sum(rate(github_request_duration_count{status=~"[45]..",status!="404",status!="410"}[5m])) by (status,path) / ignoring(status) group_left sum(rate(github_request_duration_count[5m])) by (path) * 100 > 10
      labels:
        severity: warning
    - alert: ghproxy-global-status-code-abnormal
      annotations:
        message: '{{ $value | humanize }}% of all API requests through the GitHub
          proxy are errorring with code {{ $labels.status }}. Check <https://grafana-prow-monitoring.apps.ci.l2s4.p1.openshiftapps.com/d/d72fe8d0400b2912e319b1e95d0ab1b3/github-cache?orgId=1&fullscreen&panelId=8|grafana>'
      expr: |
        sum(rate(github_request_duration_count{status=~"[45]..",status!="404",status!="410"}[5m])) by (status) / ignoring(status) group_left sum(rate(github_request_duration_count[5m])) * 100 > 3
      labels:
        severity: warning
    - alert: ghproxy-running-out-github-tokens-in-a-hour
      annotations:
        message: '{{ $labels.login }} may run out of API quota before the next reset.
          Check the <https://grafana-prow-monitoring.apps.ci.l2s4.p1.openshiftapps.com/d/d72fe8d0400b2912e319b1e95d0ab1b3/github-cache?orgId=1|dashboard>'
      expr: |
        github_token_usage * on(token_hash) group_left(login) max(github_user_info{login=~"openshift-.*"}) by (token_hash, login) + deriv(github_token_usage[20m]) * github_token_reset * on(token_hash) group_left(login) max(github_user_info{login=~"openshift-.*"}) by (token_hash, login) / 1e9 < 100
      for: 5m
      labels:
        severity: critical
    - alert: ghproxy-running-out-github-tokens-in-a-hour
      annotations:
        message: '{{ $labels.token_hash }} may run out of API quota before the next
          reset. Check the <https://grafana-prow-monitoring.apps.ci.l2s4.p1.openshiftapps.com/d/d72fe8d0400b2912e319b1e95d0ab1b3/github-cache?orgId=1|dashboard>'
      expr: |
        github_token_usage{token_hash=~"openshift-ci - .*"} + deriv(github_token_usage{token_hash=~"openshift-ci - .*"}[20m]) * github_token_reset  / 1e9 < 100
      for: 5m
      labels:
        severity: critical
    - alert: ghproxy-90-inode-percent
      annotations:
        message: '{{ $labels.token_hash }} uses 90% of the available inodes. Check
          <ghcache_disk_inode_used / ghcache_disk_inode_total * 100 |prometheus>'
      expr: |
        ghcache_disk_inode_used / ghcache_disk_inode_total * 100 > 90
      for: 5m
      labels:
        severity: critical
  - name: sinker-missing
    rules:
    - alert: SinkerNotRemovingPods
      annotations:
        message: Sinker has not removed any Pods in the last hour, likely indicating
          an outage in the service.
      expr: |
        absent(sum(rate(sinker_pods_removed[1h]))) == 1
      for: 5m
      labels:
        severity: critical
    - alert: SinkerNotRemovingProwJobs
      annotations:
        message: Sinker has not removed any Prow jobs in the last hour, likely indicating
          an outage in the service.
      expr: |
        absent(sum(rate(sinker_prow_jobs_cleaned[1h]))) == 1
      for: 5m
      labels:
        severity: critical
  - name: prow-job-failures
    rules:
    - alert: endurance-cluster-maintenance-aws-4.6-failures
      annotations:
        message: '@bparees Prow job endurance-cluster-maintenance-aws-4.6 has failures.
          Check on <https://prow.ci.openshift.org/?type=periodic&job=endurance-cluster-maintenance-aws-4.6|deck>'
      expr: |
        rate(prowjob_state_transitions{job_name="endurance-cluster-maintenance-aws-4.6",state="failure"}[5m]) > 0
      for: 1m
      labels:
        severity: critical
        team: build-cop
    - alert: periodic-ci-kubevirt-hyperconverged-cluster-operator-release-4.5-hco-e2e-nightly-bundle-release-4-5-azure4-failures
      annotations:
        message: '@build-officer Prow job periodic-ci-kubevirt-hyperconverged-cluster-operator-release-4.5-hco-e2e-nightly-bundle-release-4-5-azure4
          has failures. Check on <https://prow.ci.openshift.org/?type=periodic&job=periodic-ci-kubevirt-hyperconverged-cluster-operator-release-4.5-hco-e2e-nightly-bundle-release-4-5-azure4|deck>'
      expr: |
        rate(prowjob_state_transitions{job_name="periodic-ci-kubevirt-hyperconverged-cluster-operator-release-4.5-hco-e2e-nightly-bundle-release-4-5-azure4",state="failure"}[5m]) > 0
      for: 1m
      labels:
        severity: critical
        team: openshift-virtualization
    - alert: periodic-ci-openshift-kni-cnf-features-deploy-master-cnf-e2e-gcp-periodic-failures
      annotations:
        message: '@cnf-cop Prow job periodic-ci-openshift-kni-cnf-features-deploy-master-cnf-e2e-gcp-periodic
          has failures. Check on <https://prow.ci.openshift.org/?type=periodic&job=periodic-ci-openshift-kni-cnf-features-deploy-master-cnf-e2e-gcp-periodic|deck>'
      expr: |
        rate(prowjob_state_transitions{job_name="periodic-ci-openshift-kni-cnf-features-deploy-master-cnf-e2e-gcp-periodic",state="failure"}[5m]) > 0
      for: 1m
      labels:
        severity: critical
        team: cnf-cop
    - alert: periodic-ci-openshift-kni-cnf-features-deploy-release-4.4-cnf-e2e-gcp-periodic-failures
      annotations:
        message: '@cnf-cop Prow job periodic-ci-openshift-kni-cnf-features-deploy-release-4.4-cnf-e2e-gcp-periodic
          has failures. Check on <https://prow.ci.openshift.org/?type=periodic&job=periodic-ci-openshift-kni-cnf-features-deploy-release-4.4-cnf-e2e-gcp-periodic|deck>'
      expr: |
        rate(prowjob_state_transitions{job_name="periodic-ci-openshift-kni-cnf-features-deploy-release-4.4-cnf-e2e-gcp-periodic",state="failure"}[5m]) > 0
      for: 1m
      labels:
        severity: critical
        team: cnf-cop
    - alert: periodic-ci-openshift-kni-cnf-features-deploy-release-4.5-cnf-e2e-gcp-periodic-failures
      annotations:
        message: '@cnf-cop Prow job periodic-ci-openshift-kni-cnf-features-deploy-release-4.5-cnf-e2e-gcp-periodic
          has failures. Check on <https://prow.ci.openshift.org/?type=periodic&job=periodic-ci-openshift-kni-cnf-features-deploy-release-4.5-cnf-e2e-gcp-periodic|deck>'
      expr: |
        rate(prowjob_state_transitions{job_name="periodic-ci-openshift-kni-cnf-features-deploy-release-4.5-cnf-e2e-gcp-periodic",state="failure"}[5m]) > 0
      for: 1m
      labels:
        severity: critical
        team: cnf-cop
    - alert: periodic-ci-operator-framework-operator-lifecycle-managment-master-rhoperator-metric-e2e-aws-olm-master-daily-failures
      annotations:
        message: '@olmcop Prow job periodic-ci-operator-framework-operator-lifecycle-managment-master-rhoperator-metric-e2e-aws-olm-master-daily
          has failures. Check on <https://prow.ci.openshift.org/?type=periodic&job=periodic-ci-operator-framework-operator-lifecycle-managment-master-rhoperator-metric-e2e-aws-olm-master-daily|deck>'
      expr: |
        rate(prowjob_state_transitions{job_name="periodic-ci-operator-framework-operator-lifecycle-managment-master-rhoperator-metric-e2e-aws-olm-master-daily",state="failure"}[5m]) > 0
      for: 1m
      labels:
        severity: critical
        team: OLM
    - alert: periodic-ci-operator-framework-operator-lifecycle-managment-rhoperator-metric-e2e-aws-olm-release-4.4-daily-failures
      annotations:
        message: '@olmcop Prow job periodic-ci-operator-framework-operator-lifecycle-managment-rhoperator-metric-e2e-aws-olm-release-4.4-daily
          has failures. Check on <https://prow.ci.openshift.org/?type=periodic&job=periodic-ci-operator-framework-operator-lifecycle-managment-rhoperator-metric-e2e-aws-olm-release-4.4-daily|deck>'
      expr: |
        rate(prowjob_state_transitions{job_name="periodic-ci-operator-framework-operator-lifecycle-managment-rhoperator-metric-e2e-aws-olm-release-4.4-daily",state="failure"}[5m]) > 0
      for: 1m
      labels:
        severity: critical
        team: OLM
    - alert: periodic-ci-operator-framework-operator-lifecycle-managment-rhoperator-metric-e2e-aws-olm-release-4.5-daily-failures
      annotations:
        message: '@olmcop Prow job periodic-ci-operator-framework-operator-lifecycle-managment-rhoperator-metric-e2e-aws-olm-release-4.5-daily
          has failures. Check on <https://prow.ci.openshift.org/?type=periodic&job=periodic-ci-operator-framework-operator-lifecycle-managment-rhoperator-metric-e2e-aws-olm-release-4.5-daily|deck>'
      expr: |
        rate(prowjob_state_transitions{job_name="periodic-ci-operator-framework-operator-lifecycle-managment-rhoperator-metric-e2e-aws-olm-release-4.5-daily",state="failure"}[5m]) > 0
      for: 1m
      labels:
        severity: critical
        team: OLM
    - alert: periodic-openshift-library-import-failures
      annotations:
        message: '@devex Prow job periodic-openshift-library-import has failures.
          Check on <https://prow.ci.openshift.org/?type=periodic&job=periodic-openshift-library-import|deck>'
      expr: |
        rate(prowjob_state_transitions{job_name="periodic-openshift-library-import",state="failure"}[5m]) > 0
      for: 1m
      labels:
        severity: critical
        team: developer-experience
    - alert: release-openshift-ocp-installer-cluster-logging-operator-e2e-4.5-failures
      annotations:
        message: '@aoslogging Prow job release-openshift-ocp-installer-cluster-logging-operator-e2e-4.5
          has failures. Check on <https://prow.ci.openshift.org/?type=periodic&job=release-openshift-ocp-installer-cluster-logging-operator-e2e-4.5|deck>'
      expr: |
        rate(prowjob_state_transitions{job_name="release-openshift-ocp-installer-cluster-logging-operator-e2e-4.5",state="failure"}[5m]) > 0
      for: 1m
      labels:
        severity: critical
        team: logging
    - alert: release-openshift-ocp-installer-e2e-gcp-rt-4.4-failures
      annotations:
        message: '@cnf-cop Prow job release-openshift-ocp-installer-e2e-gcp-rt-4.4
          has failures. Check on <https://prow.ci.openshift.org/?type=periodic&job=release-openshift-ocp-installer-e2e-gcp-rt-4.4|deck>'
      expr: |
        rate(prowjob_state_transitions{job_name="release-openshift-ocp-installer-e2e-gcp-rt-4.4",state="failure"}[5m]) > 0
      for: 1m
      labels:
        severity: critical
        team: cnf-cop
    - alert: release-openshift-ocp-installer-e2e-gcp-rt-4.5-failures
      annotations:
        message: '@cnf-cop Prow job release-openshift-ocp-installer-e2e-gcp-rt-4.5
          has failures. Check on <https://prow.ci.openshift.org/?type=periodic&job=release-openshift-ocp-installer-e2e-gcp-rt-4.5|deck>'
      expr: |
        rate(prowjob_state_transitions{job_name="release-openshift-ocp-installer-e2e-gcp-rt-4.5",state="failure"}[5m]) > 0
      for: 1m
      labels:
        severity: critical
        team: cnf-cop
    - alert: release-openshift-ocp-installer-elasticsearch-operator-e2e-4.5-failures
      annotations:
        message: '@aoslogging Prow job release-openshift-ocp-installer-elasticsearch-operator-e2e-4.5
          has failures. Check on <https://prow.ci.openshift.org/?type=periodic&job=release-openshift-ocp-installer-elasticsearch-operator-e2e-4.5|deck>'
      expr: |
        rate(prowjob_state_transitions{job_name="release-openshift-ocp-installer-elasticsearch-operator-e2e-4.5",state="failure"}[5m]) > 0
      for: 1m
      labels:
        severity: critical
        team: logging
  - name: prow
    rules:
    - alert: prow-pod-crashlooping
      annotations:
        message: Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container}})
          is restarting {{ printf "%.2f" $value }} times / 5 minutes.
      expr: rate(kube_pod_container_status_restarts_total{namespace=~"ci|prow-monitoring",job="kube-state-metrics"}[15m])
        * 60 * 5 > 0
      for: 1m
      labels:
        severity: critical
    - alert: prow-job-backlog-growing
      annotations:
        message: The number of the triggered Prow jobs that have not yet been running
          has been increasing for the past hour.
      expr: sum(rate(prowjob_state_transitions{state="triggered"}[5m])) - sum(rate(prowjob_state_transitions{state!="triggered"}[5m]))
        > 0
      for: 60m
      labels:
        severity: critical
    - expr: count(label_replace(count(github_token_usage{token_hash =~ "openshift.*"})
        by (token_hash), "login", "$1", "token_hash", "(.*)") or github_user_info{login=~"openshift-.*"})
        by (login)
      record: github:identity_names
  - name: release-controller-down
    rules:
    - alert: releaseControllerDown
      annotations:
        message: '{{ $labels.deployment }} has been down for 5 minutes.'
      expr: kube_deployment_status_replicas_unavailable{namespace="ci", deployment=~"release-controller.*"}
        >= 1
      for: 5m
      labels:
        severity: critical
  - name: abnormal Tide sync durations
    rules:
    - alert: abnormal Tide controller sync duration
      annotations:
        message: The Tide sync duration is abnormally high (>45 seconds). Check <https://prometheus-prow-monitoring.apps.ci.l2s4.p1.openshiftapps.com/graph?g0.range_input=1h&g0.expr=max(syncdur%20and%20(changes(syncdur%5B1h%5D)%20%3E%200))%20%3E%2045&g0.tab=0|Prometheus>
      expr: |
        max(syncdur and (changes(syncdur[1h]) > 0)) > 45
      for: 5m
      labels:
        severity: critical
    - alert: abnormal Tide status controller sync duration
      annotations:
        message: The Tide status sync duration is abnormally high (>30 seconds). Check
          <https://prometheus-prow-monitoring.apps.ci.l2s4.p1.openshiftapps.com/graph?g0.range_input=1h&g0.expr=max(statusupdatedur%20and%20(changes(statusupdatedur%5B1h%5D)%20%3E%200))%20%3E%2030&g0.tab=1|Prometheus>
      expr: |
        max(statusupdatedur and (changes(statusupdatedur[1h]) > 0)) > 30
      for: 5m
      labels:
        severity: critical
