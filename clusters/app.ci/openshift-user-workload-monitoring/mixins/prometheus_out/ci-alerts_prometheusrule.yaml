apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: ci-alerts
  namespace: ci
spec:
  groups:
  - name: ci-absent
    rules:
    - alert: deck-Down
      annotations:
        message: The service deck has been down for 5 minutes.
      expr: |
        absent(up{job="deck"} == 1)
      for: 5m
      labels:
        severity: critical
    - alert: deck-internal-Down
      annotations:
        message: The service deck-internal has been down for 5 minutes.
      expr: |
        absent(up{job="deck-internal"} == 1)
      for: 5m
      labels:
        severity: critical
    - alert: qe-private-deck-Down
      annotations:
        message: The service qe-private-deck has been down for 5 minutes.
      expr: |
        absent(up{job="qe-private-deck"} == 1)
      for: 5m
      labels:
        severity: critical
    - alert: hook-apps-Down
      annotations:
        message: The service hook-apps has been down for 5 minutes.
      expr: |
        absent(up{job="hook-apps"} == 1)
      for: 5m
      labels:
        severity: critical
    - alert: pod-scaler-ui-Down
      annotations:
        message: The service pod-scaler-ui has been down for 5 minutes.
      expr: |
        absent(up{job="pod-scaler-ui"} == 1)
      for: 5m
      labels:
        severity: critical
    - alert: pj-rehearse-plugin-Down
      annotations:
        message: The service pj-rehearse-plugin has been down for 5 minutes.
      expr: |
        absent(up{job="pj-rehearse-plugin"} == 1)
      for: 5m
      labels:
        severity: critical
    - alert: crier-Singleton-Down
      annotations:
        message: The service crier has been down for 10 minutes.
      expr: |
        absent(up{job="crier"} == 1)
      for: 10m
      labels:
        severity: critical
    - alert: ghproxy-Singleton-Down
      annotations:
        message: The service ghproxy has been down for 10 minutes.
      expr: |
        absent(up{job="ghproxy"} == 1)
      for: 10m
      labels:
        severity: critical
    - alert: prow-controller-manager-Singleton-Down
      annotations:
        message: The service prow-controller-manager has been down for 10 minutes.
      expr: |
        absent(up{job="prow-controller-manager"} == 1)
      for: 10m
      labels:
        severity: critical
    - alert: sinker-Singleton-Down
      annotations:
        message: The service sinker has been down for 10 minutes.
      expr: |
        absent(up{job="sinker"} == 1)
      for: 10m
      labels:
        severity: critical
    - alert: tide-Singleton-Down
      annotations:
        message: The service tide has been down for 10 minutes.
      expr: |
        absent(up{job="tide"} == 1)
      for: 10m
      labels:
        severity: critical
    - alert: dptp-controller-manager-Singleton-Down
      annotations:
        message: The service dptp-controller-manager has been down for 10 minutes.
      expr: |
        absent(up{job="dptp-controller-manager"} == 1)
      for: 10m
      labels:
        severity: critical
    - alert: pod-scaler-producer-Singleton-Down
      annotations:
        message: The service pod-scaler-producer has been down for 10 minutes.
      expr: |
        absent(up{job="pod-scaler-producer"} == 1)
      for: 10m
      labels:
        severity: critical
    - alert: retester-Singleton-Down
      annotations:
        message: The service retester has been down for 10 minutes.
      expr: |
        absent(up{job="retester"} == 1)
      for: 10m
      labels:
        severity: critical
  - name: abnormal webhook behaviors
    rules:
    - alert: no-webhook-calls
      annotations:
        message: There have been no webhook calls on working hours for 5 minutes
      expr: |
        (sum(increase(prow_webhook_counter[1m])) == 0 or absent(prow_webhook_counter))
        and ( (day_of_week() > 0) and (day_of_week() < 6) and (hour() >= 7) )
      for: 5m
      labels:
        severity: critical
  - name: configmap-full
    rules:
    - alert: ConfigMapFullInOneWeek
      annotations:
        message: Based on recent sampling, the ConfigMap {{ $labels.name }} in Namespace {{ $labels.namespace }} is expected to fill up within a week. Currently {{ printf "%0.2f" $value }}% is available.
      expr: |
        100 * ((1048576 - prow_configmap_size_bytes) / 1048576) < 15
        and
        predict_linear(prow_configmap_size_bytes[12h], 7 * 24 * 3600) > 1048576
      for: 5m
      labels:
        severity: critical
  - name: plank-infra
    rules:
    - alert: ci-tools-postsubmit-failures
      annotations:
        message: ci-tools postsubmit {{ $labels.job_name }} has failures. Check <https://prow.ci.openshift.org/?job={{ $labels.job_name }}|deck>.
      expr: |
        sum by (job_name) (
          rate(
            prowjob_state_transitions{job="prow-controller-manager",job_name!~"rehearse.*",state="failure"}[5m]
          )
        )
        * on (job_name) group_left max by (job_name) (prow_job_labels{job_agent="kubernetes",label_ci_openshift_io_metadata_target="e2e-oo-post"}) > 0
      for: 1m
      labels:
        severity: critical
    - alert: infrastructure-job-failures
      annotations:
        message: Infrastructure CI job {{ $labels.job_name }} is failing. Investigate the symptoms, assess the urgency and take appropriate action (<https://grafana-route-ci-grafana.apps.ci.l2s4.p1.openshiftapps.com/d/8ce131e226b7fd2901c2fce45d4e21c1/dptp-dashboard?orgId=1&fullscreen&viewPanel=4|Grafana Dashboard> | <https://prow.ci.openshift.org/?job={{ $labels.job_name }}|Deck> | <https://github.com/openshift/release/blob/master/docs/dptp-triage-sop/infrastructure-jobs.md#{{ $labels.job_name}}|SOP>).
      expr: |
        sum by (job_name) (
          rate(
            prowjob_state_transitions{job="prow-controller-manager",job_name!~"rehearse.*",state="failure"}[5m]
          )
        )
        * on (job_name) group_left max by (job_name) (prow_job_labels{job_agent="kubernetes",label_ci_openshift_io_role="infra"}) > 0
      for: 1m
      labels:
        severity: critical
    - alert: plank-job-with-infra-internal-role-failures
      annotations:
        message: plank jobs {{ $labels.job_name }} with infra-internal role has failures. Check on <https://deck-internal-ci.apps.ci.l2s4.p1.openshiftapps.com/?job={{ $labels.job_name }}|deck-internal>. See <https://github.com/openshift/release/blob/master/docs/dptp-triage-sop/infrastructure-jobs.md#{{ $labels.job_name}}|SOP>.
      expr: |
        sum by (job_name) (
          rate(
            prowjob_state_transitions{job="prow-controller-manager",job_name!~"rehearse.*",state="failure"}[5m]
          )
        )
        * on (job_name) group_left max by (job_name) (prow_job_labels{job_agent="kubernetes",label_ci_openshift_io_role="infra-internal"}) > 0
      for: 1m
      labels:
        severity: critical
  - name: quay-io-image-mirroring
    rules:
    - alert: quay-io-image-mirroring-failures
      annotations:
        message: 'Many mirroring tasks to quay.io have been failed in the last minute. Please check errors in the pod logs to figure out the cause: <https://github.com/openshift/release/blob/master/docs/dptp-triage-sop/misc.md#quay-io-image-mirroring-failures|SOP>.'
        runbook_url: https://github.com/openshift/release/blob/master/docs/dptp-triage-sop/misc.md#quay-io-image-mirroring-failures
      expr: sum(rate(quay_io_ci_images_distributor_image_mirroring_duration_seconds_count{state="failure"}[10m])) > 0.5
      for: 1m
      labels:
        severity: critical
  - name: cluster-client-creation-failure
    rules:
    - alert: component-failed-to-construct-a-client
      annotations:
        message: Component {{ $labels.service }} failed to construct a client. Search for "failed to construct {manager,client} for cluster <<clustername>>" in the beginning of the pod log. If the cluster unavailability was transient, a restart of the pod will fix the issue.
      expr: kubernetes_failed_client_creations > 0
      labels:
        severity: critical
  - name: ci-operator-infra-error
    rules:
    - alert: high-ci-operator-infra-error-rate
      annotations:
        message: An excessive amount of CI Operator executions are failing with `{{ $labels.reason }}`, which is an infrastructure issue. See <https://search.dptools.openshift.org/?search=Reporting+job+state.*with+reason.*{{ $labels.reason }}&maxAge=6h&context=1&type=build-log&name=&excludeName=&maxMatches=5&maxBytes=20971520&groupBy=job|CI search>.
      expr: |
        sum(rate(ci_operator_error_rate{job_name!~"rehearse.*",state="failed",reason!~".*cloning_source",reason!~".*executing_template",reason!~".*executing_multi_stage_test",reason!~".*building_image_from_source",reason!~".*building_.*_image",reason!="executing_graph:interrupted"}[30m])) by (reason) > 0.02
      for: 1m
      labels:
        severity: critical
  - name: ci-operator-error
    rules:
    - alert: high-ci-operator-error-rate
      annotations:
        message: An excessive amount of CI Operator executions are failing with `{{ $labels.reason }}`, which does not necessarily point to an infrastructure issue but is happening at an excessive rate and should be investigated. See <https://search.dptools.openshift.org/?search=Reporting+job+state.*with+reason.*{{ $labels.reason }}&maxAge=6h&context=1&type=build-log&name=&excludeName=&maxMatches=5&maxBytes=20971520&groupBy=job|CI search>.
      expr: |
        sum(rate(ci_operator_error_rate{state="failed"}[30m])) by (reason) > 0.07
      for: 1m
      labels:
        severity: critical
  - name: jobs-failing-with-lease-acquire-timeout
    rules:
    - alert: jobs-failing-with-lease-acquire-timeout
      annotations:
        message: Jobs on provider {{ $labels.provider }} fail because they were unable to acquire a lease.
      expr: |
        label_replace((sum(increase(ci_operator_error_rate{state="failed",reason=~"executing_graph:step_failed:utilizing_lease:acquiring_lease:.*"}[15m])) by (reason)), "provider", "$1", "reason", "executing_graph:step_failed:utilizing_lease:acquiring_lease:(.*)-quota-slice") > 0
      for: 1m
      labels:
        severity: critical
  - name: http-probe
    rules:
    - alert: SSLCertExpiringSoon
      annotations:
        message: The SSL certificates for instance {{ $labels.instance }} are expiring in 28 days.
      expr: |
        probe_ssl_earliest_cert_expiry{job="blackbox"} - time() < 86400 * 28
      for: 1m
      labels:
        severity: critical
    - alert: ProbeFailing
      annotations:
        message: Probing the instance {{ $labels.instance }} has been failing for the past minute.
      expr: |
        up{job="blackbox"} == 0 or probe_success{job="blackbox"} == 0
      for: 1m
      labels:
        severity: critical
    - alert: ProbeFailing-Lenient
      annotations:
        message: Probing the instance {{ $labels.instance }} has been failing for the past five minutes.
      expr: |
        up{job="blackbox-lenient"} == 0 or probe_success{job="blackbox-lenient"} == 0
      for: 5m
      labels:
        severity: critical
  - name: openshift-priv-image-building-jobs-failing
    rules:
    - alert: openshift-priv-image-building-jobs-failing
      annotations:
        message: openshift-priv image-building jobs are failing at a high rate. Check on <https://deck-internal-ci.apps.ci.l2s4.p1.openshiftapps.com/?job=branch-ci-*-images|deck-internal>. See <https://github.com/openshift/release/blob/master/docs/dptp-triage-sop/openshift-priv-image-building-jobs.md|SOP>.
      expr: |
        (
          sum(
            rate(prowjob_state_transitions{job="prow-controller-manager",job_name=~"branch-ci-.*-images",org="openshift-priv",state="success"}[12h])
          )
          /
          sum(
            rate(prowjob_state_transitions{job="prow-controller-manager",job_name=~"branch-ci-.*-images",org="openshift-priv",state=~"success|failure|aborted"}[12h])
          )
        )
        < 0.90
      for: 1m
      labels:
        severity: critical
  - name: pod-scaler-admission-resource-warning
    rules:
    - alert: pod-scaler-admission-resource-warning
      annotations:
        message: 'Workload {{ $labels.workload_name }} ({{ $labels.workload_type }}) used 10x more than configured amount of {{ $labels.resource_type }} (actual: {{ $labels.determined_amount }}, configured: {{ $labels.configured_amount }}. See <https://github.com/openshift/release/blob/master/docs/dptp-triage-sop/pod-scaler-admission.md|SOP>.'
      expr: |
        sum by (workload_name, workload_type, determined_amount, configured_amount, resource_type) (increase(pod_scaler_admission_high_determined_resource{workload_type!~"undefined|build"}[5m])) > 0
      for: 1m
      labels:
        severity: warning
  - name: openshift-mirroring-failures
    rules:
    - alert: openshift-mirroring-failures
      annotations:
        message: OpenShift image mirroring jobs have failed. View failed jobs at the <https://prow.ci.openshift.org/?job=periodic-image-mirroring-openshift|overview>.
      expr: |
        sum by (job_name) (
          rate(
            prowjob_state_transitions{job="prow-controller-manager",job_name!~"rehearse.*",state="success"}[12h]
          )
        )
        * on (job_name) group_left max by (job_name) (prow_job_labels{job_agent="kubernetes",label_ci_openshift_io_role="image-mirroring",label_ci_openshift_io_area="openshift"}) == 0
      for: 1m
      keep_firing_for: 2h
      labels:
        severity: critical
  - name: ghproxy
    rules:
    - alert: ghproxy-too-many-pending-alerts
      annotations:
        message: The average size of the pending GH API request queue in ghproxy is {{ $value | humanize }} over the last 5 minutes, which can indicate insufficient proxy throughput. Inspect <https://console-openshift-console.apps.ci.l2s4.p1.openshiftapps.com/monitoring/alertrules?alerting-rule-name=ghproxy-too-many-pending-alerts|Prometheus> and if the metric is ramping up, consider whether changing ghproxy throttling parameters may be necessary
      expr: |
        sum_over_time(pending_outbound_requests{container="ghproxy"}[5m]) / count_over_time(pending_outbound_requests{container="ghproxy"}[5m]) > 150
      labels:
        severity: critical
    - alert: ghproxy-specific-status-code-abnormal
      annotations:
        message: '{{ $value | humanize }}% of all requests for {{ $labels.path }} through the GitHub proxy are errorring with code {{ $labels.status }}. Check <https://grafana-route-ci-grafana.apps.ci.l2s4.p1.openshiftapps.com/d/d72fe8d0400b2912e319b1e95d0ab1b3/github-cache?orgId=1&refresh=1m&fullscreen&viewPanel=9|grafana>'
      expr: |
        sum(rate(github_request_duration_count{status=~"[45]..",status!="404",status!="410"}[5m])) by (status,path) / ignoring(status) group_left sum(rate(github_request_duration_count[5m])) by (path) * 100 > 10
      labels:
        severity: warning
    - alert: ghproxy-global-status-code-abnormal
      annotations:
        message: '{{ $value | humanize }}% of all API requests through the GitHub proxy are errorring with code {{ $labels.status }}. Check <https://grafana-route-ci-grafana.apps.ci.l2s4.p1.openshiftapps.com/d/d72fe8d0400b2912e319b1e95d0ab1b3/github-cache?orgId=1&fullscreen&viewPanel=8|grafana>'
      expr: |
        sum(rate(github_request_duration_count{status=~"[45]..",status!="404",status!="410"}[5m])) by (status) / ignoring(status) group_left sum(rate(github_request_duration_count[5m])) * 100 > 3
      labels:
        severity: warning
    - alert: ghproxy-running-out-github-tokens-in-a-hour
      annotations:
        message: '{{ $labels.login }} may run out of API quota before the next reset. Check the <https://grafana-route-ci-grafana.apps.ci.l2s4.p1.openshiftapps.com/d/d72fe8d0400b2912e319b1e95d0ab1b3/github-cache?orgId=1|dashboard>'
      expr: |
        github_token_usage{ratelimit_resource="core"} * on(token_hash) group_left(login) max(github_user_info{login=~"openshift-.*"}) by (token_hash, login) + deriv(github_token_usage{ratelimit_resource="core"}[20m]) * github_token_reset{ratelimit_resource="core"} * on(token_hash) group_left(login) max(github_user_info{login=~"openshift-.*"}) by (token_hash, login) / 1e9 < 100
      for: 5m
      labels:
        severity: critical
    - alert: ghproxy-running-out-github-tokens-in-a-hour
      annotations:
        message: '{{ $labels.token_hash }} may run out of API quota before the next reset. Check the <https://grafana-route-ci-grafana.apps.ci.l2s4.p1.openshiftapps.com/d/d72fe8d0400b2912e319b1e95d0ab1b3/github-cache?orgId=1|dashboard>'
      expr: |
        github_token_usage{ratelimit_resource="core",token_hash=~"openshift-ci - .*"} + deriv(github_token_usage{ratelimit_resource="core",token_hash=~"openshift-ci - .*"}[20m]) * github_token_reset{ratelimit_resource="core"}  / 1e9 < 100
      for: 5m
      labels:
        severity: critical
    - alert: ghproxy-90-inode-percent
      annotations:
        message: |+
          {{ $labels.token_hash }} uses 90% of the available inode (<https://grafana-route-ci-grafana.apps.ci.l2s4.p1.openshiftapps.com/d/d72fe8d0400b2912e319b1e95d0ab1b3/github-cache?viewPanel=5&orgId=1|dashboard>)

          Resolve by pruning the cache inside the ghproxy pod:

          $ oc --context app.ci -n ci exec $(oc --context app.ci get pod -n ci -l component=ghproxy -o custom-columns=":metadata.name" --no-headers) -- find /cache/data /cache/temp -mtime +1 -type f -delete

      expr: |
        ghcache_disk_inode_used / ghcache_disk_inode_total * 100 > 90
      for: 5m
      labels:
        severity: critical
  - name: sinker-missing
    rules:
    - alert: SinkerNotRemovingPods
      annotations:
        message: Sinker has not removed any Pods in the last hour, likely indicating an outage in the service.
      expr: |
        absent(sum(rate(sinker_pods_removed[1h]))) == 1
      for: 5m
      labels:
        severity: critical
    - alert: SinkerNotRemovingProwJobs
      annotations:
        message: Sinker has not removed any Prow jobs in the last hour, likely indicating an outage in the service.
      expr: |
        absent(sum(rate(sinker_prow_jobs_cleaned[1h]))) == 1
      for: 5m
      labels:
        severity: critical
  - name: prow-job-failures
    rules:
    - alert: endurance-cluster-maintenance-aws-4.6-failures
      annotations:
        message: '@bparees Prow job endurance-cluster-maintenance-aws-4.6 has failures. Check on <https://prow.ci.openshift.org/?type=periodic&job=endurance-cluster-maintenance-aws-4.6|deck>'
      expr: |
        rate(prowjob_state_transitions{job_name="endurance-cluster-maintenance-aws-4.6",state="failure"}[5m]) > 0
      for: 1m
      labels:
        severity: critical
        team: build-cop
    - alert: periodic-ci-kubevirt-hyperconverged-cluster-operator-release-4.5-hco-e2e-nightly-bundle-release-4-5-azure4-failures
      annotations:
        message: '@build-officer Prow job periodic-ci-kubevirt-hyperconverged-cluster-operator-release-4.5-hco-e2e-nightly-bundle-release-4-5-azure4 has failures. Check on <https://prow.ci.openshift.org/?type=periodic&job=periodic-ci-kubevirt-hyperconverged-cluster-operator-release-4.5-hco-e2e-nightly-bundle-release-4-5-azure4|deck>'
      expr: |
        rate(prowjob_state_transitions{job_name="periodic-ci-kubevirt-hyperconverged-cluster-operator-release-4.5-hco-e2e-nightly-bundle-release-4-5-azure4",state="failure"}[5m]) > 0
      for: 1m
      labels:
        severity: critical
        team: openshift-virtualization
    - alert: periodic-ci-openshift-kni-cnf-features-deploy-master-cnf-e2e-gcp-periodic-failures
      annotations:
        message: '@cnf-cop Prow job periodic-ci-openshift-kni-cnf-features-deploy-master-cnf-e2e-gcp-periodic has failures. Check on <https://prow.ci.openshift.org/?type=periodic&job=periodic-ci-openshift-kni-cnf-features-deploy-master-cnf-e2e-gcp-periodic|deck>'
      expr: |
        rate(prowjob_state_transitions{job_name="periodic-ci-openshift-kni-cnf-features-deploy-master-cnf-e2e-gcp-periodic",state="failure"}[5m]) > 0
      for: 1m
      labels:
        severity: critical
        team: cnf-cop
    - alert: periodic-ci-openshift-kni-cnf-features-deploy-release-4.4-cnf-e2e-gcp-periodic-failures
      annotations:
        message: '@cnf-cop Prow job periodic-ci-openshift-kni-cnf-features-deploy-release-4.4-cnf-e2e-gcp-periodic has failures. Check on <https://prow.ci.openshift.org/?type=periodic&job=periodic-ci-openshift-kni-cnf-features-deploy-release-4.4-cnf-e2e-gcp-periodic|deck>'
      expr: |
        rate(prowjob_state_transitions{job_name="periodic-ci-openshift-kni-cnf-features-deploy-release-4.4-cnf-e2e-gcp-periodic",state="failure"}[5m]) > 0
      for: 1m
      labels:
        severity: critical
        team: cnf-cop
    - alert: periodic-ci-openshift-kni-cnf-features-deploy-release-4.5-cnf-e2e-gcp-periodic-failures
      annotations:
        message: '@cnf-cop Prow job periodic-ci-openshift-kni-cnf-features-deploy-release-4.5-cnf-e2e-gcp-periodic has failures. Check on <https://prow.ci.openshift.org/?type=periodic&job=periodic-ci-openshift-kni-cnf-features-deploy-release-4.5-cnf-e2e-gcp-periodic|deck>'
      expr: |
        rate(prowjob_state_transitions{job_name="periodic-ci-openshift-kni-cnf-features-deploy-release-4.5-cnf-e2e-gcp-periodic",state="failure"}[5m]) > 0
      for: 1m
      labels:
        severity: critical
        team: cnf-cop
    - alert: release-openshift-ocp-installer-e2e-gcp-rt-4.4-failures
      annotations:
        message: '@cnf-cop Prow job release-openshift-ocp-installer-e2e-gcp-rt-4.4 has failures. Check on <https://prow.ci.openshift.org/?type=periodic&job=release-openshift-ocp-installer-e2e-gcp-rt-4.4|deck>'
      expr: |
        rate(prowjob_state_transitions{job_name="release-openshift-ocp-installer-e2e-gcp-rt-4.4",state="failure"}[5m]) > 0
      for: 1m
      labels:
        severity: critical
        team: cnf-cop
    - alert: release-openshift-ocp-installer-e2e-gcp-rt-4.5-failures
      annotations:
        message: '@cnf-cop Prow job release-openshift-ocp-installer-e2e-gcp-rt-4.5 has failures. Check on <https://prow.ci.openshift.org/?type=periodic&job=release-openshift-ocp-installer-e2e-gcp-rt-4.5|deck>'
      expr: |
        rate(prowjob_state_transitions{job_name="release-openshift-ocp-installer-e2e-gcp-rt-4.5",state="failure"}[5m]) > 0
      for: 1m
      labels:
        severity: critical
        team: cnf-cop
  - name: prow
    rules:
    - alert: prow-pod-crashlooping
      annotations:
        message: Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container}}) is restarting {{ printf "%.2f" $value }} times over the last 1 hour.
      expr: increase(kube_pod_container_status_restarts_total{job="kube-state-metrics",namespace="ci"}[1h]) > 20
      for: 10m
      labels:
        severity: critical
    - alert: ImagePullBackOff
      annotations:
        message: Many pods have been failing on pulling images. Please check the relevant events on the cluster.
      expr: sum(sum_over_time(kube_pod_container_status_waiting_reason{reason="ImagePullBackOff"}[5m])) > 30
      for: 10m
      labels:
        severity: critical
    - alert: prow-job-backlog-growing
      annotations:
        message: The number of the triggered Prow jobs that have not yet been running has been increasing for the past hour.
      expr: sum(rate(prowjob_state_transitions{state="triggered"}[5m])) - sum(rate(prowjob_state_transitions{state!="triggered"}[5m])) > 0
      for: 10m
      labels:
        severity: critical
    - alert: Controller backlog is not being drained
      annotations:
        message: The backlog for {{ $labels.name }} is not getting drained. Check <https://console-openshift-console.apps.ci.l2s4.p1.openshiftapps.com/monitoring/alertrules?alerting-rule-name=prow-job-backlog-growing|Prometheus>
      expr: workqueue_depth{name=~"crier.*|plank"} > 100
      for: 20m
      labels:
        severity: critical
    - alert: TriggeredButPendingJobs
      annotations:
        message: There are {{ $value }} jobs that have been triggered but are still pending, indicating potential scheduling or resource constraints. Investigate pending Prow jobs. Check <https://console-openshift-console.apps.ci.l2s4.p1.openshiftapps.com/monitoring/alertrules?alerting-rule-name=TriggeredButPendingJobs|Prometheus>.
      expr: |
        sum by (job_name) (
          increase(prowjob_state_transitions{state="triggered"}[15m])
          - increase(prowjob_state_transitions{state="pending"}[15m])
        ) > 20
      for: 10m
      labels:
        severity: critical
    - expr: count(label_replace(count(github_token_usage{token_hash =~ "openshift.*"}) by (token_hash), "login", "$1", "token_hash", "(.*)") or github_user_info{login=~"openshift-.*"}) by (login)
      record: github:identity_names
  - name: ci-chat-bot-down
    rules:
    - alert: ciChatBotDown
      annotations:
        message: '{{ $labels.deployment }} has been down for 5 minutes.'
      expr: kube_deployment_status_replicas_unavailable{namespace="ci", deployment=~"ci-chat-bot"} >= 1
      for: 5m
      labels:
        severity: critical
        team: crt
  - name: ci-chat-bot-errors
    rules:
    - alert: ciChatBotError
      annotations:
        message: Cluster Bot has reported an error.
      expr: rate(cluster_bot_error_rate[5m]) > 0
      labels:
        severity: critical
        team: crt
  - name: release-controller-down
    rules:
    - alert: releaseControllerDown
      annotations:
        message: '{{ $labels.deployment }} has been down for 5 minutes.'
      expr: kube_deployment_status_replicas_unavailable{namespace="ci", deployment=~"release-controller.*"} >= 1
      for: 5m
      labels:
        severity: critical
  - name: release-controller-bugzilla-errors
    rules:
    - alert: releaseControllerBugzillaError
      annotations:
        message: Release-controller has reported errors in bugzilla verification.
      expr: rate(release_controller_bugzilla_errors_total[5m]) > 0
      labels:
        severity: critical
        team: crt
  - name: release-controller-jira-errors
    rules:
    - alert: releaseControllerJiraError
      annotations:
        message: Release-controller has reported errors in jira verification.
      expr: rate(release_controller_jira_errors_total[5m]) > 0
      labels:
        severity: critical
        team: crt
  - name: tide-missing
    rules:
    - alert: TideNotMergingPRs
      annotations:
        message: Tide has not merged any pull requests in the last hour, likely indicating an outage in the service.
      expr: |
        (sum(rate(merges_count[60m])) and on () (day_of_week() <= 5 and day_of_week() >= 1) and on() (hour() > 7 and hour() < 22 )) == 0
      for: 60m
      labels:
        severity: critical
